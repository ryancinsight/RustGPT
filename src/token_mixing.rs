use ndarray::{Array2, Axis};
use serde::{Deserialize, Serialize};

use crate::hypernetwork::Hypernetwork;
use crate::llm::Layer;

/// Token Mixing MLP for HyperMixer
/// 
/// This layer mixes information across tokens in the sequence using weights
/// dynamically generated by a hypernetwork. This is the key innovation of
/// HyperMixer - instead of using static weights (like MLPMixer) or attention
/// (like Transformers), it generates mixing weights based on the input content.
#[derive(Serialize, Deserialize, Clone, Debug)]
pub struct TokenMixingMLP {
    /// Hypernetwork that generates mixing weights
    hypernetwork: Hypernetwork,
    
    /// Hidden dimension for token mixing
    hidden_dim: usize,
    
    /// Maximum sequence length
    max_seq_len: usize,
    
    /// Embedding dimension
    embedding_dim: usize,
    
    /// Cached values for backward pass
    cached_input: Option<Array2<f32>>,
    cached_mean_pooled: Option<Array2<f32>>,
    cached_generated_weights: Option<Array2<f32>>,
    cached_transposed_input: Option<Array2<f32>>,
}

impl TokenMixingMLP {
    /// Create a new token mixing MLP
    /// 
    /// # Arguments
    /// * `embedding_dim` - Dimension of token embeddings
    /// * `hidden_dim` - Hidden dimension for token mixing
    /// * `max_seq_len` - Maximum sequence length
    /// * `hypernetwork_hidden_dim` - Hidden dimension of the hypernetwork
    pub fn new(
        embedding_dim: usize,
        hidden_dim: usize,
        max_seq_len: usize,
        hypernetwork_hidden_dim: usize,
    ) -> Self {
        // The hypernetwork generates weights for a two-layer MLP that operates on tokens
        // Output size: (max_seq_len * hidden_dim) + hidden_dim + (hidden_dim * max_seq_len) + max_seq_len
        // This represents: w1, b1, w2, b2 for the token mixing MLP
        let output_size = (max_seq_len * hidden_dim) + hidden_dim + (hidden_dim * max_seq_len) + max_seq_len;
        
        let hypernetwork = Hypernetwork::new(embedding_dim, hypernetwork_hidden_dim, output_size);
        
        Self {
            hypernetwork,
            hidden_dim,
            max_seq_len,
            embedding_dim,
            cached_input: None,
            cached_mean_pooled: None,
            cached_generated_weights: None,
            cached_transposed_input: None,
        }
    }
    
    /// Extract weight matrices from the generated weight vector
    fn extract_weights(&self, weights: &Array2<f32>, seq_len: usize) -> (Array2<f32>, Array2<f32>, Array2<f32>, Array2<f32>) {
        let weights_flat = weights.row(0);
        
        let w1_size = seq_len * self.hidden_dim;
        let b1_size = self.hidden_dim;
        let w2_size = self.hidden_dim * seq_len;
        let b2_size = seq_len;
        
        let mut idx = 0;
        
        // Extract w1: (seq_len, hidden_dim)
        let w1_flat = weights_flat.slice(ndarray::s![idx..idx + w1_size]);
        let w1 = Array2::from_shape_vec((seq_len, self.hidden_dim), w1_flat.to_vec()).unwrap();
        idx += w1_size;
        
        // Extract b1: (1, hidden_dim)
        let b1_flat = weights_flat.slice(ndarray::s![idx..idx + b1_size]);
        let b1 = Array2::from_shape_vec((1, self.hidden_dim), b1_flat.to_vec()).unwrap();
        idx += b1_size;
        
        // Extract w2: (hidden_dim, seq_len)
        let w2_flat = weights_flat.slice(ndarray::s![idx..idx + w2_size]);
        let w2 = Array2::from_shape_vec((self.hidden_dim, seq_len), w2_flat.to_vec()).unwrap();
        idx += w2_size;
        
        // Extract b2: (1, seq_len)
        let b2_flat = weights_flat.slice(ndarray::s![idx..idx + b2_size]);
        let b2 = Array2::from_shape_vec((1, seq_len), b2_flat.to_vec()).unwrap();
        
        (w1, b1, w2, b2)
    }
}

impl Layer for TokenMixingMLP {
    fn layer_type(&self) -> &str {
        "TokenMixingMLP"
    }
    
    fn forward(&mut self, input: &Array2<f32>) -> Array2<f32> {
        let (seq_len, emb_dim) = (input.shape()[0], input.shape()[1]);

        // 1. Mean pool across sequence dimension: (seq_len, emb_dim) -> (1, emb_dim)
        let mean_pooled = input.mean_axis(Axis(0)).unwrap().insert_axis(Axis(0));

        // 2. Generate weights using hypernetwork (always generates for max_seq_len)
        let generated_weights = self.hypernetwork.forward(&mean_pooled);

        // 3. Extract weight matrices for token mixing MLP (use max_seq_len, not actual seq_len)
        let (w1_full, b1, w2_full, b2_full) = self.extract_weights(&generated_weights, self.max_seq_len);

        // 4. Slice weights to match actual sequence length
        // w1: (max_seq_len, hidden_dim) -> (seq_len, hidden_dim)
        let w1 = w1_full.slice(ndarray::s![0..seq_len, ..]).to_owned();
        // w2: (hidden_dim, max_seq_len) -> (hidden_dim, seq_len)
        let w2 = w2_full.slice(ndarray::s![.., 0..seq_len]).to_owned();
        // b2: (1, max_seq_len) -> (1, seq_len)
        let b2 = b2_full.slice(ndarray::s![.., 0..seq_len]).to_owned();

        // 5. Transpose input to mix across tokens: (seq_len, emb_dim) -> (emb_dim, seq_len)
        let transposed_input = input.t().to_owned();

        // 6. Apply token mixing MLP across sequence dimension
        // For each embedding dimension, apply MLP across tokens
        let mut mixed_output = Array2::zeros((emb_dim, seq_len));

        for i in 0..emb_dim {
            let token_vec = transposed_input.row(i).insert_axis(Axis(0)); // (1, seq_len)

            // First layer: (1, seq_len) x (seq_len, hidden_dim) + (1, hidden_dim)
            let hidden = token_vec.dot(&w1) + &b1;
            let hidden_activated = hidden.mapv(|x| x.max(0.0)); // ReLU

            // Second layer: (1, hidden_dim) x (hidden_dim, seq_len) + (1, seq_len)
            let output = hidden_activated.dot(&w2) + &b2;

            // Store result
            for j in 0..seq_len {
                mixed_output[[i, j]] = output[[0, j]];
            }
        }
        
        // 7. Transpose back: (emb_dim, seq_len) -> (seq_len, emb_dim)
        let output = mixed_output.t().to_owned();

        // Cache for backward pass (cache full weights, not sliced)
        self.cached_input = Some(input.clone());
        self.cached_mean_pooled = Some(mean_pooled);
        self.cached_generated_weights = Some(generated_weights);
        self.cached_transposed_input = Some(transposed_input);
        
        // Add residual connection
        output + input
    }
    
    fn compute_gradients(
        &self,
        _input: &Array2<f32>,
        output_grads: &Array2<f32>,
    ) -> (Array2<f32>, Vec<Array2<f32>>) {
        // Complete gradient computation matching backward() implementation

        if self.cached_input.is_none() || self.cached_mean_pooled.is_none()
            || self.cached_generated_weights.is_none() || self.cached_transposed_input.is_none() {
            // If no cached values, just pass gradients through (residual only)
            return (output_grads.clone(), vec![]);
        }

        let input = self.cached_input.as_ref().unwrap();
        let _mean_pooled = self.cached_mean_pooled.as_ref().unwrap();
        let generated_weights = self.cached_generated_weights.as_ref().unwrap();
        let transposed_input = self.cached_transposed_input.as_ref().unwrap();
        let (seq_len, emb_dim) = (input.shape()[0], input.shape()[1]);

        // Extract the generated weights (use max_seq_len, then slice)
        let (w1_full, b1, w2_full, b2_full) = self.extract_weights(generated_weights, self.max_seq_len);
        let w1 = w1_full.slice(ndarray::s![0..seq_len, ..]).to_owned();
        let w2 = w2_full.slice(ndarray::s![.., 0..seq_len]).to_owned();
        let b2 = b2_full.slice(ndarray::s![.., 0..seq_len]).to_owned();

        // Gradient through residual connection
        let grad_through_residual = output_grads.clone();

        // Gradient through mixing path
        let grad_mixed_transposed = output_grads.t().to_owned();

        // Backprop through the token mixing MLP for each embedding dimension
        let mut grad_input_transposed = Array2::<f32>::zeros((emb_dim, seq_len));
        let mut grad_w1_accum = Array2::<f32>::zeros(w1.dim());
        let mut grad_b1_accum = Array2::<f32>::zeros(b1.dim());
        let mut grad_w2_accum = Array2::<f32>::zeros(w2.dim());
        let mut grad_b2_accum = Array2::<f32>::zeros(b2.dim());

        for i in 0..emb_dim {
            let token_vec = transposed_input.row(i).insert_axis(Axis(0));
            let grad_output = grad_mixed_transposed.row(i).insert_axis(Axis(0));

            // Forward pass (recompute for gradient computation)
            let hidden = token_vec.dot(&w1) + &b1;
            let hidden_activated = hidden.mapv(|x| x.max(0.0));

            // Backward through second layer
            let grad_w2 = hidden_activated.t().dot(&grad_output);
            let grad_b2 = grad_output.clone();
            let grad_hidden_activated = grad_output.dot(&w2.t());

            // Backward through ReLU
            let grad_hidden = &grad_hidden_activated * &hidden.mapv(|x| if x > 0.0 { 1.0 } else { 0.0 });

            // Backward through first layer
            let grad_w1 = token_vec.t().dot(&grad_hidden);
            let grad_b1 = grad_hidden.clone();
            let grad_token_vec = grad_hidden.dot(&w1.t());

            // Accumulate gradients for weights
            grad_w1_accum = grad_w1_accum + grad_w1;
            grad_b1_accum = grad_b1_accum + grad_b1;
            grad_w2_accum = grad_w2_accum + grad_w2;
            grad_b2_accum = grad_b2_accum + grad_b2;

            // Store gradient w.r.t. input
            for j in 0..seq_len {
                grad_input_transposed[[i, j]] = grad_token_vec[[0, j]];
            }
        }

        // Transpose gradient back
        let grad_through_mixing = grad_input_transposed.t().to_owned();

        // Pad gradients to max_seq_len before flattening
        // grad_w1_accum is (seq_len, hidden_dim), need to pad to (max_seq_len, hidden_dim)
        let mut grad_w1_padded = Array2::<f32>::zeros((self.max_seq_len, self.hidden_dim));
        grad_w1_padded.slice_mut(ndarray::s![0..seq_len, ..]).assign(&grad_w1_accum);

        // grad_w2_accum is (hidden_dim, seq_len), need to pad to (hidden_dim, max_seq_len)
        let mut grad_w2_padded = Array2::<f32>::zeros((self.hidden_dim, self.max_seq_len));
        grad_w2_padded.slice_mut(ndarray::s![.., 0..seq_len]).assign(&grad_w2_accum);

        // grad_b2_accum is (1, seq_len), need to pad to (1, max_seq_len)
        let mut grad_b2_padded = Array2::<f32>::zeros((1, self.max_seq_len));
        grad_b2_padded.slice_mut(ndarray::s![.., 0..seq_len]).assign(&grad_b2_accum);

        // Flatten weight gradients for hypernetwork (now using padded versions)
        let mut grad_generated_weights_flat = Vec::new();
        for row in grad_w1_padded.rows() {
            grad_generated_weights_flat.extend(row.iter().copied());
        }
        for val in grad_b1_accum.iter() {
            grad_generated_weights_flat.push(*val);
        }
        for row in grad_w2_padded.rows() {
            grad_generated_weights_flat.extend(row.iter().copied());
        }
        for val in grad_b2_padded.iter() {
            grad_generated_weights_flat.push(*val);
        }

        let grad_generated_weights = Array2::from_shape_vec(
            (1, grad_generated_weights_flat.len()),
            grad_generated_weights_flat,
        ).unwrap();

        // Compute gradients through hypernetwork
        let (grad_mean_pooled, hyper_param_grads) = self.hypernetwork.compute_gradients(&grad_generated_weights);

        // Gradient w.r.t. mean pooling - broadcast back to all tokens
        let grad_from_mean_pool = grad_mean_pooled.broadcast((seq_len, emb_dim)).unwrap().to_owned();

        // Combine all gradient paths
        let grad_input = grad_through_residual + grad_through_mixing + grad_from_mean_pool;

        // Return input gradients and hypernetwork parameter gradients
        (grad_input, hyper_param_grads)
    }
    
    fn apply_gradients(&mut self, param_grads: &[Array2<f32>], lr: f32) {
        // Apply gradients to the hypernetwork parameters
        // param_grads should contain 4 gradients: [grad_w1, grad_b1, grad_w2, grad_b2]
        if param_grads.len() >= 4 {
            self.hypernetwork.apply_gradients(param_grads, lr);
        }
    }
    
    fn backward(&mut self, grads: &Array2<f32>, lr: f32) -> Array2<f32> {
        // Backward pass through token mixing with proper gradient flow

        // Since we have a residual connection (output = mixed + input),
        // gradients split: part goes through the mixing path, part goes directly to input

        if self.cached_input.is_none() || self.cached_mean_pooled.is_none()
            || self.cached_generated_weights.is_none() || self.cached_transposed_input.is_none() {
            // If no cached values, just pass gradients through (residual only)
            return grads.clone();
        }

        let input = self.cached_input.as_ref().unwrap();
        let _mean_pooled = self.cached_mean_pooled.as_ref().unwrap();
        let generated_weights = self.cached_generated_weights.as_ref().unwrap();
        let transposed_input = self.cached_transposed_input.as_ref().unwrap();
        let (seq_len, emb_dim) = (input.shape()[0], input.shape()[1]);

        // Extract the generated weights (use max_seq_len, then slice)
        let (w1_full, b1, w2_full, b2_full) = self.extract_weights(generated_weights, self.max_seq_len);
        let w1 = w1_full.slice(ndarray::s![0..seq_len, ..]).to_owned();
        let w2 = w2_full.slice(ndarray::s![.., 0..seq_len]).to_owned();
        let b2 = b2_full.slice(ndarray::s![.., 0..seq_len]).to_owned();

        // Gradient through residual connection
        let grad_through_residual = grads.clone();

        // Gradient through mixing path
        // Transpose gradients to match mixed_output shape: (seq_len, emb_dim) -> (emb_dim, seq_len)
        let grad_mixed_transposed = grads.t().to_owned();

        // Backprop through the token mixing MLP for each embedding dimension
        let mut grad_input_transposed = Array2::<f32>::zeros((emb_dim, seq_len));
        let mut grad_w1_accum = Array2::<f32>::zeros(w1.dim());
        let mut grad_b1_accum = Array2::<f32>::zeros(b1.dim());
        let mut grad_w2_accum = Array2::<f32>::zeros(w2.dim());
        let mut grad_b2_accum = Array2::<f32>::zeros(b2.dim());

        for i in 0..emb_dim {
            let token_vec = transposed_input.row(i).insert_axis(Axis(0)); // (1, seq_len)
            let grad_output = grad_mixed_transposed.row(i).insert_axis(Axis(0)); // (1, seq_len)

            // Forward pass (recompute for gradient computation)
            let hidden = token_vec.dot(&w1) + &b1;
            let hidden_activated = hidden.mapv(|x| x.max(0.0)); // ReLU

            // Backward through second layer
            let grad_w2 = hidden_activated.t().dot(&grad_output);
            let grad_b2 = grad_output.clone();
            let grad_hidden_activated = grad_output.dot(&w2.t());

            // Backward through ReLU
            let grad_hidden = &grad_hidden_activated * &hidden.mapv(|x| if x > 0.0 { 1.0 } else { 0.0 });

            // Backward through first layer
            let grad_w1 = token_vec.t().dot(&grad_hidden);
            let grad_b1 = grad_hidden.clone();
            let grad_token_vec = grad_hidden.dot(&w1.t());

            // Accumulate gradients for weights
            grad_w1_accum = grad_w1_accum + grad_w1;
            grad_b1_accum = grad_b1_accum + grad_b1;
            grad_w2_accum = grad_w2_accum + grad_w2;
            grad_b2_accum = grad_b2_accum + grad_b2;

            // Store gradient w.r.t. input
            for j in 0..seq_len {
                grad_input_transposed[[i, j]] = grad_token_vec[[0, j]];
            }
        }

        // Transpose gradient back: (emb_dim, seq_len) -> (seq_len, emb_dim)
        let grad_through_mixing = grad_input_transposed.t().to_owned();

        // Pad gradients to max_seq_len before flattening
        // grad_w1_accum is (seq_len, hidden_dim), need to pad to (max_seq_len, hidden_dim)
        let mut grad_w1_padded = Array2::<f32>::zeros((self.max_seq_len, self.hidden_dim));
        grad_w1_padded.slice_mut(ndarray::s![0..seq_len, ..]).assign(&grad_w1_accum);

        // grad_w2_accum is (hidden_dim, seq_len), need to pad to (hidden_dim, max_seq_len)
        let mut grad_w2_padded = Array2::<f32>::zeros((self.hidden_dim, self.max_seq_len));
        grad_w2_padded.slice_mut(ndarray::s![.., 0..seq_len]).assign(&grad_w2_accum);

        // grad_b2_accum is (1, seq_len), need to pad to (1, max_seq_len)
        let mut grad_b2_padded = Array2::<f32>::zeros((1, self.max_seq_len));
        grad_b2_padded.slice_mut(ndarray::s![.., 0..seq_len]).assign(&grad_b2_accum);

        // Flatten weight gradients for hypernetwork (now using padded versions)
        let mut grad_generated_weights_flat = Vec::new();
        for row in grad_w1_padded.rows() {
            grad_generated_weights_flat.extend(row.iter().copied());
        }
        for val in grad_b1_accum.iter() {
            grad_generated_weights_flat.push(*val);
        }
        for row in grad_w2_padded.rows() {
            grad_generated_weights_flat.extend(row.iter().copied());
        }
        for val in grad_b2_padded.iter() {
            grad_generated_weights_flat.push(*val);
        }

        // Create gradient array for hypernetwork output
        let grad_generated_weights = Array2::from_shape_vec(
            (1, grad_generated_weights_flat.len()),
            grad_generated_weights_flat,
        ).unwrap();

        // Backprop through hypernetwork
        let (grad_mean_pooled, hyper_param_grads) = self.hypernetwork.compute_gradients(&grad_generated_weights);
        self.hypernetwork.apply_gradients(&hyper_param_grads, lr);

        // Gradient w.r.t. mean pooling
        // mean_pooled = input.mean_axis(0), so gradient broadcasts back to all tokens
        let grad_from_mean_pool = grad_mean_pooled.broadcast((seq_len, emb_dim)).unwrap().to_owned();

        // Combine all gradient paths:
        // 1. Residual connection (direct)
        // 2. Through mixing operation
        // 3. Through mean pooling to hypernetwork
        grad_through_residual + grad_through_mixing + grad_from_mean_pool
    }
    
    fn parameters(&self) -> usize {
        self.hypernetwork.parameters()
    }
}

