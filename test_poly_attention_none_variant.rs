use llm::richards::{RichardsCurve, Variant};
use llm::poly_attention::PolyAttention;
use llm::Layer;
use ndarray::{Array2, ShapeBuilder};

fn main() {
    println!("üîç PolyAttention None Variant Benefit Analysis");
    println!("==============================================\n");

    // Create PolyAttention instances with different gate variants
    let mut poly_sigmoid = PolyAttention::new(64, 4, 3, 512, None);  // p=3 (odd)
    let mut poly_none = PolyAttention::new(64, 4, 3, 512, None);     // p=3 (odd)
    
    // Replace the gate_poly with different variants
    poly_sigmoid.gate_poly = RichardsCurve::new_learnable(Variant::Sigmoid);
    poly_none.gate_poly = RichardsCurve::new_learnable(Variant::None);

    println!("1. Parameter Count Comparison:");
    println!("  Sigmoid gate parameters: {}", poly_sigmoid.gate_poly.weights().len());
    println!("  None gate parameters: {}", poly_none.gate_poly.weights().len());
    println!();

    println!("2. Initial Gate Parameters:");
    println!("  Sigmoid gate: {:?}", poly_sigmoid.gate_poly.weights());
    println!("  None gate: {:?}", poly_none.gate_poly.weights());
    println!();

    // Test with sample input
    let batch_size = 8;
    let seq_len = 16;
    let embed_dim = 64;
    
    let input = Array2::<f32>::ones((batch_size * seq_len, embed_dim).f()) * 0.1;
    
    println!("3. Forward Pass Comparison:");
    let output_sigmoid = poly_sigmoid.forward_impl(&input, true);
    let output_none = poly_none.forward_impl(&input, true);
    
    println!("  Sigmoid output shape: {:?}", output_sigmoid.shape());
    println!("  None output shape: {:?}", output_none.shape());
    
    // Calculate output statistics
    let sigmoid_mean = output_sigmoid.mean().unwrap();
    let sigmoid_std = output_sigmoid.std(0.0);
    let none_mean = output_none.mean().unwrap();
    let none_std = output_none.std(0.0);
    
    println!("  Sigmoid output - mean: {:.6}, std: {:.6}", sigmoid_mean, sigmoid_std);
    println!("  None output - mean: {:.6}, std: {:.6}", none_mean, none_std);
    println!();

    // Simulate training to show parameter learning differences
    println!("4. Training Simulation (50 epochs):");
    let learning_rate = 0.1;
    let epochs = 50;
    
    // Create synthetic gradients for training
    let grad_shape = output_sigmoid.shape();
    let synthetic_grads = Array2::<f32>::ones((grad_shape[0], grad_shape[1]).f()) * 0.01;
    
    // Store initial parameters
    let sigmoid_initial = poly_sigmoid.gate_poly.weights();
    let none_initial = poly_none.gate_poly.weights();
    
    for epoch in 0..epochs {
        // Backward pass for both models
        let _input_grad_sigmoid = poly_sigmoid.backward(&synthetic_grads, learning_rate);
        let _input_grad_none = poly_none.backward(&synthetic_grads, learning_rate);
        
        if epoch % 10 == 0 || epoch == epochs - 1 {
            println!("  Epoch {}: Sigmoid params: {:?}", epoch, poly_sigmoid.gate_poly.weights());
            println!("  Epoch {}: None params: {:?}", epoch, poly_none.gate_poly.weights());
            println!();
        }
    }

    // Calculate parameter changes
    let sigmoid_final = poly_sigmoid.gate_poly.weights();
    let none_final = poly_none.gate_poly.weights();
    
    println!("5. Parameter Change Analysis:");
    
    // Sigmoid changes (6 parameters)
    let sigmoid_changes: Vec<f64> = sigmoid_initial.iter()
        .zip(sigmoid_final.iter())
        .map(|(init, final_val)| (final_val - init).abs())
        .collect();
    
    // None changes (8 parameters)  
    let none_changes: Vec<f64> = none_initial.iter()
        .zip(none_final.iter())
        .map(|(init, final_val)| (final_val - init).abs())
        .collect();
    
    println!("  Sigmoid parameter changes: {:?}", sigmoid_changes);
    println!("  None parameter changes: {:?}", none_changes);
    
    let sigmoid_max_change = sigmoid_changes.iter().fold(0.0f64, |a, &b| a.max(b));
    let none_max_change = none_changes.iter().fold(0.0f64, |a, &b| a.max(b));
    
    println!("  Sigmoid max change: {:.6}", sigmoid_max_change);
    println!("  None max change: {:.6}", none_max_change);
    println!();

    // Analyze Richards coefficients specifically
    println!("6. Richards Coefficients Analysis:");
    println!("  Sigmoid a,b: {:.6}, {:.6} (fixed)", 
             poly_sigmoid.gate_poly.a.unwrap_or(1.0), 
             poly_sigmoid.gate_poly.b.unwrap_or(0.0));
    println!("  None a,b: {:.6}, {:.6} (learnable)", 
             poly_none.gate_poly.a.unwrap_or(1.0), 
             poly_none.gate_poly.b.unwrap_or(0.0));
    
    // Check if a,b changed for None variant
    let none_a_changed = (poly_none.gate_poly.a.unwrap_or(1.0) - 1.0).abs() > 1e-6;
    let none_b_changed = (poly_none.gate_poly.b.unwrap_or(0.0) - 0.0).abs() > 1e-6;
    
    println!("  None variant a changed: {}", none_a_changed);
    println!("  None variant b changed: {}", none_b_changed);
    println!();

    println!("7. Benefits Summary:");
    println!("  ‚úÖ None variant has {} more learnable parameters", none_final.len() - sigmoid_final.len());
    println!("  ‚úÖ None variant allows Richards coefficients a,b to adapt");
    println!("  ‚úÖ None variant provides more flexible gating behavior");
    
    if none_max_change > sigmoid_max_change {
        println!("  ‚úÖ None variant shows greater parameter adaptation ({:.6} vs {:.6})", 
                 none_max_change, sigmoid_max_change);
    }
    
    if none_a_changed || none_b_changed {
        println!("  ‚úÖ None variant successfully learned custom Richards coefficients");
    }
    
    println!("\nüéâ PolyAttention benefits from None variant for enhanced gating flexibility!");
}