warning: unused import: `AdaptiveDepthConfig`
 --> src\model_builder.rs:9:20
  |
9 |     model_config::{AdaptiveDepthConfig, ArchitectureType, ModelConfig},
  |                    ^^^^^^^^^^^^^^^^^^^
  |
  = note: `#[warn(unused_imports)]` (part of `#[warn(unused)]`) on by default

warning: unused import: `Axis`
 --> src\token_mixing.rs:1:23
  |
1 | use ndarray::{Array2, Axis};
  |                       ^^^^

warning: unused import: `Axis`
 --> src\trm.rs:1:34
  |
1 | use ndarray::{s, Array1, Array2, Axis};
  |                                  ^^^^

warning: unused imports: `info` and `warn`
 --> src\trm.rs:3:15
  |
3 | use tracing::{info, warn};
  |               ^^^^  ^^^^

warning: use of deprecated function `rand::thread_rng`: Renamed to `rng`
  --> src\swiglu.rs:30:29
   |
30 |         let mut rng = rand::thread_rng();
   |                             ^^^^^^^^^^
   |
   = note: `#[warn(deprecated)]` on by default

warning: use of deprecated function `rand::thread_rng`: Renamed to `rng`
   --> src\trm.rs:260:37
    |
260 |                 let mut rng = rand::thread_rng();
    |                                     ^^^^^^^^^^

warning: use of deprecated function `rand::thread_rng`: Renamed to `rng`
   --> src\trm.rs:362:33
    |
362 |             let mut rng = rand::thread_rng();
    |                                 ^^^^^^^^^^

warning: use of deprecated method `ndarray::impl_methods::<impl ndarray::ArrayBase<S, D>>::into_shape`: Use `.into_shape_with_order()` or `.to_shape()`
   --> src\head_router.rs:129:47
    |
129 |         let logits = input.dot(&self.weights).into_shape(input.nrows()).unwrap();
    |                                               ^^^^^^^^^^

warning: use of deprecated method `ndarray::impl_methods::<impl ndarray::ArrayBase<S, D>>::into_shape`: Use `.into_shape_with_order()` or `.to_shape()`
    --> src\head_router.rs:1353:63
     |
1353 |         let complexity_logits = input.dot(&self.w_complexity).into_shape(seq_len).unwrap();
     |                                                               ^^^^^^^^^^

warning: use of deprecated method `ndarray::impl_methods::<impl ndarray::ArrayBase<S, D>>::into_shape`: Use `.into_shape_with_order()` or `.to_shape()`
    --> src\head_router.rs:1366:61
     |
1366 |         let threshold_logits = input.dot(&self.w_threshold).into_shape(seq_len).unwrap();
     |                                                             ^^^^^^^^^^

warning: use of deprecated method `ndarray::impl_methods::<impl ndarray::ArrayBase<S, D>>::into_shape`: Use `.into_shape_with_order()` or `.to_shape()`
    --> src\head_router.rs:1373:58
     |
1373 |         let temp_logits = input.dot(&self.w_temperature).into_shape(seq_len).unwrap();
     |                                                          ^^^^^^^^^^

warning: use of deprecated method `rand::Rng::gen_range`: Renamed to `random_range`
   --> src\trm.rs:265:25
    |
265 |                     rng.gen_range(-0.01..0.01));
    |                         ^^^^^^^^^

warning: use of deprecated method `rand::Rng::gen_range`: Renamed to `random_range`
   --> src\trm.rs:270:25
    |
270 |                     rng.gen_range(-0.01..0.01));
    |                         ^^^^^^^^^

warning: use of deprecated method `rand::Rng::gen_range`: Renamed to `random_range`
   --> src\trm.rs:365:34
    |
365 |                 let u: f32 = rng.gen_range(1e-10..1.0); // Avoid log(0)
    |                                  ^^^^^^^^^

warning: use of deprecated method `ndarray::impl_methods::<impl ndarray::ArrayBase<S, D>>::into_shape`: Use `.into_shape_with_order()` or `.to_shape()`
   --> src\trm.rs:563:50
    |
563 |                 let attn_logits = attn_logits_2d.into_shape(batch_size).unwrap(); // (seq_len,)
    |                                                  ^^^^^^^^^^

warning: use of deprecated method `ndarray::impl_methods::<impl ndarray::ArrayBase<S, D>>::into_shape`: Use `.into_shape_with_order()` or `.to_shape()`
   --> src\trm.rs:597:50
    |
597 |                 let halt_logits = halt_logits_2d.into_shape(batch_size).unwrap(); // (seq_len,)
    |                                                  ^^^^^^^^^^

warning: unused variable: `router`
   --> src\head_router.rs:274:34
    |
274 |             RouterType::Standard(router) => {
    |                                  ^^^^^^ help: if this is intentional, prefix it with an underscore: `_router`
    |
    = note: `#[warn(unused_variables)]` (part of `#[warn(unused)]`) on by default

warning: unused variable: `entropy_range`
   --> src\head_router.rs:770:13
    |
770 |         let entropy_range = (max_entropy - min_entropy).max(1e-6);
    |             ^^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_entropy_range`

warning: unused variable: `target`
    --> src\head_router.rs:1626:21
     |
1626 |                 let target = target_heads[token_idx];
     |                     ^^^^^^ help: if this is intentional, prefix it with an underscore: `_target`

warning: variable does not need to be mutable
   --> src\hypermixer.rs:151:13
    |
151 |         let mut grad_x2 = output_grads.clone();
    |             ----^^^^^^^
    |             |
    |             help: remove this `mut`
    |
    = note: `#[warn(unused_mut)]` (part of `#[warn(unused)]`) on by default

warning: variable does not need to be mutable
   --> src\moe.rs:686:13
    |
686 |         let mut input_grads = output_grads.clone();
    |             ----^^^^^^^^^^^
    |             |
    |             help: remove this `mut`

warning: unused variable: `expert_idx`
   --> src\moe.rs:696:18
    |
696 |             for (expert_idx, weight) in indices.iter().zip(weights.iter()) {
    |                  ^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_expert_idx`

warning: unused variable: `weighted_grad`
   --> src\moe.rs:698:21
    |
698 |                 let weighted_grad = &token_grad * *weight;
    |                     ^^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_weighted_grad`

warning: unused variable: `input`
   --> src\token_mixing.rs:188:13
    |
188 |         let input = self.cached_input.as_ref().expect("forward must be called first");
    |             ^^^^^ help: if this is intentional, prefix it with an underscore: `_input`

warning: unused variable: `hidden_post_gelu`
   --> src\token_mixing.rs:193:13
    |
193 |         let hidden_post_gelu = self.cached_hidden_post_gelu.as_ref().unwrap();
    |             ^^^^^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_hidden_post_gelu`

warning: unused variable: `param_grads`
   --> src\token_mixing.rs:220:9
    |
220 |         param_grads: &[Array2<f32>],
    |         ^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_param_grads`

warning: unused variable: `lr`
   --> src\token_mixing.rs:221:9
    |
221 |         lr: f32,
    |         ^^ help: if this is intentional, prefix it with an underscore: `_lr`

warning: method `compute_routing_complexity` is never used
   --> src\head_router.rs:742:8
    |
495 | impl HeadRouterStandard {
    | ----------------------- method in this implementation
...
742 |     fn compute_routing_complexity(&mut self, routed_scores: &Array2<f32>) -> Array1<f32> {
    |        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    |
    = note: `#[warn(dead_code)]` (part of `#[warn(unused)]`) on by default

warning: field `cached_head_mask` is never read
  --> src\self_attention.rs:99:5
   |
46 | pub struct SelfAttention {
   |            ------------- field in this struct
...
99 |     cached_head_mask: Option<Array2<bool>>,
   |     ^^^^^^^^^^^^^^^^
   |
   = note: `SelfAttention` has derived impls for the traits `Debug` and `Clone`, but these are intentionally ignored during dead code analysis

warning: method `softmax` is never used
   --> src\self_attention.rs:224:8
    |
108 | impl AttentionHead {
    | ------------------ method in this implementation
...
224 |     fn softmax(&self, scores: &Array2<f32>) -> Array2<f32> {
    |        ^^^^^^^

warning: associated function `softmax_backward` is never used
    --> src\self_attention.rs:1073:8
     |
 874 | impl SelfAttention {
     | ------------------ associated function in this implementation
...
1073 |     fn softmax_backward<S1, S2>(
     |        ^^^^^^^^^^^^^^^^

warning: `llm` (lib) generated 31 warnings (run `cargo fix --lib -p llm` to apply 15 suggestions)
    Finished `release` profile [optimized] target(s) in 3.39s
     Running `target\release\llm.exe`
Adding word: ! to encoding: 0
Adding word: ' to encoding: 1
Adding word: ( to encoding: 2
Adding word: ) to encoding: 3
Adding word: * to encoding: 4
Adding word: + to encoding: 5
Adding word: , to encoding: 6
Adding word: - to encoding: 7
Adding word: . to encoding: 8
Adding word: / to encoding: 9
Adding word: 0 to encoding: 10
Adding word: 022e23 to encoding: 11
Adding word: 0ΓÇô1 to encoding: 12
Adding word: 1 to encoding: 13
Adding word: 100 to encoding: 14
Adding word: 11 to encoding: 15
Adding word: 137 to encoding: 16
Adding word: 15th to encoding: 17
Adding word: 180 to encoding: 18
Adding word: 19 to encoding: 19
Adding word: 1D to encoding: 20
Adding word: 1ΓÇô2D to encoding: 21
Adding word: 2 to encoding: 22
Adding word: 2006 to encoding: 23
Adding word: 250 to encoding: 24
Adding word: 27 to encoding: 25
Adding word: 299 to encoding: 26
Adding word: 3 to encoding: 27
Adding word: 34 to encoding: 28
Adding word: 4 to encoding: 29
Adding word: 44 to encoding: 30
Adding word: 4x to encoding: 31
Adding word: 6 to encoding: 32
Adding word: 602e to encoding: 33
Adding word: 626e to encoding: 34
Adding word: 674e to encoding: 35
Adding word: 792 to encoding: 36
Adding word: 8 to encoding: 37
Adding word: 86 to encoding: 38
Adding word: 998e8 to encoding: 39
Adding word: : to encoding: 40
Adding word: ; to encoding: 41
Adding word: < to encoding: 42
Adding word: </s> to encoding: 43
Adding word: = to encoding: 44
Adding word: > to encoding: 45
Adding word: ? to encoding: 46
Adding word: A to encoding: 47
Adding word: AC to encoding: 48
Adding word: ACID to encoding: 49
Adding word: ADM to encoding: 50
Adding word: ADMM to encoding: 51
Adding word: AUC to encoding: 52
Adding word: Acceleration to encoding: 53
Adding word: Airplanes to encoding: 54
Adding word: Alembert to encoding: 55
Adding word: Alexandria to encoding: 56
Adding word: Alfven to encoding: 57
Adding word: Algorithms to encoding: 58
Adding word: Amazon to encoding: 59
Adding word: Amdahl to encoding: 60
Adding word: Ampere to encoding: 61
Adding word: AmpereΓÇôMaxwell to encoding: 62
Adding word: An to encoding: 63
Adding word: Angular to encoding: 64
Adding word: Animals to encoding: 65
Adding word: Anomalies to encoding: 66
Adding word: Arc to encoding: 67
Adding word: Are to encoding: 68
Adding word: Area to encoding: 69
Adding word: Arithmetic to encoding: 70
Adding word: Art to encoding: 71
Adding word: Arzela to encoding: 72
Adding word: Arzel├á to encoding: 73
Adding word: Ascoli to encoding: 74
Adding word: Assistant to encoding: 75
Adding word: Associative to encoding: 76
Adding word: Astronauts to encoding: 77
Adding word: Astronomy to encoding: 78
Adding word: Async to encoding: 79
Adding word: At to encoding: 80
Adding word: Atomicity to encoding: 81
Adding word: Attention to encoding: 82
Adding word: Availability to encoding: 83
Adding word: Avogadro to encoding: 84
Adding word: B to encoding: 85
Adding word: BCS to encoding: 86
Adding word: BFGS to encoding: 87
Adding word: BFS to encoding: 88
Adding word: BLAS to encoding: 89
Adding word: Backpropagation to encoding: 90
Adding word: Banach to encoding: 91
Adding word: Band to encoding: 92
Adding word: Based to encoding: 93
Adding word: Batteries to encoding: 94
Adding word: Bayes to encoding: 95
Adding word: Bayesian to encoding: 96
Adding word: Beam to encoding: 97
Adding word: Bellman to encoding: 98
Adding word: Bernoulli to encoding: 99
Adding word: Betti to encoding: 100
Adding word: Bias to encoding: 101
Adding word: Bicycles to encoding: 102
Adding word: Bifurcation to encoding: 103
Adding word: Big to encoding: 104
Adding word: Binary to encoding: 105
Adding word: Binding to encoding: 106
Adding word: Biology to encoding: 107
Adding word: Birds to encoding: 108
Adding word: Birkhoff to encoding: 109
Adding word: Bisection to encoding: 110
Adding word: Black to encoding: 111
Adding word: Blackbody to encoding: 112
Adding word: Blasius to encoding: 113
Adding word: Bloch to encoding: 114
Adding word: Bohr to encoding: 115
Adding word: Boltzmann to encoding: 116
Adding word: Bolzano to encoding: 117
Adding word: Bootstrap to encoding: 118
Adding word: Borel to encoding: 119
Adding word: Born to encoding: 120
Adding word: Bose to encoding: 121
Adding word: BoseΓÇôEinstein to encoding: 122
Adding word: Boundary to encoding: 123
Adding word: BrambleΓÇôHilbert to encoding: 124
Adding word: Bread to encoding: 125
Adding word: Breadth to encoding: 126
Adding word: Bridges to encoding: 127
Adding word: Brillouin to encoding: 128
Adding word: Brownian to encoding: 129
Adding word: Buckingham to encoding: 130
Adding word: Buffers to encoding: 131
Adding word: Buoyant to encoding: 132
Adding word: Burgers to encoding: 133
Adding word: Byte to encoding: 134
Adding word: C to encoding: 135
Adding word: CAP to encoding: 136
Adding word: CDM to encoding: 137
Adding word: CI to encoding: 138
Adding word: CKM to encoding: 139
Adding word: CP to encoding: 140
Adding word: CPT to encoding: 141
Adding word: Caching to encoding: 142
Adding word: Camouflage to encoding: 143
Adding word: Can to encoding: 144
Adding word: Cantelli to encoding: 145
Adding word: Capacitance to encoding: 146
Adding word: Capacitive to encoding: 147
Adding word: Carath├⌐odory to encoding: 148
Adding word: Carath├⌐odoryΓÇÖs to encoding: 149
Adding word: Carbon to encoding: 150
Adding word: Cargo to encoding: 151
Adding word: Carlo to encoding: 152
Adding word: Caterpillars to encoding: 153
Adding word: Cats to encoding: 154
Adding word: Cauchy to encoding: 155
Adding word: CauchyΓÇôSchwarz to encoding: 156
Adding word: Cayley to encoding: 157
Adding word: Cells to encoding: 158
Adding word: Central to encoding: 159
Adding word: Ces├áro to encoding: 160
Adding word: Chandrasekhar to encoding: 161
Adding word: Change to encoding: 162
Adding word: Channel to encoding: 163
Adding word: Chaos to encoding: 164
Adding word: Characteristic to encoding: 165
Adding word: Charge to encoding: 166
Adding word: Chebyshev to encoding: 167
Adding word: Chemical to encoding: 168
Adding word: Chemistry to encoding: 169
Adding word: Chernoff to encoding: 170
Adding word: ChernΓÇôWeil to encoding: 171
Adding word: Chinese to encoding: 172
Adding word: Chocolate to encoding: 173
Adding word: Cholesky to encoding: 174
Adding word: Christoffel to encoding: 175
Adding word: Circumference to encoding: 176
Adding word: Clapeyron to encoding: 177
Adding word: Clausius to encoding: 178
Adding word: ClausiusΓÇôClapeyron to encoding: 179
Adding word: Climate to encoding: 180
Adding word: Closed to encoding: 181
Adding word: Clouds to encoding: 182
Adding word: Cluster to encoding: 183
Adding word: Combinations to encoding: 184
Adding word: Comets to encoding: 185
Adding word: Commutators to encoding: 186
Adding word: Compact to encoding: 187
Adding word: Compactness to encoding: 188
Adding word: Compasses to encoding: 189
Adding word: Complex to encoding: 190
Adding word: Complexity to encoding: 191
Adding word: Compton to encoding: 192
Adding word: Computers to encoding: 193
Adding word: Condition to encoding: 194
Adding word: Conduction to encoding: 195
Adding word: Conjugate to encoding: 196
Adding word: Connected to encoding: 197
Adding word: Connections to encoding: 198
Adding word: Consensus to encoding: 199
Adding word: Consistency to encoding: 200
Adding word: Continents to encoding: 201
Adding word: Continuity to encoding: 202
Adding word: Continuous to encoding: 203
Adding word: Convection to encoding: 204
Adding word: Convex to encoding: 205
Adding word: Convolution to encoding: 206
Adding word: Cooper to encoding: 207
Adding word: Correlation to encoding: 208
Adding word: Cosmic to encoding: 209
Adding word: Coulomb to encoding: 210
Adding word: Counterterms to encoding: 211
Adding word: Courant to encoding: 212
Adding word: CourantΓÇôFriedrichsΓÇôLewy to encoding: 213
Adding word: Covariance to encoding: 214
Adding word: Cram├⌐r to encoding: 215
Adding word: Cram├⌐rΓÇÖs to encoding: 216
Adding word: Cross to encoding: 217
Adding word: Curl to encoding: 218
Adding word: Curvature to encoding: 219
Adding word: C├⌐aΓÇÖs to encoding: 220
Adding word: DFS to encoding: 221
Adding word: DNA to encoding: 222
Adding word: Damped to encoding: 223
Adding word: Dark to encoding: 224
Adding word: Data to encoding: 225
Adding word: Database to encoding: 226
Adding word: Databases to encoding: 227
Adding word: De to encoding: 228
Adding word: Debye to encoding: 229
Adding word: Density to encoding: 230
Adding word: Deserts to encoding: 231
Adding word: Detailed to encoding: 232
Adding word: Determinant to encoding: 233
Adding word: Differential to encoding: 234
Adding word: Diffraction to encoding: 235
Adding word: Dijkstra to encoding: 236
Adding word: Dimensional to encoding: 237
Adding word: Dimensionless to encoding: 238
Adding word: Dini to encoding: 239
Adding word: Dirac to encoding: 240
Adding word: Dirichlet to encoding: 241
Adding word: Dispersion to encoding: 242
Adding word: Distributions to encoding: 243
Adding word: Distributive to encoding: 244
Adding word: Divergence to encoding: 245
Adding word: Dominated to encoding: 246
Adding word: Donsker to encoding: 247
Adding word: Doob to encoding: 248
Adding word: Doppler to encoding: 249
Adding word: Dot to encoding: 250
Adding word: Double to encoding: 251
Adding word: Driven to encoding: 252
Adding word: Drop to encoding: 253
Adding word: Durability to encoding: 254
Adding word: Dynamic to encoding: 255
Adding word: Dyson to encoding: 256
Adding word: E to encoding: 257
Adding word: ELBO to encoding: 258
Adding word: EM to encoding: 259
Adding word: Earth to encoding: 260
Adding word: EarthΓÇÖs to encoding: 261
Adding word: Eclipses to encoding: 262
Adding word: Effective to encoding: 263
Adding word: Ehrenfest to encoding: 264
Adding word: Eigenstates to encoding: 265
Adding word: Eigenvectors to encoding: 266
Adding word: Einstein to encoding: 267
Adding word: Electric to encoding: 268
Adding word: Electricity to encoding: 269
Adding word: Electromagnetic to encoding: 270
Adding word: Elementary to encoding: 271
Adding word: Elliptic to encoding: 272
Adding word: Encoding to encoding: 273
Adding word: Encryption to encoding: 274
Adding word: Energy to encoding: 275
Adding word: Enthalpy to encoding: 276
Adding word: Entropy to encoding: 277
Adding word: Equipartition to encoding: 278
Adding word: Equivalence to encoding: 279
Adding word: Ergodic to encoding: 280
Adding word: Escape to encoding: 281
Adding word: Euclidean to encoding: 282
Adding word: Euler to encoding: 283
Adding word: Expectation to encoding: 284
Adding word: Exponential to encoding: 285
Adding word: F to encoding: 286
Adding word: F1 to encoding: 287
Adding word: FIFO to encoding: 288
Adding word: FRW to encoding: 289
Adding word: FT to encoding: 290
Adding word: FULL to encoding: 291
Adding word: FanoΓÇÖs to encoding: 292
Adding word: Faraday to encoding: 293
Adding word: Fatigue to encoding: 294
Adding word: Fenchel to encoding: 295
Adding word: Fermat to encoding: 296
Adding word: Fermi to encoding: 297
Adding word: FermiΓÇôDirac to encoding: 298
Adding word: Feynman to encoding: 299
Adding word: Field to encoding: 300
Adding word: Fine to encoding: 301
Adding word: Finite to encoding: 302
Adding word: Fire to encoding: 303
Adding word: First to encoding: 304
Adding word: Fish to encoding: 305
Adding word: Fisher to encoding: 306
Adding word: Flowers to encoding: 307
Adding word: FluctuationΓÇôdissipation to encoding: 308
Adding word: Fokker to encoding: 309
Adding word: For to encoding: 310
Adding word: Force to encoding: 311
Adding word: Fourier to encoding: 312
Adding word: Fracture to encoding: 313
Adding word: Fraunhofer to encoding: 314
Adding word: Fredholm to encoding: 315
Adding word: Friction to encoding: 316
Adding word: Friedmann to encoding: 317
Adding word: Friedrichs to encoding: 318
Adding word: Frobenius to encoding: 319
Adding word: Fubini to encoding: 320
Adding word: Fundamental to encoding: 321
Adding word: Futures to encoding: 322
Adding word: F┬╖n to encoding: 323
Adding word: G to encoding: 324
Adding word: GMRES to encoding: 325
Adding word: GPU to encoding: 326
Adding word: GR to encoding: 327
Adding word: GagliardoΓÇôNirenberg to encoding: 328
Adding word: Galerkin to encoding: 329
Adding word: Galois to encoding: 330
Adding word: Gauge to encoding: 331
Adding word: Gauss to encoding: 332
Adding word: Gaussian to encoding: 333
Adding word: GaussΓÇôBonnet to encoding: 334
Adding word: Geodesic to encoding: 335
Adding word: Geodesics to encoding: 336
Adding word: Geography to encoding: 337
Adding word: Geology to encoding: 338
Adding word: Gershgorin to encoding: 339
Adding word: Gibbs to encoding: 340
Adding word: GiorgiΓÇôNashΓÇôMoser to encoding: 341
Adding word: Glass to encoding: 342
Adding word: Gold to encoding: 343
Adding word: Goldstone to encoding: 344
Adding word: Good to encoding: 345
Adding word: Gradient to encoding: 346
Adding word: Gram to encoding: 347
Adding word: GramΓÇôSchmidt to encoding: 348
Adding word: Graph to encoding: 349
Adding word: Grass to encoding: 350
Adding word: Gravitational to encoding: 351
Adding word: Gravity to encoding: 352
Adding word: Great to encoding: 353
Adding word: Greedy to encoding: 354
Adding word: Green to encoding: 355
Adding word: GreenΓÇôKubo to encoding: 356
Adding word: Greetings to encoding: 357
Adding word: Gyroscopic to encoding: 358
Adding word: H to encoding: 359
Adding word: HMMs to encoding: 360
Adding word: HTTP to encoding: 361
Adding word: Habitats to encoding: 362
Adding word: Hahn to encoding: 363
Adding word: Hamilton to encoding: 364
Adding word: Hamiltonian to encoding: 365
Adding word: Harmonic to encoding: 366
Adding word: Harnack to encoding: 367
Adding word: Hash to encoding: 368
Adding word: Hastings to encoding: 369
Adding word: Hawking to encoding: 370
Adding word: He to encoding: 371
Adding word: Heaps to encoding: 372
Adding word: Heat to encoding: 373
Adding word: Heine to encoding: 374
Adding word: HeineΓÇôBorel to encoding: 375
Adding word: Heisenberg to encoding: 376
Adding word: Hello to encoding: 377
Adding word: HellyΓÇÖs to encoding: 378
Adding word: Helmholtz to encoding: 379
Adding word: Hermitian to encoding: 380
Adding word: Hessian to encoding: 381
Adding word: HewittΓÇôSavage to encoding: 382
Adding word: Hey to encoding: 383
Adding word: Hi to encoding: 384
Adding word: Hilbert to encoding: 385
Adding word: History to encoding: 386
Adding word: Hodge to encoding: 387
Adding word: Hoeffding to encoding: 388
Adding word: Honeybees to encoding: 389
Adding word: Hooke to encoding: 390
Adding word: Hope to encoding: 391
Adding word: How to encoding: 392
Adding word: Hubble to encoding: 393
Adding word: Humans to encoding: 394
Adding word: Hydrogen to encoding: 395
Adding word: Hydrostatic to encoding: 396
Adding word: H├╢lder to encoding: 397
Adding word: I to encoding: 398
Adding word: INNER to encoding: 399
Adding word: IP to encoding: 400
Adding word: Ice to encoding: 401
Adding word: Ideal to encoding: 402
Adding word: Im to encoding: 403
Adding word: Impedance to encoding: 404
Adding word: Importance to encoding: 405
Adding word: Impulse to encoding: 406
Adding word: InclusionΓÇôexclusion to encoding: 407
Adding word: Inductance to encoding: 408
Adding word: Inductive to encoding: 409
Adding word: Information to encoding: 410
Adding word: Interference to encoding: 411
Adding word: Intermediate to encoding: 412
Adding word: Ising to encoding: 413
Adding word: Isolation to encoding: 414
Adding word: It to encoding: 415
Adding word: Iterators to encoding: 416
Adding word: Ito to encoding: 417
Adding word: It├┤ to encoding: 418
Adding word: J to encoding: 419
Adding word: Jacobi to encoding: 420
Adding word: Jacobian to encoding: 421
Adding word: Jacobians to encoding: 422
Adding word: Jarzynski to encoding: 423
Adding word: Jensen to encoding: 424
Adding word: Jordan to encoding: 425
Adding word: Jupiter to encoding: 426
Adding word: K to encoding: 427
Adding word: KAM to encoding: 428
Adding word: KKT to encoding: 429
Adding word: KL to encoding: 430
Adding word: Kalman to encoding: 431
Adding word: Kelvin to encoding: 432
Adding word: Kepler to encoding: 433
Adding word: Kernel to encoding: 434
Adding word: Kerr to encoding: 435
Adding word: Kinetic to encoding: 436
Adding word: Kirchhoff to encoding: 437
Adding word: Kolmogorov to encoding: 438
Adding word: Komar to encoding: 439
Adding word: KornΓÇÖs to encoding: 440
Adding word: Kruskal to encoding: 441
Adding word: Krylov to encoding: 442
Adding word: Kubo to encoding: 443
Adding word: Kullback to encoding: 444
Adding word: KullbackΓÇôLeibler to encoding: 445
Adding word: KuttaΓÇôJoukowski to encoding: 446
Adding word: K├ñll├⌐nΓÇôLehmann to encoding: 447
Adding word: L to encoding: 448
Adding word: L1 to encoding: 449
Adding word: L2 to encoding: 450
Adding word: LC to encoding: 451
Adding word: LDP to encoding: 452
Adding word: LEFT to encoding: 453
Adding word: LES to encoding: 454
Adding word: LFU to encoding: 455
Adding word: LIFO to encoding: 456
Adding word: LRU to encoding: 457
Adding word: LSZ to encoding: 458
Adding word: LU to encoding: 459
Adding word: Lagrange to encoding: 460
Adding word: Lagrangian to encoding: 461
Adding word: Lambda to encoding: 462
Adding word: Laminar to encoding: 463
Adding word: Landau to encoding: 464
Adding word: Landauer to encoding: 465
Adding word: Language to encoding: 466
Adding word: Languages to encoding: 467
Adding word: Laplace to encoding: 468
Adding word: LaplaceΓÇÖs to encoding: 469
Adding word: Laplacian to encoding: 470
Adding word: Law to encoding: 471
Adding word: Lax to encoding: 472
Adding word: LaxΓÇôRichtmyer to encoding: 473
Adding word: Leaf to encoding: 474
Adding word: Lebesgue to encoding: 475
Adding word: Legendre to encoding: 476
Adding word: Leibler to encoding: 477
Adding word: Length to encoding: 478
Adding word: Let to encoding: 479
Adding word: Lewy to encoding: 480
Adding word: Libraries to encoding: 481
Adding word: Library to encoding: 482
Adding word: Lie to encoding: 483
Adding word: Lifetimes to encoding: 484
Adding word: Lift to encoding: 485
Adding word: Light to encoding: 486
Adding word: Lightning to encoding: 487
Adding word: Limit to encoding: 488
Adding word: Liouville to encoding: 489
Adding word: Lipschitz to encoding: 490
Adding word: Literature to encoding: 491
Adding word: Log to encoding: 492
Adding word: Logarithms to encoding: 493
Adding word: Logging to encoding: 494
Adding word: Logistic to encoding: 495
Adding word: Lorentz to encoding: 496
Adding word: Lyapunov to encoding: 497
Adding word: M to encoding: 498
Adding word: MCMC to encoding: 499
Adding word: MHD to encoding: 500
Adding word: MLE to encoding: 501
Adding word: MOSFET to encoding: 502
Adding word: Magnetic to encoding: 503
Adding word: Magnetohydrodynamics to encoding: 504
Adding word: Magnification to encoding: 505
Adding word: Manifolds to encoding: 506
Adding word: Maps to encoding: 507
Adding word: Marcinkiewicz to encoding: 508
Adding word: Markov to encoding: 509
Adding word: Martingale to encoding: 510
Adding word: Mathematics to encoding: 511
Adding word: Matrix to encoding: 512
Adding word: Maximum to encoding: 513
Adding word: Maxwell to encoding: 514
Adding word: Mean to encoding: 515
Adding word: Meissner to encoding: 516
Adding word: Mellin to encoding: 517
Adding word: Mercer to encoding: 518
Adding word: Mergesort to encoding: 519
Adding word: MerminΓÇôWagner to encoding: 520
Adding word: Metals to encoding: 521
Adding word: Metrics to encoding: 522
Adding word: Metropolis to encoding: 523
Adding word: Milgram to encoding: 524
Adding word: Minerals to encoding: 525
Adding word: Minkowski to encoding: 526
Adding word: Model to encoding: 527
Adding word: Modules to encoding: 528
Adding word: Moment to encoding: 529
Adding word: Momentum to encoding: 530
Adding word: Monotone to encoding: 531
Adding word: Monte to encoding: 532
Adding word: MoonΓÇÖs to encoding: 533
Adding word: Moore to encoding: 534
Adding word: Morse to encoding: 535
Adding word: Most to encoding: 536
Adding word: Mountains to encoding: 537
Adding word: Music to encoding: 538
Adding word: Mutex to encoding: 539
Adding word: Mutual to encoding: 540
Adding word: N to encoding: 541
Adding word: Navier to encoding: 542
Adding word: NavierΓÇôStokes to encoding: 543
Adding word: Networks to encoding: 544
Adding word: Neumann to encoding: 545
Adding word: Neutrino to encoding: 546
Adding word: Newton to encoding: 547
Adding word: Neyman to encoding: 548
Adding word: Nikodym to encoding: 549
Adding word: No to encoding: 550
Adding word: Noether to encoding: 551
Adding word: Noetherian to encoding: 552
Adding word: Non to encoding: 553
Adding word: Nonlinear to encoding: 554
Adding word: Normalizing to encoding: 555
Adding word: Not to encoding: 556
Adding word: Nullity to encoding: 557
Adding word: Numerical to encoding: 558
Adding word: Nusselt to encoding: 559
Adding word: Nyquist to encoding: 560
Adding word: O to encoding: 561
Adding word: Oak to encoding: 562
Adding word: Ocean to encoding: 563
Adding word: Of to encoding: 564
Adding word: Onsager to encoding: 565
Adding word: Open to encoding: 566
Adding word: Operator to encoding: 567
Adding word: Optical to encoding: 568
Adding word: Option to encoding: 569
Adding word: Optional to encoding: 570
Adding word: Orbital to encoding: 571
Adding word: Ornstein to encoding: 572
Adding word: Orthogonal to encoding: 573
Adding word: P to encoding: 574
Adding word: PCA to encoding: 575
Adding word: PDE to encoding: 576
Adding word: PDEs to encoding: 577
Adding word: PSD to encoding: 578
Adding word: PV to encoding: 579
Adding word: Pair to encoding: 580
Adding word: PaleyΓÇôWiener to encoding: 581
Adding word: Parallel to encoding: 582
Adding word: Parseval to encoding: 583
Adding word: Partition to encoding: 584
Adding word: Path to encoding: 585
Adding word: Pattern to encoding: 586
Adding word: Pauli to encoding: 587
Adding word: Paxos to encoding: 588
Adding word: Pearson to encoding: 589
Adding word: Penguins to encoding: 590
Adding word: Penrose to encoding: 591
Adding word: Period to encoding: 592
Adding word: Permutations to encoding: 593
Adding word: Perron to encoding: 594
Adding word: Phase to encoding: 595
Adding word: Phi to encoding: 596
Adding word: Photoelectric to encoding: 597
Adding word: Photons to encoding: 598
Adding word: Photosensitive to encoding: 599
Adding word: Photosynthesis to encoding: 600
Adding word: Physics to encoding: 601
Adding word: Pi to encoding: 602
Adding word: Plancherel to encoding: 603
Adding word: Planck to encoding: 604
Adding word: Planets to encoding: 605
Adding word: Plants to encoding: 606
Adding word: Plasma to encoding: 607
Adding word: Pluto to encoding: 608
Adding word: Poincare to encoding: 609
Adding word: Poincar├⌐ to encoding: 610
Adding word: Poisson to encoding: 611
Adding word: Pollination to encoding: 612
Adding word: Pontryagin to encoding: 613
Adding word: Position to encoding: 614
Adding word: Positive to encoding: 615
Adding word: Power to encoding: 616
Adding word: Poynting to encoding: 617
Adding word: Prandtl to encoding: 618
Adding word: Preconditioning to encoding: 619
Adding word: Predators to encoding: 620
Adding word: Pressure to encoding: 621
Adding word: Prim to encoding: 622
Adding word: Prime to encoding: 623
Adding word: Probabilistic to encoding: 624
Adding word: Probability to encoding: 625
Adding word: Programming to encoding: 626
Adding word: Projected to encoding: 627
Adding word: Propagation to encoding: 628
Adding word: Protocol to encoding: 629
Adding word: Proximal to encoding: 630
Adding word: Pythagorean to encoding: 631
Adding word: P├⌐clet to encoding: 632
Adding word: Q to encoding: 633
Adding word: QFT to encoding: 634
Adding word: QFTs to encoding: 635
Adding word: QR to encoding: 636
Adding word: Quadratic to encoding: 637
Adding word: Quantum to encoding: 638
Adding word: Quarks to encoding: 639
Adding word: Queues to encoding: 640
Adding word: Quicksort to encoding: 641
Adding word: R to encoding: 642
Adding word: RAII to encoding: 643
Adding word: RANS to encoding: 644
Adding word: RC to encoding: 645
Adding word: REST to encoding: 646
Adding word: RIGHT to encoding: 647
Adding word: RKHS to encoding: 648
Adding word: RLC to encoding: 649
Adding word: ROC to encoding: 650
Adding word: RPC to encoding: 651
Adding word: Radiation to encoding: 652
Adding word: Radiative to encoding: 653
Adding word: Radioactive to encoding: 654
Adding word: Radon to encoding: 655
Adding word: RadonΓÇÖs to encoding: 656
Adding word: Raft to encoding: 657
Adding word: Rain to encoding: 658
Adding word: Rainbows to encoding: 659
Adding word: Rainforests to encoding: 660
Adding word: Rank to encoding: 661
Adding word: RankΓÇônullity to encoding: 662
Adding word: Rao to encoding: 663
Adding word: Rate to encoding: 664
Adding word: Rayleigh to encoding: 665
Adding word: Re to encoding: 666
Adding word: Recycling to encoding: 667
Adding word: Regularization to encoding: 668
Adding word: Relativistic to encoding: 669
Adding word: Relativity to encoding: 670
Adding word: Renewable to encoding: 671
Adding word: Renormalization to encoding: 672
Adding word: Representer to encoding: 673
Adding word: Reproducibility to encoding: 674
Adding word: Residue to encoding: 675
Adding word: Resonant to encoding: 676
Adding word: Rest to encoding: 677
Adding word: Result to encoding: 678
Adding word: Reynolds to encoding: 679
Adding word: Riccati to encoding: 680
Adding word: Riemann to encoding: 681
Adding word: Riesz to encoding: 682
Adding word: RieszΓÇôThorin to encoding: 683
Adding word: Rigid to encoding: 684
Adding word: Ring to encoding: 685
Adding word: Ritz to encoding: 686
Adding word: Rivers to encoding: 687
Adding word: Rocks to encoding: 688
Adding word: Rouch├⌐ to encoding: 689
Adding word: Running to encoding: 690
Adding word: Rust to encoding: 691
Adding word: S to encoding: 692
Adding word: SDEs to encoding: 693
Adding word: SI to encoding: 694
Adding word: SIMD to encoding: 695
Adding word: SQL to encoding: 696
Adding word: SU to encoding: 697
Adding word: SVD to encoding: 698
Adding word: SVM to encoding: 699
Adding word: Sahara to encoding: 700
Adding word: Salt to encoding: 701
Adding word: Sand to encoding: 702
Adding word: SanovΓÇÖs to encoding: 703
Adding word: Scattering to encoding: 704
Adding word: Schmidt to encoding: 705
Adding word: Scholes to encoding: 706
Adding word: Schrodinger to encoding: 707
Adding word: Schr├╢dinger to encoding: 708
Adding word: Schwarz to encoding: 709
Adding word: Schwarzschild to encoding: 710
Adding word: Seasons to encoding: 711
Adding word: Second to encoding: 712
Adding word: Seeds to encoding: 713
Adding word: Semiconductors to encoding: 714
Adding word: Send to encoding: 715
Adding word: Set to encoding: 716
Adding word: Shannon to encoding: 717
Adding word: Shell to encoding: 718
Adding word: Shock to encoding: 719
Adding word: Sigma to encoding: 720
Adding word: Simple to encoding: 721
Adding word: Singular to encoding: 722
Adding word: Skin to encoding: 723
Adding word: Slater to encoding: 724
Adding word: SlepianΓÇôWolf to encoding: 725
Adding word: Slope to encoding: 726
Adding word: Slow to encoding: 727
Adding word: Small to encoding: 728
Adding word: Smarr to encoding: 729
Adding word: Snell to encoding: 730
Adding word: Snow to encoding: 731
Adding word: Sobolev to encoding: 732
Adding word: Software to encoding: 733
Adding word: Solar to encoding: 734
Adding word: Sound to encoding: 735
Adding word: Spectral to encoding: 736
Adding word: Speed to encoding: 737
Adding word: Spin to encoding: 738
Adding word: SpinΓÇôstatistics to encoding: 739
Adding word: Stable to encoding: 740
Adding word: Stack to encoding: 741
Adding word: Stacks to encoding: 742
Adding word: Standard to encoding: 743
Adding word: Stars to encoding: 744
Adding word: Stationary to encoding: 745
Adding word: Steepest to encoding: 746
Adding word: Stefan to encoding: 747
Adding word: StefanΓÇôBoltzmann to encoding: 748
Adding word: Steinhaus to encoding: 749
Adding word: Stokes to encoding: 750
Adding word: Stone to encoding: 751
Adding word: Storms to encoding: 752
Adding word: Strain to encoding: 753
Adding word: Stratonovich to encoding: 754
Adding word: Stress to encoding: 755
Adding word: Strong to encoding: 756
Adding word: Sun to encoding: 757
Adding word: Superconductors to encoding: 758
Adding word: Superposition to encoding: 759
Adding word: Sylow to encoding: 760
Adding word: Sylvester to encoding: 761
Adding word: Sync to encoding: 762
Adding word: Systematic to encoding: 763
Adding word: T to encoding: 764
Adding word: TCP to encoding: 765
Adding word: TE to encoding: 766
Adding word: TLS to encoding: 767
Adding word: TM to encoding: 768
Adding word: Taylor to encoding: 769
Adding word: Temperature to encoding: 770
Adding word: Thank to encoding: 771
Adding word: The to encoding: 772
Adding word: Thermal to encoding: 773
Adding word: Thin to encoding: 774
Adding word: Third to encoding: 775
Adding word: Tietze to encoding: 776
Adding word: Tight to encoding: 777
Adding word: Tikhonov to encoding: 778
Adding word: Time to encoding: 779
Adding word: Tonelli to encoding: 780
Adding word: Torque to encoding: 781
Adding word: Total to encoding: 782
Adding word: Train to encoding: 783
Adding word: Traits to encoding: 784
Adding word: Trees to encoding: 785
Adding word: Triangle to encoding: 786
Adding word: U to encoding: 787
Adding word: Uhlenbeck to encoding: 788
Adding word: Uncertainty to encoding: 789
Adding word: Uniform to encoding: 790
Adding word: Uniqueness to encoding: 791
Adding word: Universal to encoding: 792
Adding word: Urysohn to encoding: 793
Adding word: User to encoding: 794
Adding word: V to encoding: 795
Adding word: VAE to encoding: 796
Adding word: Vaccines to encoding: 797
Adding word: Variance to encoding: 798
Adding word: Variational to encoding: 799
Adding word: Vectorization to encoding: 800
Adding word: Vectors to encoding: 801
Adding word: Velocity to encoding: 802
Adding word: Virial to encoding: 803
Adding word: Vlasov to encoding: 804
Adding word: Volcanoes to encoding: 805
Adding word: Voltage to encoding: 806
Adding word: Vorticity to encoding: 807
Adding word: W to encoding: 808
Adding word: WKB to encoding: 809
Adding word: Wall to encoding: 810
Adding word: Ward to encoding: 811
Adding word: Water to encoding: 812
Adding word: Wave to encoding: 813
Adding word: Wavefunction to encoding: 814
Adding word: Waveguide to encoding: 815
Adding word: Waves to encoding: 816
Adding word: WebSockets to encoding: 817
Adding word: Weierstrass to encoding: 818
Adding word: What to encoding: 819
Adding word: Why to encoding: 820
Adding word: Wien to encoding: 821
Adding word: WienerΓÇôKhinchin to encoding: 822
Adding word: Wilson to encoding: 823
Adding word: Wind to encoding: 824
Adding word: Work to encoding: 825
Adding word: X to encoding: 826
Adding word: Xavier to encoding: 827
Adding word: You to encoding: 828
Adding word: Young to encoding: 829
Adding word: Zeroth to encoding: 830
Adding word: [ to encoding: 831
Adding word: ] to encoding: 832
Adding word: ^ to encoding: 833
Adding word: _ to encoding: 834
Adding word: a to encoding: 835
Adding word: about to encoding: 836
Adding word: absolute to encoding: 837
Adding word: absolutely to encoding: 838
Adding word: absorb to encoding: 839
Adding word: absorbs to encoding: 840
Adding word: abundances to encoding: 841
Adding word: accelerate to encoding: 842
Adding word: accelerated to encoding: 843
Adding word: accelerates to encoding: 844
Adding word: acceleration to encoding: 845
Adding word: acceptance to encoding: 846
Adding word: access to encoding: 847
Adding word: according to encoding: 848
Adding word: accumulates to encoding: 849
Adding word: accuracy to encoding: 850
Adding word: accurately to encoding: 851
Adding word: achieves to encoding: 852
Adding word: acorns to encoding: 853
Adding word: across to encoding: 854
Adding word: action to encoding: 855
Adding word: activations to encoding: 856
Adding word: acts to encoding: 857
Adding word: acyclic to encoding: 858
Adding word: adapted to encoding: 859
Adding word: add to encoding: 860
Adding word: adding to encoding: 861
Adding word: addition to encoding: 862
Adding word: additively to encoding: 863
Adding word: additivity to encoding: 864
Adding word: addressing to encoding: 865
Adding word: adds to encoding: 866
Adding word: admit to encoding: 867
Adding word: admits to encoding: 868
Adding word: aerodynamic to encoding: 869
Adding word: after to encoding: 870
Adding word: afternoon to encoding: 871
Adding word: agriculture to encoding: 872
Adding word: aid to encoding: 873
Adding word: aids to encoding: 874
Adding word: air to encoding: 875
Adding word: airborne to encoding: 876
Adding word: airfoils to encoding: 877
Adding word: alexandria to encoding: 878
Adding word: algebra to encoding: 879
Adding word: algebraic to encoding: 880
Adding word: algebras to encoding: 881
Adding word: algorithm to encoding: 882
Adding word: algorithms to encoding: 883
Adding word: aligning to encoding: 884
Adding word: aligns to encoding: 885
Adding word: all to encoding: 886
Adding word: allocators to encoding: 887
Adding word: allows to encoding: 888
Adding word: almost to encoding: 889
Adding word: along to encoding: 890
Adding word: alpha to encoding: 891
Adding word: altering to encoding: 892
Adding word: alternating to encoding: 893
Adding word: alternative to encoding: 894
Adding word: always to encoding: 895
Adding word: amortized to encoding: 896
Adding word: amounts to encoding: 897
Adding word: ampere to encoding: 898
Adding word: amplitude to encoding: 899
Adding word: amplitudes to encoding: 900
Adding word: amputated to encoding: 901
Adding word: an to encoding: 902
Adding word: analysis to encoding: 903
Adding word: analytic to encoding: 904
Adding word: analyze to encoding: 905
Adding word: ancient to encoding: 906
Adding word: and to encoding: 907
Adding word: angle to encoding: 908
Adding word: angles to encoding: 909
Adding word: angular to encoding: 910
Adding word: animals to encoding: 911
Adding word: anomalies to encoding: 912
Adding word: anomaly to encoding: 913
Adding word: another to encoding: 914
Adding word: ansatz to encoding: 915
Adding word: any to encoding: 916
Adding word: apertures to encoding: 917
Adding word: appear to encoding: 918
Adding word: applies to encoding: 919
Adding word: apply to encoding: 920
Adding word: approach to encoding: 921
Adding word: approaches to encoding: 922
Adding word: appropriate to encoding: 923
Adding word: approximate to encoding: 924
Adding word: approximately to encoding: 925
Adding word: approximates to encoding: 926
Adding word: approximation to encoding: 927
Adding word: approximations to encoding: 928
Adding word: are to encoding: 929
Adding word: area to encoding: 930
Adding word: argument to encoding: 931
Adding word: arise to encoding: 932
Adding word: arises to encoding: 933
Adding word: arithmetic to encoding: 934
Adding word: arm to encoding: 935
Adding word: around to encoding: 936
Adding word: arrangements to encoding: 937
Adding word: array to encoding: 938
Adding word: arrays to encoding: 939
Adding word: as to encoding: 940
Adding word: ascending to encoding: 941
Adding word: ash to encoding: 942
Adding word: asking to encoding: 943
Adding word: assigns to encoding: 944
Adding word: assist to encoding: 945
Adding word: assistance to encoding: 946
Adding word: astronomers to encoding: 947
Adding word: asymptotic to encoding: 948
Adding word: asymptotically to encoding: 949
Adding word: at to encoding: 950
Adding word: atmosphere to encoding: 951
Adding word: atmospheric to encoding: 952
Adding word: atom to encoding: 953
Adding word: attain to encoding: 954
Adding word: attains to encoding: 955
Adding word: attractors to encoding: 956
Adding word: attracts to encoding: 957
Adding word: auditory to encoding: 958
Adding word: augmented to encoding: 959
Adding word: autocorrelation to encoding: 960
Adding word: autocorrelations to encoding: 961
Adding word: automates to encoding: 962
Adding word: autumn to encoding: 963
Adding word: available to encoding: 964
Adding word: average to encoding: 965
Adding word: averaged to encoding: 966
Adding word: averages to encoding: 967
Adding word: avoid to encoding: 968
Adding word: await to encoding: 969
Adding word: axial to encoding: 970
Adding word: axis to encoding: 971
Adding word: a┬▓ to encoding: 972
Adding word: b to encoding: 973
Adding word: background to encoding: 974
Adding word: baking to encoding: 975
Adding word: balance to encoding: 976
Adding word: balances to encoding: 977
Adding word: band to encoding: 978
Adding word: bands to encoding: 979
Adding word: bandwidth to encoding: 980
Adding word: bang to encoding: 981
Adding word: bar to encoding: 982
Adding word: barotropic to encoding: 983
Adding word: base to encoding: 984
Adding word: based to encoding: 985
Adding word: bases to encoding: 986
Adding word: basic to encoding: 987
Adding word: basis to encoding: 988
Adding word: be to encoding: 989
Adding word: beaches to encoding: 990
Adding word: beam to encoding: 991
Adding word: become to encoding: 992
Adding word: before to encoding: 993
Adding word: behavior to encoding: 994
Adding word: beliefs to encoding: 995
Adding word: below to encoding: 996
Adding word: benefits to encoding: 997
Adding word: beta to encoding: 998
Adding word: better to encoding: 999
Adding word: between to encoding: 1000
Adding word: bias to encoding: 1001
Adding word: bicycles to encoding: 1002
Adding word: bidirectional to encoding: 1003
Adding word: big to encoding: 1004
Adding word: bijects to encoding: 1005
Adding word: bilinear to encoding: 1006
Adding word: billion to encoding: 1007
Adding word: binary to encoding: 1008
Adding word: binding to encoding: 1009
Adding word: biodiverse to encoding: 1010
Adding word: biodiversity to encoding: 1011
Adding word: birds to encoding: 1012
Adding word: bit to encoding: 1013
Adding word: bits to encoding: 1014
Adding word: black to encoding: 1015
Adding word: blackbody to encoding: 1016
Adding word: blocks to encoding: 1017
Adding word: blood to encoding: 1018
Adding word: bloom to encoding: 1019
Adding word: bodies to encoding: 1020
Adding word: body to encoding: 1021
Adding word: boil to encoding: 1022
Adding word: boils to encoding: 1023
Adding word: books to encoding: 1024
Adding word: bootstraps to encoding: 1025
Adding word: borrow to encoding: 1026
Adding word: borrowing to encoding: 1027
Adding word: borrows to encoding: 1028
Adding word: bosons to encoding: 1029
Adding word: both to encoding: 1030
Adding word: bound to encoding: 1031
Adding word: boundaries to encoding: 1032
Adding word: boundary to encoding: 1033
Adding word: bounded to encoding: 1034
Adding word: boundedness to encoding: 1035
Adding word: bounds to encoding: 1036
Adding word: boxes to encoding: 1037
Adding word: brackets to encoding: 1038
Adding word: brain to encoding: 1039
Adding word: bread to encoding: 1040
Adding word: break to encoding: 1041
Adding word: breaking to encoding: 1042
Adding word: breaks to encoding: 1043
Adding word: breathe to encoding: 1044
Adding word: bright to encoding: 1045
Adding word: bring to encoding: 1046
Adding word: buckets to encoding: 1047
Adding word: budgets to encoding: 1048
Adding word: building to encoding: 1049
Adding word: builds to encoding: 1050
Adding word: built to encoding: 1051
Adding word: butterflies to encoding: 1052
Adding word: by to encoding: 1053
Adding word: byproduct to encoding: 1054
Adding word: byte to encoding: 1055
Adding word: b┬▓ to encoding: 1056
Adding word: c to encoding: 1057
Adding word: cacao to encoding: 1058
Adding word: caches to encoding: 1059
Adding word: calculus to encoding: 1060
Adding word: call to encoding: 1061
Adding word: can to encoding: 1062
Adding word: cancellation to encoding: 1063
Adding word: candela to encoding: 1064
Adding word: cannot to encoding: 1065
Adding word: canonical to encoding: 1066
Adding word: capacity to encoding: 1067
Adding word: capture to encoding: 1068
Adding word: captures to encoding: 1069
Adding word: carbon to encoding: 1070
Adding word: careful to encoding: 1071
Adding word: carries to encoding: 1072
Adding word: carry to encoding: 1073
Adding word: cars to encoding: 1074
Adding word: case to encoding: 1075
Adding word: cats to encoding: 1076
Adding word: causal to encoding: 1077
Adding word: caused to encoding: 1078
Adding word: causes to encoding: 1079
Adding word: causing to encoding: 1080
Adding word: celestial to encoding: 1081
Adding word: celsius to encoding: 1082
Adding word: centered to encoding: 1083
Adding word: century to encoding: 1084
Adding word: certificates to encoding: 1085
Adding word: chain to encoding: 1086
Adding word: chaining to encoding: 1087
Adding word: chains to encoding: 1088
Adding word: change to encoding: 1089
Adding word: changes to encoding: 1090
Adding word: changing to encoding: 1091
Adding word: channel to encoding: 1092
Adding word: channels to encoding: 1093
Adding word: chaotic to encoding: 1094
Adding word: characteristic to encoding: 1095
Adding word: characteristics to encoding: 1096
Adding word: characterize to encoding: 1097
Adding word: characterized to encoding: 1098
Adding word: characterizes to encoding: 1099
Adding word: characterizing to encoding: 1100
Adding word: charge to encoding: 1101
Adding word: charges to encoding: 1102
Adding word: charts to encoding: 1103
Adding word: checks to encoding: 1104
Adding word: chemistry to encoding: 1105
Adding word: china to encoding: 1106
Adding word: chiral to encoding: 1107
Adding word: chocolate to encoding: 1108
Adding word: choices to encoding: 1109
Adding word: choose to encoding: 1110
Adding word: chooses to encoding: 1111
Adding word: ciphers to encoding: 1112
Adding word: circle to encoding: 1113
Adding word: circuit to encoding: 1114
Adding word: circuits to encoding: 1115
Adding word: circular to encoding: 1116
Adding word: circulation to encoding: 1117
Adding word: class to encoding: 1118
Adding word: classes to encoding: 1119
Adding word: classical to encoding: 1120
Adding word: classically to encoding: 1121
Adding word: classify to encoding: 1122
Adding word: climate to encoding: 1123
Adding word: clipping to encoding: 1124
Adding word: closed to encoding: 1125
Adding word: closure to encoding: 1126
Adding word: clouds to encoding: 1127
Adding word: coalescing to encoding: 1128
Adding word: code to encoding: 1129
Adding word: coding to encoding: 1130
Adding word: coefficient to encoding: 1131
Adding word: coefficients to encoding: 1132
Adding word: coercive to encoding: 1133
Adding word: coexact to encoding: 1134
Adding word: coherence to encoding: 1135
Adding word: coherent to encoding: 1136
Adding word: cold to encoding: 1137
Adding word: collect to encoding: 1138
Adding word: collective to encoding: 1139
Adding word: collisions to encoding: 1140
Adding word: colonies to encoding: 1141
Adding word: color to encoding: 1142
Adding word: colors to encoding: 1143
Adding word: column to encoding: 1144
Adding word: columns to encoding: 1145
Adding word: comb to encoding: 1146
Adding word: combine to encoding: 1147
Adding word: combines to encoding: 1148
Adding word: come to encoding: 1149
Adding word: common to encoding: 1150
Adding word: communication to encoding: 1151
Adding word: commutative to encoding: 1152
Adding word: commutator to encoding: 1153
Adding word: commutators to encoding: 1154
Adding word: compact to encoding: 1155
Adding word: compactifies to encoding: 1156
Adding word: compactly to encoding: 1157
Adding word: compactness to encoding: 1158
Adding word: compass to encoding: 1159
Adding word: compasses to encoding: 1160
Adding word: compatible to encoding: 1161
Adding word: compiler to encoding: 1162
Adding word: complementary to encoding: 1163
Adding word: complements to encoding: 1164
Adding word: complete to encoding: 1165
Adding word: complex to encoding: 1166
Adding word: complexity to encoding: 1167
Adding word: component to encoding: 1168
Adding word: components to encoding: 1169
Adding word: composable to encoding: 1170
Adding word: composed to encoding: 1171
Adding word: composes to encoding: 1172
Adding word: compressible to encoding: 1173
Adding word: compression to encoding: 1174
Adding word: computation to encoding: 1175
Adding word: computational to encoding: 1176
Adding word: compute to encoding: 1177
Adding word: computers to encoding: 1178
Adding word: computes to encoding: 1179
Adding word: concurrency to encoding: 1180
Adding word: concurrent to encoding: 1181
Adding word: condensing to encoding: 1182
Adding word: condition to encoding: 1183
Adding word: conditional to encoding: 1184
Adding word: conditioning to encoding: 1185
Adding word: conditions to encoding: 1186
Adding word: conduct to encoding: 1187
Adding word: conduction to encoding: 1188
Adding word: conductivity to encoding: 1189
Adding word: conductor to encoding: 1190
Adding word: conductors to encoding: 1191
Adding word: confidence to encoding: 1192
Adding word: configuration to encoding: 1193
Adding word: congruence to encoding: 1194
Adding word: congruences to encoding: 1195
Adding word: conjugate to encoding: 1196
Adding word: conjugates to encoding: 1197
Adding word: connected to encoding: 1198
Adding word: connection to encoding: 1199
Adding word: connects to encoding: 1200
Adding word: conservation to encoding: 1201
Adding word: conserved to encoding: 1202
Adding word: conserves to encoding: 1203
Adding word: consider to encoding: 1204
Adding word: consistency to encoding: 1205
Adding word: consistent to encoding: 1206
Adding word: consists to encoding: 1207
Adding word: constant to encoding: 1208
Adding word: constants to encoding: 1209
Adding word: constrain to encoding: 1210
Adding word: constrained to encoding: 1211
Adding word: constraints to encoding: 1212
Adding word: constructs to encoding: 1213
Adding word: contain to encoding: 1214
Adding word: contains to encoding: 1215
Adding word: content to encoding: 1216
Adding word: contiguously to encoding: 1217
Adding word: continuation to encoding: 1218
Adding word: continuity to encoding: 1219
Adding word: continuous to encoding: 1220
Adding word: continuum to encoding: 1221
Adding word: contour to encoding: 1222
Adding word: contours to encoding: 1223
Adding word: contraction to encoding: 1224
Adding word: contractions to encoding: 1225
Adding word: contributes to encoding: 1226
Adding word: contributions to encoding: 1227
Adding word: control to encoding: 1228
Adding word: controls to encoding: 1229
Adding word: conv to encoding: 1230
Adding word: converge to encoding: 1231
Adding word: convergence to encoding: 1232
Adding word: convergent to encoding: 1233
Adding word: converges to encoding: 1234
Adding word: conversion to encoding: 1235
Adding word: convert to encoding: 1236
Adding word: converts to encoding: 1237
Adding word: convex to encoding: 1238
Adding word: convexity to encoding: 1239
Adding word: conveys to encoding: 1240
Adding word: convolution to encoding: 1241
Adding word: cookies to encoding: 1242
Adding word: cooperative to encoding: 1243
Adding word: coordinates to encoding: 1244
Adding word: coprime to encoding: 1245
Adding word: correctness to encoding: 1246
Adding word: corrects to encoding: 1247
Adding word: correlation to encoding: 1248
Adding word: correlators to encoding: 1249
Adding word: cos to encoding: 1250
Adding word: cosine to encoding: 1251
Adding word: cosines to encoding: 1252
Adding word: cosmic to encoding: 1253
Adding word: cost to encoding: 1254
Adding word: costate to encoding: 1255
Adding word: coulombs to encoding: 1256
Adding word: count to encoding: 1257
Adding word: countable to encoding: 1258
Adding word: countless to encoding: 1259
Adding word: countries to encoding: 1260
Adding word: counts to encoding: 1261
Adding word: couples to encoding: 1262
Adding word: coupling to encoding: 1263
Adding word: course to encoding: 1264
Adding word: covariance to encoding: 1265
Adding word: covariant to encoding: 1266
Adding word: cover to encoding: 1267
Adding word: covered to encoding: 1268
Adding word: covers to encoding: 1269
Adding word: crack to encoding: 1270
Adding word: crates to encoding: 1271
Adding word: create to encoding: 1272
Adding word: created to encoding: 1273
Adding word: criteria to encoding: 1274
Adding word: criterion to encoding: 1275
Adding word: critical to encoding: 1276
Adding word: cross to encoding: 1277
Adding word: crystalline to encoding: 1278
Adding word: crystals to encoding: 1279
Adding word: cubed to encoding: 1280
Adding word: cues to encoding: 1281
Adding word: cultural to encoding: 1282
Adding word: cultures to encoding: 1283
Adding word: curl to encoding: 1284
Adding word: current to encoding: 1285
Adding word: currents to encoding: 1286
Adding word: curvature to encoding: 1287
Adding word: curve to encoding: 1288
Adding word: curved to encoding: 1289
Adding word: curves to encoding: 1290
Adding word: cutoff to encoding: 1291
Adding word: cycle to encoding: 1292
Adding word: cycles to encoding: 1293
Adding word: cyclic to encoding: 1294
Adding word: c┬▓ to encoding: 1295
Adding word: d to encoding: 1296
Adding word: dA to encoding: 1297
Adding word: dB to encoding: 1298
Adding word: dE to encoding: 1299
Adding word: dP to encoding: 1300
Adding word: dS to encoding: 1301
Adding word: dT to encoding: 1302
Adding word: dV to encoding: 1303
Adding word: damage to encoding: 1304
Adding word: damping to encoding: 1305
Adding word: dark to encoding: 1306
Adding word: data to encoding: 1307
Adding word: datasets to encoding: 1308
Adding word: day to encoding: 1309
Adding word: days to encoding: 1310
Adding word: debugging to encoding: 1311
Adding word: decay to encoding: 1312
Adding word: decaying to encoding: 1313
Adding word: decays to encoding: 1314
Adding word: decomposes to encoding: 1315
Adding word: decomposition to encoding: 1316
Adding word: decreases to encoding: 1317
Adding word: deep to encoding: 1318
Adding word: deficient to encoding: 1319
Adding word: define to encoding: 1320
Adding word: defines to encoding: 1321
Adding word: definite to encoding: 1322
Adding word: definition to encoding: 1323
Adding word: deflection to encoding: 1324
Adding word: deformation to encoding: 1325
Adding word: deforms to encoding: 1326
Adding word: degeneracy to encoding: 1327
Adding word: degree to encoding: 1328
Adding word: degrees to encoding: 1329
Adding word: delta to encoding: 1330
Adding word: dense to encoding: 1331
Adding word: densities to encoding: 1332
Adding word: density to encoding: 1333
Adding word: dependence to encoding: 1334
Adding word: dependency to encoding: 1335
Adding word: dependent to encoding: 1336
Adding word: depends to encoding: 1337
Adding word: depletion to encoding: 1338
Adding word: depth to encoding: 1339
Adding word: dequeue to encoding: 1340
Adding word: derivative to encoding: 1341
Adding word: derivatives to encoding: 1342
Adding word: derive to encoding: 1343
Adding word: descent to encoding: 1344
Adding word: descents to encoding: 1345
Adding word: describe to encoding: 1346
Adding word: describes to encoding: 1347
Adding word: desert to encoding: 1348
Adding word: deserts to encoding: 1349
Adding word: designs to encoding: 1350
Adding word: desired to encoding: 1351
Adding word: destroyed to encoding: 1352
Adding word: det to encoding: 1353
Adding word: detail to encoding: 1354
Adding word: detects to encoding: 1355
Adding word: determinant to encoding: 1356
Adding word: determine to encoding: 1357
Adding word: determined to encoding: 1358
Adding word: determines to encoding: 1359
Adding word: deterministic to encoding: 1360
Adding word: deterministically to encoding: 1361
Adding word: develop to encoding: 1362
Adding word: deviation to encoding: 1363
Adding word: deviations to encoding: 1364
Adding word: devices to encoding: 1365
Adding word: diagonalization to encoding: 1366
Adding word: diagonalizes to encoding: 1367
Adding word: diagram to encoding: 1368
Adding word: diagrams to encoding: 1369
Adding word: did to encoding: 1370
Adding word: difference to encoding: 1371
Adding word: differences to encoding: 1372
Adding word: different to encoding: 1373
Adding word: differentiable to encoding: 1374
Adding word: differential to encoding: 1375
Adding word: differentiation to encoding: 1376
Adding word: differently to encoding: 1377
Adding word: diffraction to encoding: 1378
Adding word: diffusion to encoding: 1379
Adding word: diffusions to encoding: 1380
Adding word: digestive to encoding: 1381
Adding word: digital to encoding: 1382
Adding word: dilation to encoding: 1383
Adding word: dilute to encoding: 1384
Adding word: dimension to encoding: 1385
Adding word: dimensional to encoding: 1386
Adding word: dimensionalization to encoding: 1387
Adding word: dimensionless to encoding: 1388
Adding word: dioxide to encoding: 1389
Adding word: direction to encoding: 1390
Adding word: directions to encoding: 1391
Adding word: discard to encoding: 1392
Adding word: discontinuities to encoding: 1393
Adding word: discrete to encoding: 1394
Adding word: discretizations to encoding: 1395
Adding word: discretize to encoding: 1396
Adding word: discs to encoding: 1397
Adding word: disjoint to encoding: 1398
Adding word: disk to encoding: 1399
Adding word: dispersion to encoding: 1400
Adding word: displaced to encoding: 1401
Adding word: displacement to encoding: 1402
Adding word: dissipates to encoding: 1403
Adding word: dissipation to encoding: 1404
Adding word: dissolves to encoding: 1405
Adding word: distance to encoding: 1406
Adding word: distant to encoding: 1407
Adding word: distinct to encoding: 1408
Adding word: distortion to encoding: 1409
Adding word: distributed to encoding: 1410
Adding word: distribution to encoding: 1411
Adding word: distributional to encoding: 1412
Adding word: distributions to encoding: 1413
Adding word: disturbances to encoding: 1414
Adding word: div to encoding: 1415
Adding word: divergence to encoding: 1416
Adding word: divergences to encoding: 1417
Adding word: divided to encoding: 1418
Adding word: divides to encoding: 1419
Adding word: divisible to encoding: 1420
Adding word: divisor to encoding: 1421
Adding word: divisors to encoding: 1422
Adding word: do to encoding: 1423
Adding word: does to encoding: 1424
Adding word: doing to encoding: 1425
Adding word: domain to encoding: 1426
Adding word: domains to encoding: 1427
Adding word: domesticated to encoding: 1428
Adding word: dominance to encoding: 1429
Adding word: dominant to encoding: 1430
Adding word: dominates to encoding: 1431
Adding word: domination to encoding: 1432
Adding word: don to encoding: 1433
Adding word: done to encoding: 1434
Adding word: dot to encoding: 1435
Adding word: double to encoding: 1436
Adding word: down to encoding: 1437
Adding word: downhill to encoding: 1438
Adding word: dozens to encoding: 1439
Adding word: drag to encoding: 1440
Adding word: drift to encoding: 1441
Adding word: droplets to encoding: 1442
Adding word: dropout to encoding: 1443
Adding word: dry to encoding: 1444
Adding word: dt to encoding: 1445
Adding word: dual to encoding: 1446
Adding word: duality to encoding: 1447
Adding word: duals to encoding: 1448
Adding word: due to encoding: 1449
Adding word: dwarf to encoding: 1450
Adding word: dwarfs to encoding: 1451
Adding word: dynamic to encoding: 1452
Adding word: dynamical to encoding: 1453
Adding word: dynamically to encoding: 1454
Adding word: dynamics to encoding: 1455
Adding word: d╬╛ to encoding: 1456
Adding word: d╧ë to encoding: 1457
Adding word: e to encoding: 1458
Adding word: each to encoding: 1459
Adding word: early to encoding: 1460
Adding word: ears to encoding: 1461
Adding word: earth to encoding: 1462
Adding word: east to encoding: 1463
Adding word: easy to encoding: 1464
Adding word: eat to encoding: 1465
Adding word: eclipses to encoding: 1466
Adding word: ecosystems to encoding: 1467
Adding word: eddies to encoding: 1468
Adding word: edges to encoding: 1469
Adding word: effect to encoding: 1470
Adding word: effective to encoding: 1471
Adding word: effects to encoding: 1472
Adding word: efficiency to encoding: 1473
Adding word: efficient to encoding: 1474
Adding word: efficiently to encoding: 1475
Adding word: eigen to encoding: 1476
Adding word: eigenfunctions to encoding: 1477
Adding word: eigenpair to encoding: 1478
Adding word: eigenpairs to encoding: 1479
Adding word: eigenstates to encoding: 1480
Adding word: eigenvalue to encoding: 1481
Adding word: eigenvalues to encoding: 1482
Adding word: eigenvector to encoding: 1483
Adding word: eigenvectors to encoding: 1484
Adding word: eight to encoding: 1485
Adding word: ejects to encoding: 1486
Adding word: elastic to encoding: 1487
Adding word: elasticity to encoding: 1488
Adding word: electric to encoding: 1489
Adding word: electricity to encoding: 1490
Adding word: electromagnetic to encoding: 1491
Adding word: electromotive to encoding: 1492
Adding word: electron to encoding: 1493
Adding word: electronic to encoding: 1494
Adding word: electronics to encoding: 1495
Adding word: electrons to encoding: 1496
Adding word: electrostatic to encoding: 1497
Adding word: element to encoding: 1498
Adding word: elements to encoding: 1499
Adding word: elevation to encoding: 1500
Adding word: elliptic to encoding: 1501
Adding word: elliptical to encoding: 1502
Adding word: embedding to encoding: 1503
Adding word: embeddings to encoding: 1504
Adding word: empirical to encoding: 1505
Adding word: enable to encoding: 1506
Adding word: enables to encoding: 1507
Adding word: enclosed to encoding: 1508
Adding word: encode to encoding: 1509
Adding word: encoder to encoding: 1510
Adding word: encodes to encoding: 1511
Adding word: endpoints to encoding: 1512
Adding word: energies to encoding: 1513
Adding word: energy to encoding: 1514
Adding word: enforce to encoding: 1515
Adding word: enforces to encoding: 1516
Adding word: engines to encoding: 1517
Adding word: enhancement to encoding: 1518
Adding word: enqueue to encoding: 1519
Adding word: ensemble to encoding: 1520
Adding word: enstrophy to encoding: 1521
Adding word: ensure to encoding: 1522
Adding word: ensures to encoding: 1523
Adding word: enthalpy to encoding: 1524
Adding word: entire to encoding: 1525
Adding word: entropy to encoding: 1526
Adding word: enum to encoding: 1527
Adding word: environment to encoding: 1528
Adding word: environments to encoding: 1529
Adding word: epoch to encoding: 1530
Adding word: epsilon to encoding: 1531
Adding word: equal to encoding: 1532
Adding word: equality to encoding: 1533
Adding word: equalization to encoding: 1534
Adding word: equally to encoding: 1535
Adding word: equals to encoding: 1536
Adding word: equate to encoding: 1537
Adding word: equates to encoding: 1538
Adding word: equation to encoding: 1539
Adding word: equations to encoding: 1540
Adding word: equicontinuity to encoding: 1541
Adding word: equilibria to encoding: 1542
Adding word: equilibrium to encoding: 1543
Adding word: equivalence to encoding: 1544
Adding word: erasure to encoding: 1545
Adding word: ergodic to encoding: 1546
Adding word: ergodicity to encoding: 1547
Adding word: error to encoding: 1548
Adding word: errors to encoding: 1549
Adding word: erupt to encoding: 1550
Adding word: essential to encoding: 1551
Adding word: estimate to encoding: 1552
Adding word: estimates to encoding: 1553
Adding word: estimation to encoding: 1554
Adding word: estimator to encoding: 1555
Adding word: eta to encoding: 1556
Adding word: evaluates to encoding: 1557
Adding word: evaluation to encoding: 1558
Adding word: evaporation to encoding: 1559
Adding word: evening to encoding: 1560
Adding word: evenly to encoding: 1561
Adding word: event to encoding: 1562
Adding word: events to encoding: 1563
Adding word: every to encoding: 1564
Adding word: everywhere to encoding: 1565
Adding word: eviction to encoding: 1566
Adding word: evidence to encoding: 1567
Adding word: evidences to encoding: 1568
Adding word: evolution to encoding: 1569
Adding word: evolve to encoding: 1570
Adding word: exact to encoding: 1571
Adding word: exactly to encoding: 1572
Adding word: examines to encoding: 1573
Adding word: exchange to encoding: 1574
Adding word: exchangeable to encoding: 1575
Adding word: exchanging to encoding: 1576
Adding word: exclusion to encoding: 1577
Adding word: executors to encoding: 1578
Adding word: exhale to encoding: 1579
Adding word: exhaustive to encoding: 1580
Adding word: exhibits to encoding: 1581
Adding word: exist to encoding: 1582
Adding word: existence to encoding: 1583
Adding word: exists to encoding: 1584
Adding word: exp to encoding: 1585
Adding word: expand to encoding: 1586
Adding word: expands to encoding: 1587
Adding word: expansion to encoding: 1588
Adding word: expansions to encoding: 1589
Adding word: expectation to encoding: 1590
Adding word: expectations to encoding: 1591
Adding word: expected to encoding: 1592
Adding word: expels to encoding: 1593
Adding word: explain to encoding: 1594
Adding word: explains to encoding: 1595
Adding word: explicit to encoding: 1596
Adding word: exploding to encoding: 1597
Adding word: explores to encoding: 1598
Adding word: exponential to encoding: 1599
Adding word: exponentially to encoding: 1600
Adding word: exponentials to encoding: 1601
Adding word: exponentiation to encoding: 1602
Adding word: exponents to encoding: 1603
Adding word: express to encoding: 1604
Adding word: expresses to encoding: 1605
Adding word: extend to encoding: 1606
Adding word: extended to encoding: 1607
Adding word: extends to encoding: 1608
Adding word: extension to encoding: 1609
Adding word: extensions to encoding: 1610
Adding word: extraction to encoding: 1611
Adding word: extrema to encoding: 1612
Adding word: extremal to encoding: 1613
Adding word: extremize to encoding: 1614
Adding word: f to encoding: 1615
Adding word: faces to encoding: 1616
Adding word: factor to encoding: 1617
Adding word: factorization to encoding: 1618
Adding word: factorize to encoding: 1619
Adding word: factors to encoding: 1620
Adding word: failure to encoding: 1621
Adding word: fall to encoding: 1622
Adding word: falls to encoding: 1623
Adding word: families to encoding: 1624
Adding word: family to encoding: 1625
Adding word: far to encoding: 1626
Adding word: fast to encoding: 1627
Adding word: fastest to encoding: 1628
Adding word: feature to encoding: 1629
Adding word: features to encoding: 1630
Adding word: fermions to encoding: 1631
Adding word: ferromagnetism to encoding: 1632
Adding word: fidelity to encoding: 1633
Adding word: field to encoding: 1634
Adding word: fields to encoding: 1635
Adding word: fight to encoding: 1636
Adding word: filter to encoding: 1637
Adding word: finds to encoding: 1638
Adding word: finite to encoding: 1639
Adding word: first to encoding: 1640
Adding word: fission to encoding: 1641
Adding word: fit to encoding: 1642
Adding word: five to encoding: 1643
Adding word: fixed to encoding: 1644
Adding word: flat to encoding: 1645
Adding word: flatness to encoding: 1646
Adding word: flavor to encoding: 1647
Adding word: flavored to encoding: 1648
Adding word: flexible to encoding: 1649
Adding word: flightless to encoding: 1650
Adding word: flour to encoding: 1651
Adding word: flow to encoding: 1652
Adding word: flows to encoding: 1653
Adding word: fluctuation to encoding: 1654
Adding word: fluctuations to encoding: 1655
Adding word: fluid to encoding: 1656
Adding word: fluids to encoding: 1657
Adding word: flux to encoding: 1658
Adding word: fly to encoding: 1659
Adding word: focus to encoding: 1660
Adding word: focusing to encoding: 1661
Adding word: fold to encoding: 1662
Adding word: follow to encoding: 1663
Adding word: follows to encoding: 1664
Adding word: food to encoding: 1665
Adding word: for to encoding: 1666
Adding word: forbids to encoding: 1667
Adding word: force to encoding: 1668
Adding word: forces to encoding: 1669
Adding word: form to encoding: 1670
Adding word: formalism to encoding: 1671
Adding word: formation to encoding: 1672
Adding word: formations to encoding: 1673
Adding word: formed to encoding: 1674
Adding word: forming to encoding: 1675
Adding word: forms to encoding: 1676
Adding word: formula to encoding: 1677
Adding word: formulation to encoding: 1678
Adding word: forward to encoding: 1679
Adding word: found to encoding: 1680
Adding word: four to encoding: 1681
Adding word: fourth to encoding: 1682
Adding word: fraction to encoding: 1683
Adding word: frames to encoding: 1684
Adding word: free to encoding: 1685
Adding word: freedom to encoding: 1686
Adding word: frequencies to encoding: 1687
Adding word: frequency to encoding: 1688
Adding word: frequent to encoding: 1689
Adding word: fringes to encoding: 1690
Adding word: from to encoding: 1691
Adding word: frozen to encoding: 1692
Adding word: fruit to encoding: 1693
Adding word: function to encoding: 1694
Adding word: functional to encoding: 1695
Adding word: functionals to encoding: 1696
Adding word: functions to encoding: 1697
Adding word: fundamental to encoding: 1698
Adding word: fusion to encoding: 1699
Adding word: g to encoding: 1700
Adding word: gRPC to encoding: 1701
Adding word: gain to encoding: 1702
Adding word: gap to encoding: 1703
Adding word: gaps to encoding: 1704
Adding word: gas to encoding: 1705
Adding word: gases to encoding: 1706
Adding word: gasoline to encoding: 1707
Adding word: gate to encoding: 1708
Adding word: gauge to encoding: 1709
Adding word: gears to encoding: 1710
Adding word: general to encoding: 1711
Adding word: generalization to encoding: 1712
Adding word: generalize to encoding: 1713
Adding word: generate to encoding: 1714
Adding word: generated to encoding: 1715
Adding word: generating to encoding: 1716
Adding word: generation to encoding: 1717
Adding word: generations to encoding: 1718
Adding word: generics to encoding: 1719
Adding word: genetic to encoding: 1720
Adding word: geodesic to encoding: 1721
Adding word: geodesics to encoding: 1722
Adding word: geographic to encoding: 1723
Adding word: geological to encoding: 1724
Adding word: geometric to encoding: 1725
Adding word: geometry to encoding: 1726
Adding word: get to encoding: 1727
Adding word: gives to encoding: 1728
Adding word: glass to encoding: 1729
Adding word: global to encoding: 1730
Adding word: go to encoding: 1731
Adding word: going to encoding: 1732
Adding word: gold to encoding: 1733
Adding word: golden to encoding: 1734
Adding word: good to encoding: 1735
Adding word: govern to encoding: 1736
Adding word: governed to encoding: 1737
Adding word: governing to encoding: 1738
Adding word: governs to encoding: 1739
Adding word: gradient to encoding: 1740
Adding word: gradients to encoding: 1741
Adding word: grammar to encoding: 1742
Adding word: graph to encoding: 1743
Adding word: graphs to encoding: 1744
Adding word: grating to encoding: 1745
Adding word: gravitational to encoding: 1746
Adding word: gravity to encoding: 1747
Adding word: great to encoding: 1748
Adding word: greatest to encoding: 1749
Adding word: green to encoding: 1750
Adding word: grids to encoding: 1751
Adding word: ground to encoding: 1752
Adding word: group to encoding: 1753
Adding word: groups to encoding: 1754
Adding word: grow to encoding: 1755
Adding word: grows to encoding: 1756
Adding word: growth to encoding: 1757
Adding word: guarantees to encoding: 1758
Adding word: guards to encoding: 1759
Adding word: guesses to encoding: 1760
Adding word: guides to encoding: 1761
Adding word: h to encoding: 1762
Adding word: hair to encoding: 1763
Adding word: half to encoding: 1764
Adding word: halves to encoding: 1765
Adding word: handles to encoding: 1766
Adding word: handling to encoding: 1767
Adding word: happened to encoding: 1768
Adding word: happy to encoding: 1769
Adding word: hard to encoding: 1770
Adding word: harmonic to encoding: 1771
Adding word: harmony to encoding: 1772
Adding word: harnessed to encoding: 1773
Adding word: has to encoding: 1774
Adding word: hash to encoding: 1775
Adding word: have to encoding: 1776
Adding word: having to encoding: 1777
Adding word: head to encoding: 1778
Adding word: heap to encoding: 1779
Adding word: heart to encoding: 1780
Adding word: heat to encoding: 1781
Adding word: heated to encoding: 1782
Adding word: heating to encoding: 1783
Adding word: heavy to encoding: 1784
Adding word: help to encoding: 1785
Adding word: helps to encoding: 1786
Adding word: here to encoding: 1787
Adding word: hidden to encoding: 1788
Adding word: high to encoding: 1789
Adding word: histories to encoding: 1790
Adding word: hit to encoding: 1791
Adding word: holds to encoding: 1792
Adding word: hole to encoding: 1793
Adding word: holes to encoding: 1794
Adding word: home to encoding: 1795
Adding word: homotopy to encoding: 1796
Adding word: honey to encoding: 1797
Adding word: honeybees to encoding: 1798
Adding word: horizon to encoding: 1799
Adding word: hot to encoding: 1800
Adding word: hotspots to encoding: 1801
Adding word: how to encoding: 1802
Adding word: hull to encoding: 1803
Adding word: hulls to encoding: 1804
Adding word: human to encoding: 1805
Adding word: hundreds to encoding: 1806
Adding word: hunt to encoding: 1807
Adding word: hunting to encoding: 1808
Adding word: hydrogen to encoding: 1809
Adding word: hypercontractivity to encoding: 1810
Adding word: hypersurfaces to encoding: 1811
Adding word: hypotheses to encoding: 1812
Adding word: i to encoding: 1813
Adding word: icy to encoding: 1814
Adding word: ideal to encoding: 1815
Adding word: ideals to encoding: 1816
Adding word: ideas to encoding: 1817
Adding word: identical to encoding: 1818
Adding word: identifies to encoding: 1819
Adding word: identify to encoding: 1820
Adding word: identities to encoding: 1821
Adding word: identity to encoding: 1822
Adding word: if to encoding: 1823
Adding word: iff to encoding: 1824
Adding word: ill to encoding: 1825
Adding word: image to encoding: 1826
Adding word: imaginary to encoding: 1827
Adding word: immune to encoding: 1828
Adding word: immutable to encoding: 1829
Adding word: implies to encoding: 1830
Adding word: imply to encoding: 1831
Adding word: important to encoding: 1832
Adding word: imposes to encoding: 1833
Adding word: improve to encoding: 1834
Adding word: improves to encoding: 1835
Adding word: in to encoding: 1836
Adding word: include to encoding: 1837
Adding word: includes to encoding: 1838
Adding word: including to encoding: 1839
Adding word: incompatibility to encoding: 1840
Adding word: incompleteness to encoding: 1841
Adding word: increase to encoding: 1842
Adding word: increases to encoding: 1843
Adding word: increasing to encoding: 1844
Adding word: increments to encoding: 1845
Adding word: independence to encoding: 1846
Adding word: independent to encoding: 1847
Adding word: index to encoding: 1848
Adding word: indexes to encoding: 1849
Adding word: indicate to encoding: 1850
Adding word: indicates to encoding: 1851
Adding word: indicating to encoding: 1852
Adding word: indices to encoding: 1853
Adding word: indistinguishable to encoding: 1854
Adding word: induced to encoding: 1855
Adding word: induces to encoding: 1856
Adding word: induction to encoding: 1857
Adding word: inequalities to encoding: 1858
Adding word: inequality to encoding: 1859
Adding word: inertia to encoding: 1860
Adding word: inertial to encoding: 1861
Adding word: inference to encoding: 1862
Adding word: inferred to encoding: 1863
Adding word: infinitely to encoding: 1864
Adding word: inflation to encoding: 1865
Adding word: influenced to encoding: 1866
Adding word: influences to encoding: 1867
Adding word: information to encoding: 1868
Adding word: initial to encoding: 1869
Adding word: initialization to encoding: 1870
Adding word: inner to encoding: 1871
Adding word: input to encoding: 1872
Adding word: inputs to encoding: 1873
Adding word: insert to encoding: 1874
Adding word: insertion to encoding: 1875
Adding word: inside to encoding: 1876
Adding word: instincts to encoding: 1877
Adding word: instruct to encoding: 1878
Adding word: integer to encoding: 1879
Adding word: integrability to encoding: 1880
Adding word: integrable to encoding: 1881
Adding word: integral to encoding: 1882
Adding word: integrals to encoding: 1883
Adding word: integrands to encoding: 1884
Adding word: integrate to encoding: 1885
Adding word: integrates to encoding: 1886
Adding word: integration to encoding: 1887
Adding word: intensity to encoding: 1888
Adding word: interaction to encoding: 1889
Adding word: interactions to encoding: 1890
Adding word: interarrival to encoding: 1891
Adding word: interchange to encoding: 1892
Adding word: interchanges to encoding: 1893
Adding word: interfaces to encoding: 1894
Adding word: interior to encoding: 1895
Adding word: internal to encoding: 1896
Adding word: internet to encoding: 1897
Adding word: interpolation to encoding: 1898
Adding word: intersect to encoding: 1899
Adding word: intersecting to encoding: 1900
Adding word: intersection to encoding: 1901
Adding word: intersects to encoding: 1902
Adding word: interval to encoding: 1903
Adding word: intervals to encoding: 1904
Adding word: into to encoding: 1905
Adding word: introduces to encoding: 1906
Adding word: invariance to encoding: 1907
Adding word: invariant to encoding: 1908
Adding word: invariants to encoding: 1909
Adding word: invasions to encoding: 1910
Adding word: inverse to encoding: 1911
Adding word: inversely to encoding: 1912
Adding word: inversion to encoding: 1913
Adding word: invertible to encoding: 1914
Adding word: investigates to encoding: 1915
Adding word: inviscid to encoding: 1916
Adding word: iron to encoding: 1917
Adding word: irreducible to encoding: 1918
Adding word: is to encoding: 1919
Adding word: isolated to encoding: 1920
Adding word: isometry to encoding: 1921
Adding word: isotropy to encoding: 1922
Adding word: it to encoding: 1923
Adding word: iterated to encoding: 1924
Adding word: iteration to encoding: 1925
Adding word: iterative to encoding: 1926
Adding word: iteratively to encoding: 1927
Adding word: its to encoding: 1928
Adding word: itself to encoding: 1929
Adding word: jewelry to encoding: 1930
Adding word: joins to encoding: 1931
Adding word: joint to encoding: 1932
Adding word: junction to encoding: 1933
Adding word: just to encoding: 1934
Adding word: justify to encoding: 1935
Adding word: k to encoding: 1936
Adding word: kT to encoding: 1937
Adding word: keeps to encoding: 1938
Adding word: kelvin to encoding: 1939
Adding word: kernel to encoding: 1940
Adding word: kernels to encoding: 1941
Adding word: key to encoding: 1942
Adding word: keys to encoding: 1943
Adding word: kg to encoding: 1944
Adding word: kilogram to encoding: 1945
Adding word: kilometers to encoding: 1946
Adding word: kinetic to encoding: 1947
Adding word: km to encoding: 1948
Adding word: know to encoding: 1949
Adding word: known to encoding: 1950
Adding word: ladder to encoding: 1951
Adding word: lag to encoding: 1952
Adding word: lakes to encoding: 1953
Adding word: lambda to encoding: 1954
Adding word: landmasses to encoding: 1955
Adding word: landscapes to encoding: 1956
Adding word: language to encoding: 1957
Adding word: languages to encoding: 1958
Adding word: lapse to encoding: 1959
Adding word: large to encoding: 1960
Adding word: larger to encoding: 1961
Adding word: largest to encoding: 1962
Adding word: laser to encoding: 1963
Adding word: lateral to encoding: 1964
Adding word: lattice to encoding: 1965
Adding word: lava to encoding: 1966
Adding word: law to encoding: 1967
Adding word: laws to encoding: 1968
Adding word: layer to encoding: 1969
Adding word: layers to encoding: 1970
Adding word: lazy to encoding: 1971
Adding word: leading to encoding: 1972
Adding word: leakage to encoding: 1973
Adding word: leapfrog to encoding: 1974
Adding word: learn to encoding: 1975
Adding word: learning to encoding: 1976
Adding word: least to encoding: 1977
Adding word: leaves to encoding: 1978
Adding word: lemma to encoding: 1979
Adding word: length to encoding: 1980
Adding word: lens to encoding: 1981
Adding word: lensing to encoding: 1982
Adding word: leptons to encoding: 1983
Adding word: less to encoding: 1984
Adding word: level to encoding: 1985
Adding word: levels to encoding: 1986
Adding word: lever to encoding: 1987
Adding word: libraries to encoding: 1988
Adding word: library to encoding: 1989
Adding word: lie to encoding: 1990
Adding word: life to encoding: 1991
Adding word: lift to encoding: 1992
Adding word: light to encoding: 1993
Adding word: like to encoding: 1994
Adding word: likelihood to encoding: 1995
Adding word: limestone to encoding: 1996
Adding word: limit to encoding: 1997
Adding word: limits to encoding: 1998
Adding word: line to encoding: 1999
Adding word: linear to encoding: 2000
Adding word: linearization to encoding: 2001
Adding word: linearize to encoding: 2002
Adding word: linearly to encoding: 2003
Adding word: lines to encoding: 2004
Adding word: linked to encoding: 2005
Adding word: links to encoding: 2006
Adding word: list to encoding: 2007
Adding word: little to encoding: 2008
Adding word: live to encoding: 2009
Adding word: living to encoding: 2010
Adding word: ln to encoding: 2011
Adding word: loading to encoding: 2012
Adding word: local to encoding: 2013
Adding word: locally to encoding: 2014
Adding word: locations to encoding: 2015
Adding word: log to encoding: 2016
Adding word: logarithm to encoding: 2017
Adding word: logs to encoding: 2018
Adding word: long to encoding: 2019
Adding word: lookup to encoding: 2020
Adding word: loop to encoding: 2021
Adding word: loops to encoding: 2022
Adding word: loss to encoding: 2023
Adding word: lossless to encoding: 2024
Adding word: lower to encoding: 2025
Adding word: m to encoding: 2026
Adding word: made to encoding: 2027
Adding word: magic to encoding: 2028
Adding word: magnetic to encoding: 2029
Adding word: magnetism to encoding: 2030
Adding word: magnetized to encoding: 2031
Adding word: magnitude to encoding: 2032
Adding word: maintain to encoding: 2033
Adding word: maintains to encoding: 2034
Adding word: make to encoding: 2035
Adding word: makes to encoding: 2036
Adding word: making to encoding: 2037
Adding word: mammals to encoding: 2038
Adding word: management to encoding: 2039
Adding word: manifold to encoding: 2040
Adding word: manifolds to encoding: 2041
Adding word: many to encoding: 2042
Adding word: map to encoding: 2043
Adding word: mapping to encoding: 2044
Adding word: maps to encoding: 2045
Adding word: margin to encoding: 2046
Adding word: marginals to encoding: 2047
Adding word: marks to encoding: 2048
Adding word: mass to encoding: 2049
Adding word: masses to encoding: 2050
Adding word: massless to encoding: 2051
Adding word: match to encoding: 2052
Adding word: matching to encoding: 2053
Adding word: material to encoding: 2054
Adding word: materials to encoding: 2055
Adding word: matrices to encoding: 2056
Adding word: matrix to encoding: 2057
Adding word: matroid to encoding: 2058
Adding word: matter to encoding: 2059
Adding word: max to encoding: 2060
Adding word: maxima to encoding: 2061
Adding word: maximization to encoding: 2062
Adding word: maximize to encoding: 2063
Adding word: maximizes to encoding: 2064
Adding word: maximizing to encoding: 2065
Adding word: maximum to encoding: 2066
Adding word: may to encoding: 2067
Adding word: me to encoding: 2068
Adding word: mean to encoding: 2069
Adding word: means to encoding: 2070
Adding word: measure to encoding: 2071
Adding word: measurement to encoding: 2072
Adding word: measurements to encoding: 2073
Adding word: measures to encoding: 2074
Adding word: measuring to encoding: 2075
Adding word: mechanics to encoding: 2076
Adding word: mechanism to encoding: 2077
Adding word: media to encoding: 2078
Adding word: meet to encoding: 2079
Adding word: melt to encoding: 2080
Adding word: melts to encoding: 2081
Adding word: memory to encoding: 2082
Adding word: memoryless to encoding: 2083
Adding word: merges to encoding: 2084
Adding word: mesh to encoding: 2085
Adding word: meshes to encoding: 2086
Adding word: metal to encoding: 2087
Adding word: metallic to encoding: 2088
Adding word: metamorphose to encoding: 2089
Adding word: meter to encoding: 2090
Adding word: meters to encoding: 2091
Adding word: method to encoding: 2092
Adding word: methods to encoding: 2093
Adding word: metric to encoding: 2094
Adding word: mgf to encoding: 2095
Adding word: microstates to encoding: 2096
Adding word: microwave to encoding: 2097
Adding word: middle to encoding: 2098
Adding word: midpoint to encoding: 2099
Adding word: millimeters to encoding: 2100
Adding word: min to encoding: 2101
Adding word: minima to encoding: 2102
Adding word: minimization to encoding: 2103
Adding word: minimize to encoding: 2104
Adding word: minimizers to encoding: 2105
Adding word: minimizes to encoding: 2106
Adding word: minimizing to encoding: 2107
Adding word: minimum to encoding: 2108
Adding word: minus to encoding: 2109
Adding word: mitigate to encoding: 2110
Adding word: mitigates to encoding: 2111
Adding word: mix to encoding: 2112
Adding word: mixed to encoding: 2113
Adding word: mixing to encoding: 2114
Adding word: mod to encoding: 2115
Adding word: mode to encoding: 2116
Adding word: model to encoding: 2117
Adding word: modeling to encoding: 2118
Adding word: models to encoding: 2119
Adding word: modes to encoding: 2120
Adding word: modular to encoding: 2121
Adding word: modulo to encoding: 2122
Adding word: modulus to encoding: 2123
Adding word: mole to encoding: 2124
Adding word: moment to encoding: 2125
Adding word: momentum to encoding: 2126
Adding word: monatomic to encoding: 2127
Adding word: monopoles to encoding: 2128
Adding word: monotone to encoding: 2129
Adding word: monotonically to encoding: 2130
Adding word: moon to encoding: 2131
Adding word: moons to encoding: 2132
Adding word: more to encoding: 2133
Adding word: morning to encoding: 2134
Adding word: most to encoding: 2135
Adding word: motion to encoding: 2136
Adding word: mountains to encoding: 2137
Adding word: move to encoding: 2138
Adding word: moves to encoding: 2139
Adding word: moving to encoding: 2140
Adding word: mu to encoding: 2141
Adding word: much to encoding: 2142
Adding word: multiple to encoding: 2143
Adding word: multiples to encoding: 2144
Adding word: multiplication to encoding: 2145
Adding word: multiplicative to encoding: 2146
Adding word: multipliers to encoding: 2147
Adding word: multiplies to encoding: 2148
Adding word: multiply to encoding: 2149
Adding word: mutability to encoding: 2150
Adding word: mutable to encoding: 2151
Adding word: mutual to encoding: 2152
Adding word: n to encoding: 2153
Adding word: nRT to encoding: 2154
Adding word: natural to encoding: 2155
Adding word: naturally to encoding: 2156
Adding word: naught to encoding: 2157
Adding word: near to encoding: 2158
Adding word: nearby to encoding: 2159
Adding word: necessary to encoding: 2160
Adding word: need to encoding: 2161
Adding word: needle to encoding: 2162
Adding word: needs to encoding: 2163
Adding word: negative to encoding: 2164
Adding word: negativity to encoding: 2165
Adding word: nervous to encoding: 2166
Adding word: net to encoding: 2167
Adding word: network to encoding: 2168
Adding word: networks to encoding: 2169
Adding word: neurons to encoding: 2170
Adding word: neutral to encoding: 2171
Adding word: new to encoding: 2172
Adding word: next to encoding: 2173
Adding word: nice to encoding: 2174
Adding word: night to encoding: 2175
Adding word: no to encoding: 2176
Adding word: node to encoding: 2177
Adding word: nodes to encoding: 2178
Adding word: noise to encoding: 2179
Adding word: non to encoding: 2180
Adding word: nonanalytic to encoding: 2181
Adding word: noncommutativity to encoding: 2182
Adding word: nondecreasing to encoding: 2183
Adding word: nondegenerate to encoding: 2184
Adding word: nonempty to encoding: 2185
Adding word: nonequilibrium to encoding: 2186
Adding word: nonlinear to encoding: 2187
Adding word: nonnegative to encoding: 2188
Adding word: nonnegativity to encoding: 2189
Adding word: nonsymmetric to encoding: 2190
Adding word: nontrivial to encoding: 2191
Adding word: nonzero to encoding: 2192
Adding word: norm to encoding: 2193
Adding word: normal to encoding: 2194
Adding word: normality to encoding: 2195
Adding word: normalization to encoding: 2196
Adding word: normalized to encoding: 2197
Adding word: normalizes to encoding: 2198
Adding word: normed to encoding: 2199
Adding word: norms to encoding: 2200
Adding word: north to encoding: 2201
Adding word: notation to encoding: 2202
Adding word: nuclear to encoding: 2203
Adding word: nucleon to encoding: 2204
Adding word: nucleosynthesis to encoding: 2205
Adding word: null to encoding: 2206
Adding word: nullable to encoding: 2207
Adding word: nullity to encoding: 2208
Adding word: number to encoding: 2209
Adding word: numbers to encoding: 2210
Adding word: nutrients to encoding: 2211
Adding word: oak to encoding: 2212
Adding word: obey to encoding: 2213
Adding word: obeys to encoding: 2214
Adding word: object to encoding: 2215
Adding word: objective to encoding: 2216
Adding word: objectives to encoding: 2217
Adding word: objects to encoding: 2218
Adding word: observability to encoding: 2219
Adding word: observables to encoding: 2220
Adding word: observer to encoding: 2221
Adding word: obstacles to encoding: 2222
Adding word: obtain to encoding: 2223
Adding word: occur to encoding: 2224
Adding word: occurrence to encoding: 2225
Adding word: occurs to encoding: 2226
Adding word: oceans to encoding: 2227
Adding word: odd to encoding: 2228
Adding word: of to encoding: 2229
Adding word: off to encoding: 2230
Adding word: often to encoding: 2231
Adding word: omega to encoding: 2232
Adding word: on to encoding: 2233
Adding word: once to encoding: 2234
Adding word: one to encoding: 2235
Adding word: only to encoding: 2236
Adding word: onset to encoding: 2237
Adding word: open to encoding: 2238
Adding word: operations to encoding: 2239
Adding word: operator to encoding: 2240
Adding word: operators to encoding: 2241
Adding word: opposite to encoding: 2242
Adding word: optics to encoding: 2243
Adding word: optimal to encoding: 2244
Adding word: optimality to encoding: 2245
Adding word: optimization to encoding: 2246
Adding word: option to encoding: 2247
Adding word: optional to encoding: 2248
Adding word: or to encoding: 2249
Adding word: orbit to encoding: 2250
Adding word: orbits to encoding: 2251
Adding word: order to encoding: 2252
Adding word: ordered to encoding: 2253
Adding word: ordering to encoding: 2254
Adding word: orderly to encoding: 2255
Adding word: orders to encoding: 2256
Adding word: organic to encoding: 2257
Adding word: organisms to encoding: 2258
Adding word: organize to encoding: 2259
Adding word: organized to encoding: 2260
Adding word: organizes to encoding: 2261
Adding word: orthogonal to encoding: 2262
Adding word: orthogonalizes to encoding: 2263
Adding word: orthonormal to encoding: 2264
Adding word: orthonormalizes to encoding: 2265
Adding word: oscillation to encoding: 2266
Adding word: oscillations to encoding: 2267
Adding word: oscillator to encoding: 2268
Adding word: oscillatory to encoding: 2269
Adding word: other to encoding: 2270
Adding word: our to encoding: 2271
Adding word: out to encoding: 2272
Adding word: outflow to encoding: 2273
Adding word: output to encoding: 2274
Adding word: outside to encoding: 2275
Adding word: over to encoding: 2276
Adding word: overcounting to encoding: 2277
Adding word: overfitting to encoding: 2278
Adding word: overflow to encoding: 2279
Adding word: owner to encoding: 2280
Adding word: ownership to encoding: 2281
Adding word: oxygen to encoding: 2282
Adding word: p to encoding: 2283
Adding word: pairing to encoding: 2284
Adding word: pairs to encoding: 2285
Adding word: panels to encoding: 2286
Adding word: parabolic to encoding: 2287
Adding word: parallel to encoding: 2288
Adding word: parallelizes to encoding: 2289
Adding word: parameters to encoding: 2290
Adding word: part to encoding: 2291
Adding word: partial to encoding: 2292
Adding word: particle to encoding: 2293
Adding word: particles to encoding: 2294
Adding word: partition to encoding: 2295
Adding word: partitioned to encoding: 2296
Adding word: partitions to encoding: 2297
Adding word: parts to encoding: 2298
Adding word: passes to encoding: 2299
Adding word: path to encoding: 2300
Adding word: pathogens to encoding: 2301
Adding word: paths to encoding: 2302
Adding word: patterns to encoding: 2303
Adding word: peak to encoding: 2304
Adding word: peaks to encoding: 2305
Adding word: penalizing to encoding: 2306
Adding word: penalty to encoding: 2307
Adding word: pendulum to encoding: 2308
Adding word: penguins to encoding: 2309
Adding word: per to encoding: 2310
Adding word: perfect to encoding: 2311
Adding word: perform to encoding: 2312
Adding word: performance to encoding: 2313
Adding word: period to encoding: 2314
Adding word: periodic to encoding: 2315
Adding word: periodize to encoding: 2316
Adding word: periods to encoding: 2317
Adding word: perpendicular to encoding: 2318
Adding word: persistent to encoding: 2319
Adding word: perspectives to encoding: 2320
Adding word: perturbation to encoding: 2321
Adding word: perturbations to encoding: 2322
Adding word: perturbative to encoding: 2323
Adding word: phase to encoding: 2324
Adding word: phases to encoding: 2325
Adding word: phenomena to encoding: 2326
Adding word: phi to encoding: 2327
Adding word: phonon to encoding: 2328
Adding word: photon to encoding: 2329
Adding word: photosynthesis to encoding: 2330
Adding word: physical to encoding: 2331
Adding word: physics to encoding: 2332
Adding word: pi to encoding: 2333
Adding word: picture to encoding: 2334
Adding word: pictures to encoding: 2335
Adding word: pivot to encoding: 2336
Adding word: place to encoding: 2337
Adding word: places to encoding: 2338
Adding word: plane to encoding: 2339
Adding word: planet to encoding: 2340
Adding word: planetary to encoding: 2341
Adding word: plants to encoding: 2342
Adding word: plasma to encoding: 2343
Adding word: plasmas to encoding: 2344
Adding word: plastic to encoding: 2345
Adding word: plate to encoding: 2346
Adding word: play to encoding: 2347
Adding word: plus to encoding: 2348
Adding word: point to encoding: 2349
Adding word: pointers to encoding: 2350
Adding word: points to encoding: 2351
Adding word: pointwise to encoding: 2352
Adding word: pole to encoding: 2353
Adding word: poles to encoding: 2354
Adding word: pollination to encoding: 2355
Adding word: pollution to encoding: 2356
Adding word: polyhedra to encoding: 2357
Adding word: polymorphism to encoding: 2358
Adding word: polynomial to encoding: 2359
Adding word: polynomials to encoding: 2360
Adding word: pop to encoding: 2361
Adding word: posed to encoding: 2362
Adding word: position to encoding: 2363
Adding word: positive to encoding: 2364
Adding word: posterior to encoding: 2365
Adding word: posteriori to encoding: 2366
Adding word: posteriors to encoding: 2367
Adding word: potential to encoding: 2368
Adding word: potentials to encoding: 2369
Adding word: power to encoding: 2370
Adding word: powered to encoding: 2371
Adding word: powerful to encoding: 2372
Adding word: practice to encoding: 2373
Adding word: precession to encoding: 2374
Adding word: precipitation to encoding: 2375
Adding word: precision to encoding: 2376
Adding word: predators to encoding: 2377
Adding word: predict to encoding: 2378
Adding word: predictive to encoding: 2379
Adding word: predicts to encoding: 2380
Adding word: premeasure to encoding: 2381
Adding word: preserved to encoding: 2382
Adding word: preserves to encoding: 2383
Adding word: preserving to encoding: 2384
Adding word: press to encoding: 2385
Adding word: pressure to encoding: 2386
Adding word: prevent to encoding: 2387
Adding word: prevents to encoding: 2388
Adding word: prey to encoding: 2389
Adding word: pricing to encoding: 2390
Adding word: primal to encoding: 2391
Adding word: primarily to encoding: 2392
Adding word: prime to encoding: 2393
Adding word: primes to encoding: 2394
Adding word: primordial to encoding: 2395
Adding word: principal to encoding: 2396
Adding word: principle to encoding: 2397
Adding word: printing to encoding: 2398
Adding word: prior to encoding: 2399
Adding word: priority to encoding: 2400
Adding word: priors to encoding: 2401
Adding word: probabilities to encoding: 2402
Adding word: probability to encoding: 2403
Adding word: problem to encoding: 2404
Adding word: problems to encoding: 2405
Adding word: procedures to encoding: 2406
Adding word: process to encoding: 2407
Adding word: processes to encoding: 2408
Adding word: processing to encoding: 2409
Adding word: produce to encoding: 2410
Adding word: produced to encoding: 2411
Adding word: produces to encoding: 2412
Adding word: product to encoding: 2413
Adding word: products to encoding: 2414
Adding word: profiling to encoding: 2415
Adding word: programming to encoding: 2416
Adding word: programs to encoding: 2417
Adding word: progressions to encoding: 2418
Adding word: projection to encoding: 2419
Adding word: promotes to encoding: 2420
Adding word: proof to encoding: 2421
Adding word: proofs to encoding: 2422
Adding word: propagate to encoding: 2423
Adding word: propagates to encoding: 2424
Adding word: propagation to encoding: 2425
Adding word: proper to encoding: 2426
Adding word: properties to encoding: 2427
Adding word: property to encoding: 2428
Adding word: proportional to encoding: 2429
Adding word: proposal to encoding: 2430
Adding word: propose to encoding: 2431
Adding word: protect to encoding: 2432
Adding word: protects to encoding: 2433
Adding word: protocols to encoding: 2434
Adding word: provide to encoding: 2435
Adding word: provides to encoding: 2436
Adding word: pseudoinverse to encoding: 2437
Adding word: psi to encoding: 2438
Adding word: pumps to encoding: 2439
Adding word: push to encoding: 2440
Adding word: p┬╖q╠ç to encoding: 2441
Adding word: p╠ç to encoding: 2442
Adding word: pΓêÆ1 to encoding: 2443
Adding word: q to encoding: 2444
Adding word: quadratic to encoding: 2445
Adding word: quadratically to encoding: 2446
Adding word: quadrupole to encoding: 2447
Adding word: qualitative to encoding: 2448
Adding word: quality to encoding: 2449
Adding word: quantified to encoding: 2450
Adding word: quantifies to encoding: 2451
Adding word: quantify to encoding: 2452
Adding word: quantities to encoding: 2453
Adding word: quantization to encoding: 2454
Adding word: quantized to encoding: 2455
Adding word: quantizes to encoding: 2456
Adding word: quantum to encoding: 2457
Adding word: quark to encoding: 2458
Adding word: quasi to encoding: 2459
Adding word: qubit to encoding: 2460
Adding word: queries to encoding: 2461
Adding word: queue to encoding: 2462
Adding word: quotient to encoding: 2463
Adding word: q╠ç to encoding: 2464
Adding word: r to encoding: 2465
Adding word: races to encoding: 2466
Adding word: radiation to encoding: 2467
Adding word: rain to encoding: 2468
Adding word: rainfall to encoding: 2469
Adding word: rainforest to encoding: 2470
Adding word: random to encoding: 2471
Adding word: range to encoding: 2472
Adding word: rank to encoding: 2473
Adding word: rate to encoding: 2474
Adding word: rates to encoding: 2475
Adding word: ratio to encoding: 2476
Adding word: re to encoding: 2477
Adding word: reach to encoding: 2478
Adding word: reactance to encoding: 2479
Adding word: reaction to encoding: 2480
Adding word: reactions to encoding: 2481
Adding word: real to encoding: 2482
Adding word: recall to encoding: 2483
Adding word: receive to encoding: 2484
Adding word: recessional to encoding: 2485
Adding word: reciprocal to encoding: 2486
Adding word: reciprocity to encoding: 2487
Adding word: reclassified to encoding: 2488
Adding word: recognize to encoding: 2489
Adding word: recombination to encoding: 2490
Adding word: recomputation to encoding: 2491
Adding word: reconstruction to encoding: 2492
Adding word: reconstructs to encoding: 2493
Adding word: recorded to encoding: 2494
Adding word: records to encoding: 2495
Adding word: recover to encoding: 2496
Adding word: recoverable to encoding: 2497
Adding word: recurrence to encoding: 2498
Adding word: recursively to encoding: 2499
Adding word: redefined to encoding: 2500
Adding word: redshift to encoding: 2501
Adding word: reduce to encoding: 2502
Adding word: reduces to encoding: 2503
Adding word: reduction to encoding: 2504
Adding word: redundancy to encoding: 2505
Adding word: reference to encoding: 2506
Adding word: references to encoding: 2507
Adding word: refers to encoding: 2508
Adding word: refines to encoding: 2509
Adding word: reflections to encoding: 2510
Adding word: refraction to encoding: 2511
Adding word: refractive to encoding: 2512
Adding word: refracts to encoding: 2513
Adding word: regime to encoding: 2514
Adding word: regimes to encoding: 2515
Adding word: region to encoding: 2516
Adding word: regression to encoding: 2517
Adding word: regularity to encoding: 2518
Adding word: regularization to encoding: 2519
Adding word: relate to encoding: 2520
Adding word: related to encoding: 2521
Adding word: relates to encoding: 2522
Adding word: relation to encoding: 2523
Adding word: relations to encoding: 2524
Adding word: relationships to encoding: 2525
Adding word: relative to encoding: 2526
Adding word: relativistic to encoding: 2527
Adding word: relativity to encoding: 2528
Adding word: reliable to encoding: 2529
Adding word: remain to encoding: 2530
Adding word: remainder to encoding: 2531
Adding word: remains to encoding: 2532
Adding word: remove to encoding: 2533
Adding word: renewable to encoding: 2534
Adding word: renormalized to encoding: 2535
Adding word: repeated to encoding: 2536
Adding word: repetition to encoding: 2537
Adding word: replenish to encoding: 2538
Adding word: replicated to encoding: 2539
Adding word: represent to encoding: 2540
Adding word: representation to encoding: 2541
Adding word: represents to encoding: 2542
Adding word: reproduce to encoding: 2543
Adding word: require to encoding: 2544
Adding word: required to encoding: 2545
Adding word: requires to encoding: 2546
Adding word: resamples to encoding: 2547
Adding word: rescale to encoding: 2548
Adding word: resemble to encoding: 2549
Adding word: residues to encoding: 2550
Adding word: resistance to encoding: 2551
Adding word: resistors to encoding: 2552
Adding word: resolution to encoding: 2553
Adding word: resolved to encoding: 2554
Adding word: resonance to encoding: 2555
Adding word: resources to encoding: 2556
Adding word: respect to encoding: 2557
Adding word: respond to encoding: 2558
Adding word: response to encoding: 2559
Adding word: responses to encoding: 2560
Adding word: restart to encoding: 2561
Adding word: results to encoding: 2562
Adding word: retrieval to encoding: 2563
Adding word: retrieves to encoding: 2564
Adding word: returns to encoding: 2565
Adding word: reveal to encoding: 2566
Adding word: reveals to encoding: 2567
Adding word: reverting to encoding: 2568
Adding word: revisit to encoding: 2569
Adding word: revolutionized to encoding: 2570
Adding word: revolves to encoding: 2571
Adding word: reweighting to encoding: 2572
Adding word: rho to encoding: 2573
Adding word: rhythm to encoding: 2574
Adding word: rich to encoding: 2575
Adding word: right to encoding: 2576
Adding word: rings to encoding: 2577
Adding word: rise to encoding: 2578
Adding word: rises to encoding: 2579
Adding word: risk to encoding: 2580
Adding word: rivers to encoding: 2581
Adding word: roads to encoding: 2582
Adding word: roasted to encoding: 2583
Adding word: robust to encoding: 2584
Adding word: robustly to encoding: 2585
Adding word: rockets to encoding: 2586
Adding word: rocks to encoding: 2587
Adding word: rocky to encoding: 2588
Adding word: role to encoding: 2589
Adding word: roll to encoding: 2590
Adding word: root to encoding: 2591
Adding word: roots to encoding: 2592
Adding word: rotating to encoding: 2593
Adding word: rotation to encoding: 2594
Adding word: rows to encoding: 2595
Adding word: rule to encoding: 2596
Adding word: rules to encoding: 2597
Adding word: run to encoding: 2598
Adding word: runs to encoding: 2599
Adding word: s to encoding: 2600
Adding word: safety to encoding: 2601
Adding word: sahara to encoding: 2602
Adding word: salient to encoding: 2603
Adding word: salt to encoding: 2604
Adding word: same to encoding: 2605
Adding word: sample to encoding: 2606
Adding word: samples to encoding: 2607
Adding word: sampling to encoding: 2608
Adding word: sand to encoding: 2609
Adding word: satisfies to encoding: 2610
Adding word: satisfy to encoding: 2611
Adding word: says to encoding: 2612
Adding word: scalar to encoding: 2613
Adding word: scale to encoding: 2614
Adding word: scaled to encoding: 2615
Adding word: scales to encoding: 2616
Adding word: scaling to encoding: 2617
Adding word: scattering to encoding: 2618
Adding word: schedule to encoding: 2619
Adding word: schemes to encoding: 2620
Adding word: scope to encoding: 2621
Adding word: score to encoding: 2622
Adding word: screening to encoding: 2623
Adding word: screens to encoding: 2624
Adding word: search to encoding: 2625
Adding word: secant to encoding: 2626
Adding word: second to encoding: 2627
Adding word: section to encoding: 2628
Adding word: sections to encoding: 2629
Adding word: secures to encoding: 2630
Adding word: seeds to encoding: 2631
Adding word: select to encoding: 2632
Adding word: selections to encoding: 2633
Adding word: self to encoding: 2634
Adding word: semiclassical to encoding: 2635
Adding word: semiconductors to encoding: 2636
Adding word: semidefinite to encoding: 2637
Adding word: semigroups to encoding: 2638
Adding word: sends to encoding: 2639
Adding word: sensitivity to encoding: 2640
Adding word: separate to encoding: 2641
Adding word: separated to encoding: 2642
Adding word: separates to encoding: 2643
Adding word: separating to encoding: 2644
Adding word: separation to encoding: 2645
Adding word: separations to encoding: 2646
Adding word: separators to encoding: 2647
Adding word: sequence to encoding: 2648
Adding word: sequences to encoding: 2649
Adding word: sequentially to encoding: 2650
Adding word: serial to encoding: 2651
Adding word: series to encoding: 2652
Adding word: sessions to encoding: 2653
Adding word: set to encoding: 2654
Adding word: sets to encoding: 2655
Adding word: several to encoding: 2656
Adding word: shadow to encoding: 2657
Adding word: shapes to encoding: 2658
Adding word: share to encoding: 2659
Adding word: shared to encoding: 2660
Adding word: sharing to encoding: 2661
Adding word: shear to encoding: 2662
Adding word: shell to encoding: 2663
Adding word: shelter to encoding: 2664
Adding word: shielding to encoding: 2665
Adding word: shift to encoding: 2666
Adding word: shifted to encoding: 2667
Adding word: shifts to encoding: 2668
Adding word: shine to encoding: 2669
Adding word: short to encoding: 2670
Adding word: shortest to encoding: 2671
Adding word: show to encoding: 2672
Adding word: shows to encoding: 2673
Adding word: sigma to encoding: 2674
Adding word: signals to encoding: 2675
Adding word: signature to encoding: 2676
Adding word: significant to encoding: 2677
Adding word: similarity to encoding: 2678
Adding word: simple to encoding: 2679
Adding word: simplifies to encoding: 2680
Adding word: simplifying to encoding: 2681
Adding word: simply to encoding: 2682
Adding word: simulates to encoding: 2683
Adding word: simultaneous to encoding: 2684
Adding word: simultaneously to encoding: 2685
Adding word: sin to encoding: 2686
Adding word: sine to encoding: 2687
Adding word: sines to encoding: 2688
Adding word: single to encoding: 2689
Adding word: singular to encoding: 2690
Adding word: singularity to encoding: 2691
Adding word: sinusoidal to encoding: 2692
Adding word: size to encoding: 2693
Adding word: sized to encoding: 2694
Adding word: sky to encoding: 2695
Adding word: slackness to encoding: 2696
Adding word: slit to encoding: 2697
Adding word: slowly to encoding: 2698
Adding word: slows to encoding: 2699
Adding word: small to encoding: 2700
Adding word: smooth to encoding: 2701
Adding word: smoothness to encoding: 2702
Adding word: social to encoding: 2703
Adding word: soda to encoding: 2704
Adding word: soft to encoding: 2705
Adding word: softmax to encoding: 2706
Adding word: solar to encoding: 2707
Adding word: solid to encoding: 2708
Adding word: solids to encoding: 2709
Adding word: solution to encoding: 2710
Adding word: solutions to encoding: 2711
Adding word: solvability to encoding: 2712
Adding word: solve to encoding: 2713
Adding word: solved to encoding: 2714
Adding word: solvers to encoding: 2715
Adding word: solves to encoding: 2716
Adding word: solving to encoding: 2717
Adding word: some to encoding: 2718
Adding word: something to encoding: 2719
Adding word: sorted to encoding: 2720
Adding word: sound to encoding: 2721
Adding word: source to encoding: 2722
Adding word: space to encoding: 2723
Adding word: spaced to encoding: 2724
Adding word: spaces to encoding: 2725
Adding word: spacetime to encoding: 2726
Adding word: spacetimes to encoding: 2727
Adding word: span to encoding: 2728
Adding word: spanning to encoding: 2729
Adding word: spans to encoding: 2730
Adding word: sparsity to encoding: 2731
Adding word: spatial to encoding: 2732
Adding word: special to encoding: 2733
Adding word: species to encoding: 2734
Adding word: specific to encoding: 2735
Adding word: spectral to encoding: 2736
Adding word: spectrum to encoding: 2737
Adding word: speed to encoding: 2738
Adding word: speeds to encoding: 2739
Adding word: speedup to encoding: 2740
Adding word: sphere to encoding: 2741
Adding word: spherically to encoding: 2742
Adding word: spin to encoding: 2743
Adding word: spinning to encoding: 2744
Adding word: split to encoding: 2745
Adding word: splits to encoding: 2746
Adding word: spontaneity to encoding: 2747
Adding word: spontaneous to encoding: 2748
Adding word: spread to encoding: 2749
Adding word: spring to encoding: 2750
Adding word: sprout to encoding: 2751
Adding word: sqrt to encoding: 2752
Adding word: square to encoding: 2753
Adding word: squared to encoding: 2754
Adding word: squares to encoding: 2755
Adding word: stability to encoding: 2756
Adding word: stabilize to encoding: 2757
Adding word: stabilizes to encoding: 2758
Adding word: stable to encoding: 2759
Adding word: standard to encoding: 2760
Adding word: standardized to encoding: 2761
Adding word: star to encoding: 2762
Adding word: stars to encoding: 2763
Adding word: starting to encoding: 2764
Adding word: state to encoding: 2765
Adding word: stateless to encoding: 2766
Adding word: states to encoding: 2767
Adding word: static to encoding: 2768
Adding word: statically to encoding: 2769
Adding word: stationary to encoding: 2770
Adding word: statistical to encoding: 2771
Adding word: statistics to encoding: 2772
Adding word: steady to encoding: 2773
Adding word: stencils to encoding: 2774
Adding word: step to encoding: 2775
Adding word: stepping to encoding: 2776
Adding word: steps to encoding: 2777
Adding word: stiffness to encoding: 2778
Adding word: stochastic to encoding: 2779
Adding word: stopping to encoding: 2780
Adding word: store to encoding: 2781
Adding word: stores to encoding: 2782
Adding word: stories to encoding: 2783
Adding word: straight to encoding: 2784
Adding word: strain to encoding: 2785
Adding word: strains to encoding: 2786
Adding word: strange to encoding: 2787
Adding word: strategies to encoding: 2788
Adding word: streamline to encoding: 2789
Adding word: streamlines to encoding: 2790
Adding word: strength to encoding: 2791
Adding word: stress to encoding: 2792
Adding word: strong to encoding: 2793
Adding word: structure to encoding: 2794
Adding word: structured to encoding: 2795
Adding word: studies to encoding: 2796
Adding word: sub to encoding: 2797
Adding word: subalgebra to encoding: 2798
Adding word: subcover to encoding: 2799
Adding word: subfamilies to encoding: 2800
Adding word: subgrid to encoding: 2801
Adding word: subgroup to encoding: 2802
Adding word: subject to encoding: 2803
Adding word: subproblem to encoding: 2804
Adding word: subsequence to encoding: 2805
Adding word: subsequences to encoding: 2806
Adding word: subsets to encoding: 2807
Adding word: subspaces to encoding: 2808
Adding word: substrings to encoding: 2809
Adding word: subtracts to encoding: 2810
Adding word: sufficient to encoding: 2811
Adding word: sum to encoding: 2812
Adding word: summation to encoding: 2813
Adding word: summer to encoding: 2814
Adding word: sums to encoding: 2815
Adding word: sun to encoding: 2816
Adding word: sunlight to encoding: 2817
Adding word: superconducting to encoding: 2818
Adding word: superconductors to encoding: 2819
Adding word: superposition to encoding: 2820
Adding word: supersonic to encoding: 2821
Adding word: support to encoding: 2822
Adding word: supported to encoding: 2823
Adding word: supports to encoding: 2824
Adding word: sure to encoding: 2825
Adding word: surely to encoding: 2826
Adding word: surface to encoding: 2827
Adding word: surfaces to encoding: 2828
Adding word: surjective to encoding: 2829
Adding word: survive to encoding: 2830
Adding word: swap to encoding: 2831
Adding word: sweetened to encoding: 2832
Adding word: swim to encoding: 2833
Adding word: symbol to encoding: 2834
Adding word: symbols to encoding: 2835
Adding word: symmetric to encoding: 2836
Adding word: symmetries to encoding: 2837
Adding word: symmetry to encoding: 2838
Adding word: synthesize to encoding: 2839
Adding word: system to encoding: 2840
Adding word: systematic to encoding: 2841
Adding word: systems to encoding: 2842
Adding word: t to encoding: 2843
Adding word: tables to encoding: 2844
Adding word: tail to encoding: 2845
Adding word: tails to encoding: 2846
Adding word: take to encoding: 2847
Adding word: tall to encoding: 2848
Adding word: tangent to encoding: 2849
Adding word: tangential to encoding: 2850
Adding word: tanh to encoding: 2851
Adding word: target to encoding: 2852
Adding word: targets to encoding: 2853
Adding word: task to encoding: 2854
Adding word: tasks to encoding: 2855
Adding word: technological to encoding: 2856
Adding word: tectonic to encoding: 2857
Adding word: telescoping to encoding: 2858
Adding word: temperature to encoding: 2859
Adding word: temperatures to encoding: 2860
Adding word: tends to encoding: 2861
Adding word: tensor to encoding: 2862
Adding word: term to encoding: 2863
Adding word: terms to encoding: 2864
Adding word: test to encoding: 2865
Adding word: testing to encoding: 2866
Adding word: tests to encoding: 2867
Adding word: text to encoding: 2868
Adding word: than to encoding: 2869
Adding word: thank to encoding: 2870
Adding word: that to encoding: 2871
Adding word: the to encoding: 2872
Adding word: their to encoding: 2873
Adding word: theorem to encoding: 2874
Adding word: theorems to encoding: 2875
Adding word: theory to encoding: 2876
Adding word: there to encoding: 2877
Adding word: thermal to encoding: 2878
Adding word: thermodynamic to encoding: 2879
Adding word: thermodynamics to encoding: 2880
Adding word: thermometer to encoding: 2881
Adding word: theta to encoding: 2882
Adding word: they to encoding: 2883
Adding word: thickness to encoding: 2884
Adding word: think to encoding: 2885
Adding word: thirds to encoding: 2886
Adding word: thread to encoding: 2887
Adding word: three to encoding: 2888
Adding word: thresholding to encoding: 2889
Adding word: through to encoding: 2890
Adding word: throughout to encoding: 2891
Adding word: throughput to encoding: 2892
Adding word: thunder to encoding: 2893
Adding word: tidal to encoding: 2894
Adding word: tides to encoding: 2895
Adding word: ties to encoding: 2896
Adding word: tiling to encoding: 2897
Adding word: tilt to encoding: 2898
Adding word: time to encoding: 2899
Adding word: times to encoding: 2900
Adding word: to to encoding: 2901
Adding word: today to encoding: 2902
Adding word: token to encoding: 2903
Adding word: tokens to encoding: 2904
Adding word: tolerance to encoding: 2905
Adding word: tonight to encoding: 2906
Adding word: too to encoding: 2907
Adding word: top to encoding: 2908
Adding word: tori to encoding: 2909
Adding word: torque to encoding: 2910
Adding word: total to encoding: 2911
Adding word: toughness to encoding: 2912
Adding word: toward to encoding: 2913
Adding word: tr to encoding: 2914
Adding word: trace to encoding: 2915
Adding word: traced to encoding: 2916
Adding word: traces to encoding: 2917
Adding word: tracing to encoding: 2918
Adding word: track to encoding: 2919
Adding word: tradeoff to encoding: 2920
Adding word: trades to encoding: 2921
Adding word: traditional to encoding: 2922
Adding word: trained to encoding: 2923
Adding word: training to encoding: 2924
Adding word: trait to encoding: 2925
Adding word: traits to encoding: 2926
Adding word: trajectories to encoding: 2927
Adding word: transactions to encoding: 2928
Adding word: transfer to encoding: 2929
Adding word: transfers to encoding: 2930
Adding word: transform to encoding: 2931
Adding word: transformation to encoding: 2932
Adding word: transformations to encoding: 2933
Adding word: transformed to encoding: 2934
Adding word: transforms to encoding: 2935
Adding word: transition to encoding: 2936
Adding word: transitions to encoding: 2937
Adding word: transitive to encoding: 2938
Adding word: translational to encoding: 2939
Adding word: transmission to encoding: 2940
Adding word: transmit to encoding: 2941
Adding word: transmits to encoding: 2942
Adding word: transport to encoding: 2943
Adding word: transports to encoding: 2944
Adding word: transpose to encoding: 2945
Adding word: trapped to encoding: 2946
Adding word: travel to encoding: 2947
Adding word: traveling to encoding: 2948
Adding word: travels to encoding: 2949
Adding word: traversals to encoding: 2950
Adding word: tree to encoding: 2951
Adding word: trees to encoding: 2952
Adding word: trends to encoding: 2953
Adding word: trial to encoding: 2954
Adding word: trials to encoding: 2955
Adding word: triangle to encoding: 2956
Adding word: triangular to encoding: 2957
Adding word: trick to encoding: 2958
Adding word: trivial to encoding: 2959
Adding word: try to encoding: 2960
Adding word: turbines to encoding: 2961
Adding word: turbulence to encoding: 2962
Adding word: turbulent to encoding: 2963
Adding word: two to encoding: 2964
Adding word: type to encoding: 2965
Adding word: typed to encoding: 2966
Adding word: types to encoding: 2967
Adding word: typically to encoding: 2968
Adding word: u to encoding: 2969
Adding word: ultraviolet to encoding: 2970
Adding word: unbiased to encoding: 2971
Adding word: uncertainty to encoding: 2972
Adding word: unconstrained to encoding: 2973
Adding word: under to encoding: 2974
Adding word: underpin to encoding: 2975
Adding word: understand to encoding: 2976
Adding word: unifies to encoding: 2977
Adding word: uniform to encoding: 2978
Adding word: uniformly to encoding: 2979
Adding word: unify to encoding: 2980
Adding word: union to encoding: 2981
Adding word: unions to encoding: 2982
Adding word: unique to encoding: 2983
Adding word: uniquely to encoding: 2984
Adding word: uniqueness to encoding: 2985
Adding word: unit to encoding: 2986
Adding word: unitary to encoding: 2987
Adding word: units to encoding: 2988
Adding word: unity to encoding: 2989
Adding word: unless to encoding: 2990
Adding word: unordered to encoding: 2991
Adding word: unreadable to encoding: 2992
Adding word: unreliable to encoding: 2993
Adding word: until to encoding: 2994
Adding word: unweighted to encoding: 2995
Adding word: up to encoding: 2996
Adding word: update to encoding: 2997
Adding word: updates to encoding: 2998
Adding word: upgrades to encoding: 2999
Adding word: upper to encoding: 3000
Adding word: use to encoding: 3001
Adding word: used to encoding: 3002
Adding word: useful to encoding: 3003
Adding word: uses to encoding: 3004
Adding word: using to encoding: 3005
Adding word: v to encoding: 3006
Adding word: vaccines to encoding: 3007
Adding word: vacuum to encoding: 3008
Adding word: valence to encoding: 3009
Adding word: validation to encoding: 3010
Adding word: validity to encoding: 3011
Adding word: value to encoding: 3012
Adding word: valued to encoding: 3013
Adding word: values to encoding: 3014
Adding word: vanish to encoding: 3015
Adding word: vanishes to encoding: 3016
Adding word: vanishing to encoding: 3017
Adding word: vapor to encoding: 3018
Adding word: variability to encoding: 3019
Adding word: variable to encoding: 3020
Adding word: variables to encoding: 3021
Adding word: variance to encoding: 3022
Adding word: variances to encoding: 3023
Adding word: variants to encoding: 3024
Adding word: variation to encoding: 3025
Adding word: variational to encoding: 3026
Adding word: varies to encoding: 3027
Adding word: vary to encoding: 3028
Adding word: varying to encoding: 3029
Adding word: vast to encoding: 3030
Adding word: vector to encoding: 3031
Adding word: vectors to encoding: 3032
Adding word: vehicles to encoding: 3033
Adding word: velocity to encoding: 3034
Adding word: verbs to encoding: 3035
Adding word: verification to encoding: 3036
Adding word: versions to encoding: 3037
Adding word: versus to encoding: 3038
Adding word: vertex to encoding: 3039
Adding word: vertices to encoding: 3040
Adding word: very to encoding: 3041
Adding word: via to encoding: 3042
Adding word: vibration to encoding: 3043
Adding word: violation to encoding: 3044
Adding word: viscosity to encoding: 3045
Adding word: viscous to encoding: 3046
Adding word: visits to encoding: 3047
Adding word: visual to encoding: 3048
Adding word: visualize to encoding: 3049
Adding word: vital to encoding: 3050
Adding word: vocabularies to encoding: 3051
Adding word: vocabulary to encoding: 3052
Adding word: volcanism to encoding: 3053
Adding word: volcanoes to encoding: 3054
Adding word: voltage to encoding: 3055
Adding word: voltages to encoding: 3056
Adding word: volume to encoding: 3057
Adding word: volumes to encoding: 3058
Adding word: von to encoding: 3059
Adding word: w to encoding: 3060
Adding word: waist to encoding: 3061
Adding word: walks to encoding: 3062
Adding word: wall to encoding: 3063
Adding word: was to encoding: 3064
Adding word: waste to encoding: 3065
Adding word: water to encoding: 3066
Adding word: wave to encoding: 3067
Adding word: wavefunction to encoding: 3068
Adding word: wavelength to encoding: 3069
Adding word: waves to encoding: 3070
Adding word: weak to encoding: 3071
Adding word: weather to encoding: 3072
Adding word: weight to encoding: 3073
Adding word: weighted to encoding: 3074
Adding word: weights to encoding: 3075
Adding word: welcome to encoding: 3076
Adding word: well to encoding: 3077
Adding word: west to encoding: 3078
Adding word: what to encoding: 3079
Adding word: when to encoding: 3080
Adding word: where to encoding: 3081
Adding word: which to encoding: 3082
Adding word: while to encoding: 3083
Adding word: white to encoding: 3084
Adding word: whole to encoding: 3085
Adding word: wind to encoding: 3086
Adding word: winds to encoding: 3087
Adding word: wings to encoding: 3088
Adding word: wise to encoding: 3089
Adding word: with to encoding: 3090
Adding word: within to encoding: 3091
Adding word: without to encoding: 3092
Adding word: words to encoding: 3093
Adding word: work to encoding: 3094
Adding word: works to encoding: 3095
Adding word: world to encoding: 3096
Adding word: worst to encoding: 3097
Adding word: would to encoding: 3098
Adding word: writes to encoding: 3099
Adding word: written to encoding: 3100
Adding word: x to encoding: 3101
Adding word: x╠ç to encoding: 3102
Adding word: y to encoding: 3103
Adding word: yards to encoding: 3104
Adding word: year to encoding: 3105
Adding word: years to encoding: 3106
Adding word: yeast to encoding: 3107
Adding word: yield to encoding: 3108
Adding word: yielding to encoding: 3109
Adding word: yields to encoding: 3110
Adding word: you to encoding: 3111
Adding word: zero to encoding: 3112
Adding word: zeta to encoding: 3113
Adding word: zone to encoding: 3114
Adding word: { to encoding: 3115
Adding word: | to encoding: 3116
Adding word: } to encoding: 3117
Adding word: ~ to encoding: 3118
Adding word: ┬╖ to encoding: 3119
Adding word: ├ù to encoding: 3120
Adding word: ├╗ to encoding: 3121
Adding word: ├╗0 to encoding: 3122
Adding word: ─ñ to encoding: 3123
Adding word: ─º to encoding: 3124
Adding word: ╬ö to encoding: 3125
Adding word: ╬öH to encoding: 3126
Adding word: ╬öV to encoding: 3127
Adding word: ╬¢ to encoding: 3128
Adding word: ╬┤ to encoding: 3129
Adding word: ╬╗ to encoding: 3130
Adding word: ╬╝╬╜ to encoding: 3131
Adding word: ╬╛ to encoding: 3132
Adding word: ╧Ç to encoding: 3133
Adding word: ╧ü to encoding: 3134
Adding word: ╧â to encoding: 3135
Adding word: ╧ê to encoding: 3136
Adding word: ╧ë to encoding: 3137
Adding word: ΓäÆ to encoding: 3138
Adding word: ΓåÆ to encoding: 3139
Adding word: Γåö to encoding: 3140
Adding word: ΓêéA to encoding: 3141
Adding word: ΓêéH to encoding: 3142
Adding word: ΓêéL to encoding: 3143
Adding word: Γêép to encoding: 3144
Adding word: Γêéq to encoding: 3145
Adding word: Γêét to encoding: 3146
Adding word: Γêéw to encoding: 3147
Adding word: Γêéx to encoding: 3148
Adding word: Γêéx╠ç to encoding: 3149
Adding word: Γêç to encoding: 3150
Adding word: Γêçf to encoding: 3151
Adding word: Γêçg to encoding: 3152
Adding word: Γêç┬╖F to encoding: 3153
Adding word: Γêæ to encoding: 3154
Adding word: ΓêÆ to encoding: 3155
Adding word: ΓêÆ2 to encoding: 3156
Adding word: ΓêÆ5 to encoding: 3157
Adding word: ΓêÆt to encoding: 3158
Adding word: ΓêÆ╬▓W to encoding: 3159
Adding word: ΓêÆ╬▓╬öF to encoding: 3160
Adding word: ΓêÆΓêéH to encoding: 3161
Adding word: ΓêÜΓêÆg to encoding: 3162
Adding word: Γê¥ to encoding: 3163
Adding word: Γê½ to encoding: 3164
Adding word: Γê½╬⌐ to encoding: 3165
Adding word: Γê«ΓêéM to encoding: 3166
Adding word: Γê«Γêé╬⌐ to encoding: 3167
Adding word: Γëí to encoding: 3168
Adding word: Γëñ to encoding: 3169
Adding word: ΓëÑ to encoding: 3170
Adding word: Γƒ¿e to encoding: 3171
Adding word: Γƒ⌐ to encoding: 3172

ΓòöΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòù
Γòæ          MODEL ARCHITECTURE SUMMARY                            Γòæ
ΓòÜΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓò¥

≡ƒôÉ Base Configuration:
  Architecture Type: Transformer
  Embedding Dimension: 128
  Hidden Dimension: 256
  Number of Layers: 3
  Max Sequence Length: 80

≡ƒÜÇ Modern LLM Enhancements:
  Γ£ô DynamicTanhNorm (adaptive, tanh-based)
  Γ£ô SwiGLU (gated activation, no bias)
  Γ£ô CoPE (context-aware, max_pos=64, best performance)
  Γ£ô Group-Query Attention (GQA)
    - Query Heads: 8
    - KV Heads: 4
    - Queries per KV: 2
    - KV Cache Reduction: ~50%
  Γ£ô Adaptive Sliding Window Attention (Phase 4)
    - Strategy: AttentionEntropy
    - Window Range: 512 - 4096
    - Base Window: 4096
    - Entropy EMA Alpha: 0.2
    - Adapts dynamically based on context
  Γ£ô Fully Adaptive Mixture-of-Heads - Complexity-Aware Routing
    - Total Heads: 8 (all are routing candidates)
    - Min Heads: 1 (for simple inputs)
    - Max Heads: 8 (for complex inputs)
    - Estimated Avg Active: 4.5/8 heads
    - Complexity Predictor: ENABLED
      ΓåÆ Learns to predict input complexity ΓåÆ target head count
    - Threshold Predictor: ENABLED
      ΓåÆ Learns per-token threshold for top-p selection
    - Load Balance Weight: 0.1
    - Complexity Loss Weight: 0.1
    - Sparsity Weight: 0.01
    - Estimated Compute Savings: ~43%
    - Expected Efficiency Gain: 15-25% (vs 5-8% for standard MoH)

≡ƒÄ» Architecture Alignment:
  Custom Configuration with CoPE (Research/Experimental) ≡ƒö¼

≡ƒôè Layer Stack:
  0: Embeddings
  1: MultiHeadSelfAttention
  2: DynamicTanhNorm
  3: SwiGLU
  4: DynamicTanhNorm
  5: MultiHeadSelfAttention
  6: DynamicTanhNorm
  7: SwiGLU
  8: DynamicTanhNorm
  9: MultiHeadSelfAttention
  10: DynamicTanhNorm
  11: SwiGLU
  12: DynamicTanhNorm
  13: OutputProjection

≡ƒÆ╛ Total Parameters: 1136271

ΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉΓòÉ


=== MODEL INFORMATION ===
Network architecture: Embeddings, MultiHeadSelfAttention, DynamicTanhNorm, SwiGLU, DynamicTanhNorm, MultiHeadSelfAttention, DynamicTanhNorm, SwiGLU, DynamicTanhNorm, MultiHeadSelfAttention, DynamicTanhNorm, SwiGLU, DynamicTanhNorm, OutputProjection
Total parameters: 1136271

=== BEFORE TRAINING ===
Input: User: How do mountains form?
Output: support immutable libraries semiclassical decays explains duality action results Damped physical Cram├⌐r 0 asymptotically Ampere ChernΓÇôWeil connection Algorithms times millimeters Minkowski hulls barotropic Probabilistic Chocolate kernel perspectives by shift integral falls HMMs too fluctuations nonnegative head logs ChernΓÇôWeil mixed coherence ΓêéL shortest ChernΓÇôWeil mixed coherence equate elliptic Conduction matrix harnessed hypersurfaces indicating byproduct GreenΓÇôKubo Geology nervous technological turbines ultraviolet ╧ê Convection global pairing explores explores ordering organizes tonight conductor mixed Parallel studies gap

=== PRE-TRAINING MODEL ===
Pre-training on 833 examples for 100 epochs with learning rate 0.0005
[2m2025-10-21T17:40:27.759967Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 1/15) | MoH L1: 4.70h@0.55p | L5: 4.49h@0.55p | L9: 4.67h@0.57p | ThreshRange: [0.55-0.57] | PredNorm: 1.8750 | Complexity: 0.517 [0.412-0.652] | Temp: 5.49 [5.02-5.99] [3mepoch[0m[2m=[0m0 [3mloss[0m[2m=[0m8.101804733276367 [3mgrad_norm[0m[2m=[0m1.0849250555038452 [3mlearning_rate[0m[2m=[0m6.666667468380183e-5
[2m2025-10-21T17:40:30.957854Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 2/15) | MoH L1: 4.69h@0.55p | L5: 4.48h@0.55p | L9: 4.65h@0.57p | ThreshRange: [0.55-0.57] | PredNorm: 1.8786 | Complexity: 0.516 [0.412-0.647] | Temp: 5.49 [5.03-5.97] [3mepoch[0m[2m=[0m1 [3mloss[0m[2m=[0m8.041791915893555 [3mgrad_norm[0m[2m=[0m1.0552356243133545 [3mlearning_rate[0m[2m=[0m0.00013333334936760366
[2m2025-10-21T17:40:34.088468Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 3/15) | MoH L1: 4.69h@0.55p | L5: 4.48h@0.55p | L9: 4.64h@0.57p | ThreshRange: [0.55-0.57] | PredNorm: 1.8851 | Complexity: 0.515 [0.411-0.647] | Temp: 5.50 [5.04-5.95] [3mepoch[0m[2m=[0m2 [3mloss[0m[2m=[0m7.941800117492676 [3mgrad_norm[0m[2m=[0m1.207127571105957 [3mlearning_rate[0m[2m=[0m0.00020000000949949026
[2m2025-10-21T17:40:37.330938Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 4/15) | MoH L1: 4.68h@0.55p | L5: 4.49h@0.55p | L9: 4.67h@0.56p | ThreshRange: [0.55-0.56] | PredNorm: 1.8960 | Complexity: 0.516 [0.406-0.645] | Temp: 5.52 [5.05-5.97] [3mepoch[0m[2m=[0m3 [3mloss[0m[2m=[0m7.71688175201416 [3mgrad_norm[0m[2m=[0m2.025247097015381 [3mlearning_rate[0m[2m=[0m0.0002666666987352073
[2m2025-10-21T17:40:40.584537Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 5/15) | MoH L1: 4.67h@0.55p | L5: 4.51h@0.55p | L9: 4.77h@0.54p | ThreshRange: [0.54-0.55] | PredNorm: 1.9149 | Complexity: 0.521 [0.404-0.663] | Temp: 5.62 [5.07-6.21] [3mepoch[0m[2m=[0m4 [3mloss[0m[2m=[0m7.220495223999023 [3mgrad_norm[0m[2m=[0m2.419517993927002 [3mlearning_rate[0m[2m=[0m0.0003333333588670939
[2m2025-10-21T17:40:43.811404Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 6/15) | MoH L1: 4.67h@0.55p | L5: 4.49h@0.54p | L9: 4.66h@0.54p | ThreshRange: [0.54-0.55] | PredNorm: 1.9480 | Complexity: 0.515 [0.392-0.660] | Temp: 5.61 [5.09-6.21] [3mepoch[0m[2m=[0m5 [3mloss[0m[2m=[0m6.799187660217285 [3mgrad_norm[0m[2m=[0m1.45683753490448 [3mlearning_rate[0m[2m=[0m0.0004000000189989805
[2m2025-10-21T17:40:46.924535Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 7/15) | MoH L1: 4.67h@0.55p | L5: 4.52h@0.54p | L9: 4.49h@0.54p | ThreshRange: [0.54-0.55] | PredNorm: 2.0001 | Complexity: 0.509 [0.394-0.656] | Temp: 5.61 [5.09-6.13] [3mepoch[0m[2m=[0m6 [3mloss[0m[2m=[0m6.478518009185791 [3mgrad_norm[0m[2m=[0m1.7208961248397827 [3mlearning_rate[0m[2m=[0m0.0004666666791308671
[2m2025-10-21T17:40:50.383686Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 8/15) | MoH L1: 4.66h@0.55p | L5: 4.51h@0.54p | L9: 4.38h@0.55p | ThreshRange: [0.54-0.55] | PredNorm: 2.0689 | Complexity: 0.503 [0.392-0.651] | Temp: 5.54 [4.97-5.99] [3mepoch[0m[2m=[0m7 [3mloss[0m[2m=[0m6.30422306060791 [3mgrad_norm[0m[2m=[0m1.872771978378296 [3mlearning_rate[0m[2m=[0m0.0005333333974704146
[2m2025-10-21T17:40:53.702688Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 9/15) | MoH L1: 4.66h@0.55p | L5: 4.54h@0.54p | L9: 4.36h@0.55p | ThreshRange: [0.54-0.55] | PredNorm: 2.1502 | Complexity: 0.503 [0.395-0.662] | Temp: 5.53 [4.86-6.01] [3mepoch[0m[2m=[0m8 [3mloss[0m[2m=[0m6.05023193359375 [3mgrad_norm[0m[2m=[0m1.2660200595855713 [3mlearning_rate[0m[2m=[0m0.0006000000284984708
[2m2025-10-21T17:40:56.983528Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 10/15) | MoH L1: 4.66h@0.55p | L5: 4.57h@0.54p | L9: 4.41h@0.56p | ThreshRange: [0.54-0.56] | PredNorm: 2.2461 | Complexity: 0.507 [0.394-0.681] | Temp: 5.52 [4.71-6.06] [3mepoch[0m[2m=[0m9 [3mloss[0m[2m=[0m5.857752323150635 [3mgrad_norm[0m[2m=[0m1.966021180152893 [3mlearning_rate[0m[2m=[0m0.0006666667177341878
[2m2025-10-21T17:41:00.529420Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 11/15) | MoH L1: 4.66h@0.55p | L5: 4.61h@0.54p | L9: 4.30h@0.56p | ThreshRange: [0.54-0.56] | PredNorm: 2.3584 | Complexity: 0.504 [0.369-0.676] | Temp: 5.48 [4.66-6.07] [3mepoch[0m[2m=[0m10 [3mloss[0m[2m=[0m5.595279693603516 [3mgrad_norm[0m[2m=[0m2.1043541431427 [3mlearning_rate[0m[2m=[0m0.0007333334069699049
[2m2025-10-21T17:41:04.009704Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 12/15) | MoH L1: 4.66h@0.56p | L5: 4.63h@0.54p | L9: 4.42h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 2.4881 | Complexity: 0.510 [0.327-0.706] | Temp: 5.44 [4.19-6.13] [3mepoch[0m[2m=[0m11 [3mloss[0m[2m=[0m5.248995780944824 [3mgrad_norm[0m[2m=[0m2.2626612186431885 [3mlearning_rate[0m[2m=[0m0.000800000037997961
[2m2025-10-21T17:41:07.429207Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 13/15) | MoH L1: 4.66h@0.56p | L5: 4.62h@0.54p | L9: 4.20h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 2.6399 | Complexity: 0.499 [0.293-0.703] | Temp: 5.38 [3.62-6.21] [3mepoch[0m[2m=[0m12 [3mloss[0m[2m=[0m4.839089870452881 [3mgrad_norm[0m[2m=[0m3.2924396991729736 [3mlearning_rate[0m[2m=[0m0.0008666667272336781
[2m2025-10-21T17:41:07.786775Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.38", "L1: 10.38", "L2: 5.21", "L3: 4.82", "L4: 3.36", "L5: 3.17", "L6: 2.02", "L7: 1.91", "L8: 0.98", "L9: 1.06", "L10: 0.72", "L11: 0.83", "L12: 0.28", "L13: 0.50"]
[2m2025-10-21T17:41:10.085747Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 14/15) | MoH L1: 4.66h@0.56p | L5: 4.64h@0.54p | L9: 4.32h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 2.8082 | Complexity: 0.506 [0.315-0.744] | Temp: 5.37 [3.62-6.24] [3mepoch[0m[2m=[0m13 [3mloss[0m[2m=[0m4.435826778411865 [3mgrad_norm[0m[2m=[0m4.579353332519531 [3mlearning_rate[0m[2m=[0m0.0009333333582617342
[2m2025-10-21T17:41:10.402678Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.02", "L1: 15.02", "L2: 7.29", "L3: 6.64", "L4: 4.36", "L5: 4.07", "L6: 2.50", "L7: 2.33", "L8: 1.13", "L9: 1.22", "L10: 0.78", "L11: 0.89", "L12: 0.29", "L13: 0.51"]
[2m2025-10-21T17:41:10.747364Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.80", "L1: 12.80", "L2: 6.20", "L3: 5.63", "L4: 3.78", "L5: 3.51", "L6: 2.22", "L7: 2.05", "L8: 1.02", "L9: 1.08", "L10: 0.70", "L11: 0.79", "L12: 0.26", "L13: 0.46"]
[2m2025-10-21T17:41:11.119797Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.33", "L1: 10.33", "L2: 5.02", "L3: 4.55", "L4: 3.13", "L5: 2.89", "L6: 1.92", "L7: 1.74", "L8: 0.94", "L9: 0.95", "L10: 0.67", "L11: 0.71", "L12: 0.25", "L13: 0.41"]
[2m2025-10-21T17:41:11.468913Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.34", "L1: 11.34", "L2: 5.46", "L3: 4.95", "L4: 3.35", "L5: 3.09", "L6: 2.00", "L7: 1.83", "L8: 0.97", "L9: 1.00", "L10: 0.68", "L11: 0.74", "L12: 0.25", "L13: 0.41"]
[2m2025-10-21T17:41:11.862845Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.15", "L1: 13.15", "L2: 6.05", "L3: 5.47", "L4: 3.63", "L5: 3.33", "L6: 2.09", "L7: 1.90", "L8: 1.01", "L9: 1.02", "L10: 0.68", "L11: 0.73", "L12: 0.24", "L13: 0.40"]
[2m2025-10-21T17:41:12.262669Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.54", "L1: 10.54", "L2: 5.05", "L3: 4.56", "L4: 3.12", "L5: 2.86", "L6: 1.94", "L7: 1.75", "L8: 0.98", "L9: 0.97", "L10: 0.68", "L11: 0.71", "L12: 0.24", "L13: 0.38"]
[2m2025-10-21T17:41:12.452288Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.87", "L1: 10.87", "L2: 5.24", "L3: 4.73", "L4: 3.24", "L5: 2.97", "L6: 1.94", "L7: 1.76", "L8: 0.96", "L9: 0.98", "L10: 0.66", "L11: 0.70", "L12: 0.23", "L13: 0.38"]
[2m2025-10-21T17:41:12.465404Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 15/15) | MoH L1: 4.65h@0.56p | L5: 4.63h@0.54p | L9: 4.09h@0.56p | ThreshRange: [0.54-0.56] | PredNorm: 2.9750 | Complexity: 0.494 [0.225-0.734] | Temp: 5.39 [3.37-6.11] [3mepoch[0m[2m=[0m14 [3mloss[0m[2m=[0m4.118569850921631 [3mgrad_norm[0m[2m=[0m7.944947242736816 [3mlearning_rate[0m[2m=[0m0.0010000000474974513
[2m2025-10-21T17:41:12.747401Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.48", "L1: 16.48", "L2: 7.99", "L3: 7.20", "L4: 4.83", "L5: 4.43", "L6: 2.75", "L7: 2.51", "L8: 1.23", "L9: 1.31", "L10: 0.81", "L11: 0.91", "L12: 0.27", "L13: 0.49"]
[2m2025-10-21T17:41:13.074666Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.81", "L1: 13.81", "L2: 6.82", "L3: 6.13", "L4: 4.23", "L5: 3.87", "L6: 2.49", "L7: 2.25", "L8: 1.17", "L9: 1.20", "L10: 0.76", "L11: 0.83", "L12: 0.25", "L13: 0.45"]
[2m2025-10-21T17:41:13.442466Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.28", "L1: 12.28", "L2: 5.96", "L3: 5.35", "L4: 3.70", "L5: 3.37", "L6: 2.20", "L7: 1.97", "L8: 1.08", "L9: 1.07", "L10: 0.72", "L11: 0.76", "L12: 0.25", "L13: 0.41"]
[2m2025-10-21T17:41:13.814115Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.25", "L1: 13.25", "L2: 6.39", "L3: 5.73", "L4: 3.95", "L5: 3.59", "L6: 2.34", "L7: 2.09", "L8: 1.13", "L9: 1.12", "L10: 0.75", "L11: 0.78", "L12: 0.25", "L13: 0.41"]
[2m2025-10-21T17:41:14.214935Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.78", "L1: 11.78", "L2: 5.83", "L3: 5.22", "L4: 3.66", "L5: 3.32", "L6: 2.18", "L7: 1.94", "L8: 1.07", "L9: 1.05", "L10: 0.71", "L11: 0.74", "L12: 0.24", "L13: 0.39"]
[2m2025-10-21T17:41:14.598728Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.48", "L1: 10.48", "L2: 5.28", "L3: 4.72", "L4: 3.35", "L5: 3.03", "L6: 2.08", "L7: 1.85", "L8: 1.04", "L9: 1.00", "L10: 0.70", "L11: 0.72", "L12: 0.24", "L13: 0.37"]
[2m2025-10-21T17:41:14.794415Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.16", "L1: 12.16", "L2: 5.88", "L3: 5.24", "L4: 3.57", "L5: 3.24", "L6: 2.12", "L7: 1.88", "L8: 1.01", "L9: 1.01", "L10: 0.67", "L11: 0.70", "L12: 0.22", "L13: 0.37"]
[2m2025-10-21T17:41:14.808063Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.65h@0.56p | L5: 4.55h@0.54p | L9: 4.27h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 3.1360 | Complexity: 0.498 [0.315-0.744] | Temp: 5.40 [3.41-6.15] [3mepoch[0m[2m=[0m15 [3mloss[0m[2m=[0m3.6744229793548584 [3mgrad_norm[0m[2m=[0m7.2284040451049805 [3mlearning_rate[0m[2m=[0m0.0010000000474974513
[2m2025-10-21T17:41:15.109480Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.25", "L1: 20.25", "L2: 9.61", "L3: 8.56", "L4: 5.64", "L5: 5.11", "L6: 3.17", "L7: 2.84", "L8: 1.38", "L9: 1.44", "L10: 0.86", "L11: 0.96", "L12: 0.27", "L13: 0.49"]
[2m2025-10-21T17:41:15.453077Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.90", "L1: 15.90", "L2: 7.78", "L3: 6.91", "L4: 4.77", "L5: 4.30", "L6: 2.81", "L7: 2.49", "L8: 1.26", "L9: 1.29", "L10: 0.80", "L11: 0.87", "L12: 0.25", "L13: 0.44"]
[2m2025-10-21T17:41:15.848852Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.25", "L1: 13.25", "L2: 6.62", "L3: 5.87", "L4: 4.10", "L5: 3.69", "L6: 2.47", "L7: 2.17", "L8: 1.16", "L9: 1.14", "L10: 0.75", "L11: 0.78", "L12: 0.25", "L13: 0.40"]
[2m2025-10-21T17:41:16.210532Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.40", "L1: 14.40", "L2: 7.10", "L3: 6.30", "L4: 4.36", "L5: 3.91", "L6: 2.60", "L7: 2.29", "L8: 1.21", "L9: 1.19", "L10: 0.76", "L11: 0.80", "L12: 0.25", "L13: 0.41"]
[2m2025-10-21T17:41:16.656576Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.69", "L1: 13.69", "L2: 6.86", "L3: 6.06", "L4: 4.22", "L5: 3.77", "L6: 2.54", "L7: 2.22", "L8: 1.18", "L9: 1.16", "L10: 0.74", "L11: 0.78", "L12: 0.24", "L13: 0.39"]
[2m2025-10-21T17:41:17.082745Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.33", "L1: 13.33", "L2: 6.70", "L3: 5.91", "L4: 4.11", "L5: 3.67", "L6: 2.55", "L7: 2.22", "L8: 1.18", "L9: 1.15", "L10: 0.74", "L11: 0.76", "L12: 0.23", "L13: 0.37"]
[2m2025-10-21T17:41:17.305791Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.82", "L1: 14.82", "L2: 7.13", "L3: 6.27", "L4: 4.21", "L5: 3.77", "L6: 2.48", "L7: 2.18", "L8: 1.10", "L9: 1.12", "L10: 0.66", "L11: 0.70", "L12: 0.20", "L13: 0.35"]
[2m2025-10-21T17:41:17.322096Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.65h@0.56p | L5: 4.59h@0.54p | L9: 4.29h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 3.2902 | Complexity: 0.501 [0.292-0.744] | Temp: 5.40 [3.90-6.17] [3mepoch[0m[2m=[0m16 [3mloss[0m[2m=[0m3.220470905303955 [3mgrad_norm[0m[2m=[0m8.439345359802246 [3mlearning_rate[0m[2m=[0m0.0009996927110478282
[2m2025-10-21T17:41:17.644839Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.43", "L1: 20.43", "L2: 9.83", "L3: 8.65", "L4: 5.77", "L5: 5.17", "L6: 3.27", "L7: 2.89", "L8: 1.34", "L9: 1.41", "L10: 0.79", "L11: 0.90", "L12: 0.23", "L13: 0.45"]
[2m2025-10-21T17:41:17.982490Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.02", "L1: 18.02", "L2: 8.99", "L3: 7.89", "L4: 5.32", "L5: 4.74", "L6: 3.18", "L7: 2.79", "L8: 1.31", "L9: 1.36", "L10: 0.78", "L11: 0.88", "L12: 0.22", "L13: 0.43"]
[2m2025-10-21T17:41:18.348200Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.93", "L1: 14.93", "L2: 7.57", "L3: 6.63", "L4: 4.54", "L5: 4.04", "L6: 2.77", "L7: 2.40", "L8: 1.22", "L9: 1.21", "L10: 0.74", "L11: 0.79", "L12: 0.23", "L13: 0.39"]
[2m2025-10-21T17:41:18.698675Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.22", "L1: 16.22", "L2: 8.20", "L3: 7.17", "L4: 4.91", "L5: 4.35", "L6: 2.95", "L7: 2.55", "L8: 1.29", "L9: 1.29", "L10: 0.76", "L11: 0.81", "L12: 0.23", "L13: 0.40"]
[2m2025-10-21T17:41:19.118517Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.39", "L1: 16.39", "L2: 8.10", "L3: 7.08", "L4: 4.89", "L5: 4.33", "L6: 2.88", "L7: 2.50", "L8: 1.26", "L9: 1.25", "L10: 0.75", "L11: 0.80", "L12: 0.23", "L13: 0.38"]
[2m2025-10-21T17:41:19.536694Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.80", "L1: 15.80", "L2: 7.91", "L3: 6.90", "L4: 4.79", "L5: 4.24", "L6: 2.94", "L7: 2.53", "L8: 1.31", "L9: 1.27", "L10: 0.76", "L11: 0.79", "L12: 0.23", "L13: 0.37"]
[2m2025-10-21T17:41:19.732389Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.10", "L1: 15.10", "L2: 7.30", "L3: 6.35", "L4: 4.27", "L5: 3.78", "L6: 2.54", "L7: 2.20", "L8: 1.10", "L9: 1.11", "L10: 0.64", "L11: 0.69", "L12: 0.19", "L13: 0.33"]
[2m2025-10-21T17:41:19.745439Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.58h@0.54p | L9: 4.52h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 3.4446 | Complexity: 0.511 [0.277-0.764] | Temp: 5.42 [4.05-6.38] [3mepoch[0m[2m=[0m17 [3mloss[0m[2m=[0m2.760014533996582 [3mgrad_norm[0m[2m=[0m8.667204856872559 [3mlearning_rate[0m[2m=[0m0.0009987711673602462
[2m2025-10-21T17:41:20.026009Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.80", "L1: 20.80", "L2: 10.20", "L3: 8.87", "L4: 5.79", "L5: 5.13", "L6: 3.29", "L7: 2.88", "L8: 1.31", "L9: 1.37", "L10: 0.76", "L11: 0.86", "L12: 0.23", "L13: 0.41"]
[2m2025-10-21T17:41:20.351906Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.45", "L1: 19.45", "L2: 9.71", "L3: 8.43", "L4: 5.65", "L5: 4.98", "L6: 3.33", "L7: 2.88", "L8: 1.34", "L9: 1.38", "L10: 0.76", "L11: 0.85", "L12: 0.21", "L13: 0.40"]
[2m2025-10-21T17:41:20.712919Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.67", "L1: 19.67", "L2: 9.53", "L3: 8.26", "L4: 5.62", "L5: 4.94", "L6: 3.28", "L7: 2.81", "L8: 1.38", "L9: 1.36", "L10: 0.78", "L11: 0.84", "L12: 0.22", "L13: 0.38"]
[2m2025-10-21T17:41:21.072804Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.97", "L1: 19.97", "L2: 9.67", "L3: 8.37", "L4: 5.64", "L5: 4.95", "L6: 3.33", "L7: 2.85", "L8: 1.39", "L9: 1.39", "L10: 0.79", "L11: 0.86", "L12: 0.22", "L13: 0.39"]
[2m2025-10-21T17:41:21.522596Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.18", "L1: 18.18", "L2: 8.92", "L3: 7.71", "L4: 5.21", "L5: 4.56", "L6: 3.10", "L7: 2.65", "L8: 1.29", "L9: 1.28", "L10: 0.73", "L11: 0.79", "L12: 0.21", "L13: 0.36"]
[2m2025-10-21T17:41:21.913787Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.85", "L1: 16.85", "L2: 8.63", "L3: 7.45", "L4: 5.12", "L5: 4.47", "L6: 3.16", "L7: 2.69", "L8: 1.35", "L9: 1.31", "L10: 0.75", "L11: 0.79", "L12: 0.22", "L13: 0.36"]
[2m2025-10-21T17:41:22.099287Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.21", "L1: 15.21", "L2: 7.46", "L3: 6.43", "L4: 4.31", "L5: 3.78", "L6: 2.55", "L7: 2.18", "L8: 1.05", "L9: 1.06", "L10: 0.59", "L11: 0.64", "L12: 0.17", "L13: 0.29"]
[2m2025-10-21T17:41:22.112323Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.60h@0.54p | L9: 4.50h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 3.5988 | Complexity: 0.511 [0.254-0.757] | Temp: 5.41 [3.99-6.28] [3mepoch[0m[2m=[0m18 [3mloss[0m[2m=[0m2.359656572341919 [3mgrad_norm[0m[2m=[0m9.195830345153809 [3mlearning_rate[0m[2m=[0m0.0009972366970032454
[2m2025-10-21T17:41:22.398699Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.49", "L1: 23.49", "L2: 11.41", "L3: 9.83", "L4: 6.41", "L5: 5.63", "L6: 3.57", "L7: 3.09", "L8: 1.38", "L9: 1.46", "L10: 0.76", "L11: 0.88", "L12: 0.22", "L13: 0.40"]
[2m2025-10-21T17:41:22.720306Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.06", "L1: 23.06", "L2: 11.51", "L3: 9.89", "L4: 6.58", "L5: 5.75", "L6: 3.84", "L7: 3.28", "L8: 1.47", "L9: 1.52", "L10: 0.79", "L11: 0.90", "L12: 0.22", "L13: 0.39"]
[2m2025-10-21T17:41:23.104944Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.31", "L1: 18.31", "L2: 9.23", "L3: 7.92", "L4: 5.40", "L5: 4.70", "L6: 3.24", "L7: 2.75", "L8: 1.35", "L9: 1.32", "L10: 0.74", "L11: 0.80", "L12: 0.21", "L13: 0.36"]
[2m2025-10-21T17:41:23.466253Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.42", "L1: 18.42", "L2: 9.44", "L3: 8.10", "L4: 5.58", "L5: 4.84", "L6: 3.32", "L7: 2.82", "L8: 1.38", "L9: 1.35", "L10: 0.74", "L11: 0.80", "L12: 0.21", "L13: 0.36"]
[2m2025-10-21T17:41:23.863828Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.38", "L1: 19.38", "L2: 9.74", "L3: 8.33", "L4: 5.64", "L5: 4.88", "L6: 3.32", "L7: 2.81", "L8: 1.35", "L9: 1.32", "L10: 0.72", "L11: 0.78", "L12: 0.20", "L13: 0.34"]
[2m2025-10-21T17:41:24.245438Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.20", "L1: 19.20", "L2: 9.89", "L3: 8.46", "L4: 5.80", "L5: 5.01", "L6: 3.53", "L7: 2.97", "L8: 1.47", "L9: 1.42", "L10: 0.76", "L11: 0.81", "L12: 0.21", "L13: 0.35"]
[2m2025-10-21T17:41:24.426606Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.58", "L1: 15.58", "L2: 7.69", "L3: 6.56", "L4: 4.32", "L5: 3.75", "L6: 2.54", "L7: 2.16", "L8: 1.00", "L9: 1.02", "L10: 0.52", "L11: 0.57", "L12: 0.14", "L13: 0.26"]
[2m2025-10-21T17:41:24.439406Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.62h@0.54p | L9: 4.42h@0.51p | ThreshRange: [0.51-0.56] | PredNorm: 3.7524 | Complexity: 0.509 [0.274-0.768] | Temp: 5.47 [3.93-6.49] [3mepoch[0m[2m=[0m19 [3mloss[0m[2m=[0m1.9944673776626587 [3mgrad_norm[0m[2m=[0m8.759946823120117 [3mlearning_rate[0m[2m=[0m0.0009950912790372968
[2m2025-10-21T17:41:24.725765Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.07", "L1: 23.07", "L2: 11.40", "L3: 9.72", "L4: 6.32", "L5: 5.48", "L6: 3.58", "L7: 3.06", "L8: 1.30", "L9: 1.38", "L10: 0.68", "L11: 0.80", "L12: 0.18", "L13: 0.35"]
[2m2025-10-21T17:41:25.074286Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.91", "L1: 21.91", "L2: 11.04", "L3: 9.39", "L4: 6.27", "L5: 5.41", "L6: 3.65", "L7: 3.09", "L8: 1.35", "L9: 1.41", "L10: 0.71", "L11: 0.82", "L12: 0.19", "L13: 0.36"]
[2m2025-10-21T17:41:25.442604Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.69", "L1: 20.69", "L2: 10.53", "L3: 8.95", "L4: 6.08", "L5: 5.23", "L6: 3.59", "L7: 3.01", "L8: 1.40", "L9: 1.39", "L10: 0.72", "L11: 0.80", "L12: 0.19", "L13: 0.34"]
[2m2025-10-21T17:41:25.784283Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.02", "L1: 21.02", "L2: 10.80", "L3: 9.17", "L4: 6.22", "L5: 5.34", "L6: 3.68", "L7: 3.08", "L8: 1.44", "L9: 1.43", "L10: 0.73", "L11: 0.81", "L12: 0.19", "L13: 0.35"]
[2m2025-10-21T17:41:26.190939Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.37", "L1: 19.37", "L2: 9.91", "L3: 8.39", "L4: 5.71", "L5: 4.89", "L6: 3.38", "L7: 2.83", "L8: 1.32", "L9: 1.31", "L10: 0.67", "L11: 0.74", "L12: 0.18", "L13: 0.31"]
[2m2025-10-21T17:41:26.589922Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.59", "L1: 20.59", "L2: 10.53", "L3: 8.90", "L4: 6.10", "L5: 5.21", "L6: 3.71", "L7: 3.09", "L8: 1.49", "L9: 1.44", "L10: 0.73", "L11: 0.80", "L12: 0.20", "L13: 0.33"]
[2m2025-10-21T17:41:26.795247Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.27", "L1: 13.27", "L2: 6.76", "L3: 5.70", "L4: 3.74", "L5: 3.22", "L6: 2.21", "L7: 1.86", "L8: 0.84", "L9: 0.86", "L10: 0.43", "L11: 0.49", "L12: 0.12", "L13: 0.21"]
[2m2025-10-21T17:41:26.808328Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.56h@0.54p | L9: 4.37h@0.51p | ThreshRange: [0.51-0.56] | PredNorm: 3.9099 | Complexity: 0.503 [0.251-0.773] | Temp: 5.39 [3.68-6.37] [3mepoch[0m[2m=[0m20 [3mloss[0m[2m=[0m1.6284736394882202 [3mgrad_norm[0m[2m=[0m7.688793659210205 [3mlearning_rate[0m[2m=[0m0.000992337940260768
[2m2025-10-21T17:41:27.115476Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.93", "L1: 24.93", "L2: 12.16", "L3: 10.25", "L4: 6.44", "L5: 5.54", "L6: 3.52", "L7: 3.00", "L8: 1.25", "L9: 1.33", "L10: 0.65", "L11: 0.78", "L12: 0.18", "L13: 0.33"]
[2m2025-10-21T17:41:27.464245Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.82", "L1: 25.82", "L2: 12.83", "L3: 10.80", "L4: 6.98", "L5: 5.98", "L6: 3.99", "L7: 3.35", "L8: 1.41", "L9: 1.47", "L10: 0.70", "L11: 0.83", "L12: 0.19", "L13: 0.34"]
[2m2025-10-21T17:41:27.860187Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.22", "L1: 21.22", "L2: 10.92", "L3: 9.19", "L4: 6.08", "L5: 5.19", "L6: 3.60", "L7: 3.00", "L8: 1.37", "L9: 1.35", "L10: 0.68", "L11: 0.77", "L12: 0.18", "L13: 0.32"]
[2m2025-10-21T17:41:28.320724Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.63", "L1: 22.63", "L2: 11.47", "L3: 9.63", "L4: 6.43", "L5: 5.46", "L6: 3.75", "L7: 3.12", "L8: 1.41", "L9: 1.40", "L10: 0.67", "L11: 0.77", "L12: 0.18", "L13: 0.32"]
[2m2025-10-21T17:41:28.764283Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.92", "L1: 21.92", "L2: 11.00", "L3: 9.23", "L4: 6.11", "L5: 5.19", "L6: 3.53", "L7: 2.94", "L8: 1.32", "L9: 1.31", "L10: 0.64", "L11: 0.72", "L12: 0.17", "L13: 0.30"]
[2m2025-10-21T17:41:29.172742Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.78", "L1: 24.78", "L2: 12.59", "L3: 10.56", "L4: 7.01", "L5: 5.95", "L6: 4.12", "L7: 3.42", "L8: 1.56", "L9: 1.53", "L10: 0.73", "L11: 0.81", "L12: 0.20", "L13: 0.33"]
[2m2025-10-21T17:41:29.375973Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.12", "L1: 14.12", "L2: 7.04", "L3: 5.89", "L4: 3.73", "L5: 3.19", "L6: 2.14", "L7: 1.80", "L8: 0.77", "L9: 0.81", "L10: 0.38", "L11: 0.45", "L12: 0.10", "L13: 0.19"]
[2m2025-10-21T17:41:29.389542Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.58h@0.54p | L9: 4.59h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 4.0645 | Complexity: 0.514 [0.255-0.769] | Temp: 5.39 [3.56-6.48] [3mepoch[0m[2m=[0m21 [3mloss[0m[2m=[0m1.3869880437850952 [3mgrad_norm[0m[2m=[0m9.117735862731934 [3mlearning_rate[0m[2m=[0m0.0009889805223792791
[2m2025-10-21T17:41:29.689494Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.19", "L1: 23.19", "L2: 11.52", "L3: 9.63", "L4: 6.05", "L5: 5.15", "L6: 3.34", "L7: 2.82", "L8: 1.13", "L9: 1.21", "L10: 0.54", "L11: 0.67", "L12: 0.14", "L13: 0.28"]
[2m2025-10-21T17:41:30.017278Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.03", "L1: 24.03", "L2: 12.32", "L3: 10.28", "L4: 6.65", "L5: 5.64", "L6: 3.83", "L7: 3.20", "L8: 1.30", "L9: 1.38", "L10: 0.62", "L11: 0.76", "L12: 0.16", "L13: 0.31"]
[2m2025-10-21T17:41:30.420605Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.21", "L1: 23.21", "L2: 11.95", "L3: 9.96", "L4: 6.62", "L5: 5.58", "L6: 3.85", "L7: 3.18", "L8: 1.40", "L9: 1.39", "L10: 0.65", "L11: 0.75", "L12: 0.17", "L13: 0.30"]
[2m2025-10-21T17:41:30.791581Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.06", "L1: 25.06", "L2: 12.89", "L3: 10.74", "L4: 7.11", "L5: 5.99", "L6: 4.09", "L7: 3.39", "L8: 1.46", "L9: 1.45", "L10: 0.66", "L11: 0.78", "L12: 0.17", "L13: 0.31"]
[2m2025-10-21T17:41:31.204617Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.78", "L1: 20.78", "L2: 10.57", "L3: 8.80", "L4: 5.87", "L5: 4.93", "L6: 3.39", "L7: 2.80", "L8: 1.24", "L9: 1.21", "L10: 0.58", "L11: 0.66", "L12: 0.15", "L13: 0.27"]
[2m2025-10-21T17:41:31.591911Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.36", "L1: 23.36", "L2: 11.94", "L3: 9.93", "L4: 6.55", "L5: 5.52", "L6: 3.88", "L7: 3.19", "L8: 1.45", "L9: 1.40", "L10: 0.66", "L11: 0.75", "L12: 0.18", "L13: 0.30"]
[2m2025-10-21T17:41:31.781708Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.07", "L1: 13.07", "L2: 6.62", "L3: 5.50", "L4: 3.52", "L5: 2.98", "L6: 1.99", "L7: 1.66", "L8: 0.69", "L9: 0.73", "L10: 0.35", "L11: 0.42", "L12: 0.09", "L13: 0.16"]
[2m2025-10-21T17:41:31.794839Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.56h@0.54p | L9: 4.22h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 4.2265 | Complexity: 0.496 [0.238-0.763] | Temp: 5.35 [3.32-6.36] [3mepoch[0m[2m=[0m22 [3mloss[0m[2m=[0m1.1327109336853027 [3mgrad_norm[0m[2m=[0m7.095531940460205 [3mlearning_rate[0m[2m=[0m0.000985023332759738
[2m2025-10-21T17:41:32.105425Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.49", "L1: 23.49", "L2: 11.73", "L3: 9.72", "L4: 6.16", "L5: 5.20", "L6: 3.37", "L7: 2.83", "L8: 1.12", "L9: 1.20", "L10: 0.52", "L11: 0.67", "L12: 0.14", "L13: 0.26"]
[2m2025-10-21T17:41:32.447282Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 27.79", "L1: 27.79", "L2: 14.30", "L3: 11.82", "L4: 7.66", "L5: 6.42", "L6: 4.39", "L7: 3.63", "L8: 1.45", "L9: 1.49", "L10: 0.63", "L11: 0.78", "L12: 0.16", "L13: 0.30"]
[2m2025-10-21T17:41:32.837700Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.85", "L1: 22.85", "L2: 11.75", "L3: 9.72", "L4: 6.39", "L5: 5.35", "L6: 3.69", "L7: 3.04", "L8: 1.30", "L9: 1.30", "L10: 0.59", "L11: 0.71", "L12: 0.16", "L13: 0.28"]
[2m2025-10-21T17:41:33.199198Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.93", "L1: 24.93", "L2: 12.71", "L3: 10.50", "L4: 6.81", "L5: 5.70", "L6: 3.86", "L7: 3.19", "L8: 1.34", "L9: 1.36", "L10: 0.60", "L11: 0.71", "L12: 0.16", "L13: 0.28"]
[2m2025-10-21T17:41:33.640163Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.13", "L1: 21.13", "L2: 10.92", "L3: 9.00", "L4: 6.00", "L5: 4.99", "L6: 3.47", "L7: 2.85", "L8: 1.22", "L9: 1.20", "L10: 0.52", "L11: 0.62", "L12: 0.14", "L13: 0.25"]
[2m2025-10-21T17:41:34.050288Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.36", "L1: 25.36", "L2: 13.29", "L3: 10.95", "L4: 7.23", "L5: 6.03", "L6: 4.23", "L7: 3.46", "L8: 1.49", "L9: 1.47", "L10: 0.63", "L11: 0.74", "L12: 0.16", "L13: 0.29"]
[2m2025-10-21T17:41:34.241808Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.37", "L1: 13.37", "L2: 6.87", "L3: 5.66", "L4: 3.59", "L5: 3.01", "L6: 2.03", "L7: 1.69", "L8: 0.68", "L9: 0.71", "L10: 0.32", "L11: 0.39", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:41:34.254670Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.53h@0.54p | L9: 4.04h@0.51p | ThreshRange: [0.51-0.56] | PredNorm: 4.3851 | Complexity: 0.486 [0.260-0.742] | Temp: 5.38 [3.25-6.37] [3mepoch[0m[2m=[0m23 [3mloss[0m[2m=[0m0.9659209847450256 [3mgrad_norm[0m[2m=[0m8.17757511138916 [3mlearning_rate[0m[2m=[0m0.000980472075752914
[2m2025-10-21T17:41:34.539321Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.42", "L1: 22.42", "L2: 11.12", "L3: 9.15", "L4: 5.71", "L5: 4.80", "L6: 3.07", "L7: 2.58", "L8: 0.97", "L9: 1.06", "L10: 0.46", "L11: 0.60", "L12: 0.12", "L13: 0.24"]
[2m2025-10-21T17:41:34.853786Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.59", "L1: 24.59", "L2: 12.58", "L3: 10.33", "L4: 6.60", "L5: 5.50", "L6: 3.72", "L7: 3.07", "L8: 1.19", "L9: 1.26", "L10: 0.53", "L11: 0.67", "L12: 0.14", "L13: 0.27"]
[2m2025-10-21T17:41:35.227355Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.19", "L1: 25.19", "L2: 12.68", "L3: 10.40", "L4: 6.65", "L5: 5.52", "L6: 3.72", "L7: 3.06", "L8: 1.25", "L9: 1.26", "L10: 0.56", "L11: 0.68", "L12: 0.15", "L13: 0.26"]
[2m2025-10-21T17:41:35.585536Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 27.75", "L1: 27.75", "L2: 13.90", "L3: 11.40", "L4: 7.27", "L5: 6.05", "L6: 4.06", "L7: 3.34", "L8: 1.36", "L9: 1.39", "L10: 0.59", "L11: 0.73", "L12: 0.15", "L13: 0.28"]
[2m2025-10-21T17:41:35.991573Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.82", "L1: 22.82", "L2: 11.81", "L3: 9.67", "L4: 6.35", "L5: 5.25", "L6: 3.61", "L7: 2.95", "L8: 1.24", "L9: 1.22", "L10: 0.51", "L11: 0.61", "L12: 0.14", "L13: 0.23"]
[2m2025-10-21T17:41:36.381496Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.79", "L1: 24.79", "L2: 13.01", "L3: 10.64", "L4: 6.94", "L5: 5.75", "L6: 4.02", "L7: 3.28", "L8: 1.39", "L9: 1.37", "L10: 0.58", "L11: 0.69", "L12: 0.15", "L13: 0.26"]
[2m2025-10-21T17:41:36.570716Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.97", "L1: 12.97", "L2: 6.57", "L3: 5.37", "L4: 3.34", "L5: 2.78", "L6: 1.88", "L7: 1.55", "L8: 0.62", "L9: 0.64", "L10: 0.26", "L11: 0.33", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:41:36.583471Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.56h@0.54p | L9: 4.17h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 4.5433 | Complexity: 0.494 [0.255-0.760] | Temp: 5.37 [3.18-6.37] [3mepoch[0m[2m=[0m24 [3mloss[0m[2m=[0m0.8134931921958923 [3mgrad_norm[0m[2m=[0m8.203753471374512 [3mlearning_rate[0m[2m=[0m0.0009753327467478812
[2m2025-10-21T17:41:36.872805Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.48", "L1: 23.48", "L2: 11.50", "L3: 9.39", "L4: 5.67", "L5: 4.72", "L6: 3.01", "L7: 2.51", "L8: 0.94", "L9: 1.02", "L10: 0.44", "L11: 0.59", "L12: 0.12", "L13: 0.22"]
[2m2025-10-21T17:41:37.201082Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 29.28", "L1: 29.28", "L2: 14.36", "L3: 11.70", "L4: 7.23", "L5: 5.99", "L6: 3.96", "L7: 3.26", "L8: 1.26", "L9: 1.34", "L10: 0.55", "L11: 0.71", "L12: 0.14", "L13: 0.27"]
[2m2025-10-21T17:41:37.552326Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.29", "L1: 24.29", "L2: 12.26", "L3: 9.99", "L4: 6.41", "L5: 5.29", "L6: 3.61", "L7: 2.95", "L8: 1.21", "L9: 1.22", "L10: 0.52", "L11: 0.65", "L12: 0.14", "L13: 0.24"]
[2m2025-10-21T17:41:37.929308Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.98", "L1: 25.98", "L2: 13.42", "L3: 10.93", "L4: 6.98", "L5: 5.77", "L6: 3.94", "L7: 3.23", "L8: 1.30", "L9: 1.32", "L10: 0.53", "L11: 0.67", "L12: 0.14", "L13: 0.25"]
[2m2025-10-21T17:41:38.419263Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.50", "L1: 23.50", "L2: 12.15", "L3: 9.89", "L4: 6.44", "L5: 5.29", "L6: 3.63", "L7: 2.95", "L8: 1.22", "L9: 1.20", "L10: 0.48", "L11: 0.59", "L12: 0.13", "L13: 0.22"]
[2m2025-10-21T17:41:38.826821Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.31", "L1: 25.31", "L2: 13.03", "L3: 10.61", "L4: 6.84", "L5: 5.63", "L6: 3.91", "L7: 3.17", "L8: 1.33", "L9: 1.30", "L10: 0.56", "L11: 0.67", "L12: 0.14", "L13: 0.25"]
[2m2025-10-21T17:41:39.015951Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.69", "L1: 14.69", "L2: 7.34", "L3: 5.96", "L4: 3.68", "L5: 3.04", "L6: 2.05", "L7: 1.69", "L8: 0.65", "L9: 0.68", "L10: 0.27", "L11: 0.36", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:41:39.029469Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.52h@0.54p | L9: 4.11h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 4.6996 | Complexity: 0.489 [0.242-0.723] | Temp: 5.42 [3.24-6.48] [3mepoch[0m[2m=[0m25 [3mloss[0m[2m=[0m0.7121706008911133 [3mgrad_norm[0m[2m=[0m9.246480941772461 [3mlearning_rate[0m[2m=[0m0.0009696125634945929
[2m2025-10-21T17:41:39.321037Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.42", "L1: 22.42", "L2: 11.13", "L3: 9.04", "L4: 5.55", "L5: 4.58", "L6: 2.91", "L7: 2.43", "L8: 0.86", "L9: 0.94", "L10: 0.38", "L11: 0.52", "L12: 0.10", "L13: 0.19"]
[2m2025-10-21T17:41:39.646901Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.76", "L1: 24.76", "L2: 12.69", "L3: 10.29", "L4: 6.46", "L5: 5.32", "L6: 3.54", "L7: 2.93", "L8: 1.09", "L9: 1.18", "L10: 0.47", "L11: 0.63", "L12: 0.12", "L13: 0.24"]
[2m2025-10-21T17:41:40.020926Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.56", "L1: 24.56", "L2: 12.73", "L3: 10.32", "L4: 6.56", "L5: 5.39", "L6: 3.65", "L7: 2.98", "L8: 1.18", "L9: 1.17", "L10: 0.48", "L11: 0.62", "L12: 0.13", "L13: 0.23"]
[2m2025-10-21T17:41:40.381304Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 26.32", "L1: 26.32", "L2: 13.67", "L3: 11.09", "L4: 7.13", "L5: 5.85", "L6: 3.96", "L7: 3.23", "L8: 1.26", "L9: 1.27", "L10: 0.50", "L11: 0.64", "L12: 0.13", "L13: 0.24"]
[2m2025-10-21T17:41:40.785663Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.36", "L1: 22.36", "L2: 11.74", "L3: 9.51", "L4: 6.26", "L5: 5.11", "L6: 3.52", "L7: 2.86", "L8: 1.17", "L9: 1.13", "L10: 0.45", "L11: 0.57", "L12: 0.12", "L13: 0.21"]
[2m2025-10-21T17:41:41.171563Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.51", "L1: 24.51", "L2: 12.78", "L3: 10.33", "L4: 6.71", "L5: 5.49", "L6: 3.79", "L7: 3.07", "L8: 1.27", "L9: 1.24", "L10: 0.51", "L11: 0.63", "L12: 0.13", "L13: 0.23"]
[2m2025-10-21T17:41:41.357981Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.76", "L1: 11.76", "L2: 6.08", "L3: 4.92", "L4: 3.12", "L5: 2.55", "L6: 1.77", "L7: 1.45", "L8: 0.57", "L9: 0.56", "L10: 0.23", "L11: 0.30", "L12: 0.06", "L13: 0.10"]
[2m2025-10-21T17:41:41.371674Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.56h@0.53p | L9: 3.83h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 4.8590 | Complexity: 0.478 [0.210-0.681] | Temp: 5.34 [3.01-6.44] [3mepoch[0m[2m=[0m26 [3mloss[0m[2m=[0m0.5997897386550903 [3mgrad_norm[0m[2m=[0m6.729862689971924 [3mlearning_rate[0m[2m=[0m0.0009633191511966288
[2m2025-10-21T17:41:41.670120Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.08", "L1: 19.08", "L2: 9.57", "L3: 7.74", "L4: 4.73", "L5: 3.89", "L6: 2.50", "L7: 2.08", "L8: 0.76", "L9: 0.83", "L10: 0.34", "L11: 0.46", "L12: 0.09", "L13: 0.17"]
[2m2025-10-21T17:41:42.019292Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.32", "L1: 25.32", "L2: 12.95", "L3: 10.43", "L4: 6.49", "L5: 5.31", "L6: 3.53", "L7: 2.89", "L8: 1.07", "L9: 1.13", "L10: 0.46", "L11: 0.62", "L12: 0.12", "L13: 0.23"]
[2m2025-10-21T17:41:42.403896Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.09", "L1: 23.09", "L2: 11.76", "L3: 9.47", "L4: 6.04", "L5: 4.93", "L6: 3.31", "L7: 2.70", "L8: 1.06", "L9: 1.05", "L10: 0.44", "L11: 0.56", "L12: 0.11", "L13: 0.21"]
[2m2025-10-21T17:41:42.760646Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 26.39", "L1: 26.39", "L2: 13.53", "L3: 10.90", "L4: 6.93", "L5: 5.67", "L6: 3.78", "L7: 3.10", "L8: 1.18", "L9: 1.19", "L10: 0.46", "L11: 0.61", "L12: 0.12", "L13: 0.22"]
[2m2025-10-21T17:41:43.184651Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.04", "L1: 21.04", "L2: 10.85", "L3: 8.74", "L4: 5.66", "L5: 4.61", "L6: 3.15", "L7: 2.55", "L8: 1.05", "L9: 1.03", "L10: 0.42", "L11: 0.53", "L12: 0.11", "L13: 0.20"]
[2m2025-10-21T17:41:43.565666Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.91", "L1: 22.91", "L2: 12.17", "L3: 9.78", "L4: 6.37", "L5: 5.17", "L6: 3.61", "L7: 2.91", "L8: 1.17", "L9: 1.13", "L10: 0.47", "L11: 0.58", "L12: 0.12", "L13: 0.22"]
[2m2025-10-21T17:41:43.755040Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.80", "L1: 10.80", "L2: 5.48", "L3: 4.39", "L4: 2.78", "L5: 2.26", "L6: 1.53", "L7: 1.25", "L8: 0.49", "L9: 0.51", "L10: 0.19", "L11: 0.25", "L12: 0.05", "L13: 0.09"]
[2m2025-10-21T17:41:43.768359Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.52h@0.54p | L9: 3.91h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 5.0191 | Complexity: 0.479 [0.201-0.697] | Temp: 5.36 [2.99-6.42] [3mepoch[0m[2m=[0m27 [3mloss[0m[2m=[0m0.5173534750938416 [3mgrad_norm[0m[2m=[0m5.641081809997559 [3mlearning_rate[0m[2m=[0m0.0009564612410031259
[2m2025-10-21T17:41:44.064989Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.75", "L1: 17.75", "L2: 8.78", "L3: 7.05", "L4: 4.28", "L5: 3.49", "L6: 2.23", "L7: 1.84", "L8: 0.66", "L9: 0.73", "L10: 0.28", "L11: 0.39", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:41:44.447340Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.81", "L1: 25.81", "L2: 13.17", "L3: 10.56", "L4: 6.60", "L5: 5.36", "L6: 3.54", "L7: 2.89", "L8: 1.07", "L9: 1.14", "L10: 0.42", "L11: 0.58", "L12: 0.11", "L13: 0.22"]
[2m2025-10-21T17:41:44.805650Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.62", "L1: 22.62", "L2: 11.40", "L3: 9.14", "L4: 5.79", "L5: 4.70", "L6: 3.14", "L7: 2.56", "L8: 1.00", "L9: 1.01", "L10: 0.40", "L11: 0.53", "L12: 0.11", "L13: 0.19"]
[2m2025-10-21T17:41:45.156326Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.58", "L1: 23.58", "L2: 12.15", "L3: 9.74", "L4: 6.24", "L5: 5.05", "L6: 3.40", "L7: 2.76", "L8: 1.08", "L9: 1.09", "L10: 0.44", "L11: 0.58", "L12: 0.12", "L13: 0.21"]
[2m2025-10-21T17:41:45.571348Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.48", "L1: 21.48", "L2: 11.31", "L3: 9.06", "L4: 5.93", "L5: 4.79", "L6: 3.27", "L7: 2.65", "L8: 1.05", "L9: 1.03", "L10: 0.39", "L11: 0.50", "L12: 0.11", "L13: 0.18"]
[2m2025-10-21T17:41:45.983148Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.38", "L1: 22.38", "L2: 11.63", "L3: 9.31", "L4: 6.03", "L5: 4.88", "L6: 3.35", "L7: 2.70", "L8: 1.10", "L9: 1.08", "L10: 0.42", "L11: 0.54", "L12: 0.11", "L13: 0.20"]
[2m2025-10-21T17:41:46.187439Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.13", "L1: 11.13", "L2: 5.66", "L3: 4.53", "L4: 2.81", "L5: 2.28", "L6: 1.52", "L7: 1.25", "L8: 0.47", "L9: 0.50", "L10: 0.20", "L11: 0.27", "L12: 0.05", "L13: 0.09"]
[2m2025-10-21T17:41:46.201520Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.53h@0.53p | L9: 3.86h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 5.1806 | Complexity: 0.478 [0.190-0.681] | Temp: 5.33 [2.79-6.44] [3mepoch[0m[2m=[0m28 [3mloss[0m[2m=[0m0.4445132911205292 [3mgrad_norm[0m[2m=[0m5.398406982421875 [3mlearning_rate[0m[2m=[0m0.0009490482043474913
[2m2025-10-21T17:41:46.492576Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.62", "L1: 16.62", "L2: 8.20", "L3: 6.55", "L4: 3.94", "L5: 3.21", "L6: 2.01", "L7: 1.67", "L8: 0.58", "L9: 0.64", "L10: 0.26", "L11: 0.37", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:41:46.833147Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.37", "L1: 22.37", "L2: 11.44", "L3: 9.14", "L4: 5.68", "L5: 4.60", "L6: 2.97", "L7: 2.44", "L8: 0.87", "L9: 0.95", "L10: 0.39", "L11: 0.55", "L12: 0.10", "L13: 0.21"]
[2m2025-10-21T17:41:47.218879Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.15", "L1: 23.15", "L2: 11.71", "L3: 9.34", "L4: 5.81", "L5: 4.69", "L6: 3.11", "L7: 2.53", "L8: 0.95", "L9: 0.97", "L10: 0.38", "L11: 0.52", "L12: 0.10", "L13: 0.19"]
[2m2025-10-21T17:41:47.599131Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.95", "L1: 24.95", "L2: 12.71", "L3: 10.14", "L4: 6.41", "L5: 5.18", "L6: 3.42", "L7: 2.79", "L8: 1.04", "L9: 1.08", "L10: 0.41", "L11: 0.56", "L12: 0.11", "L13: 0.20"]
[2m2025-10-21T17:41:48.060665Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.10", "L1: 22.10", "L2: 11.43", "L3: 9.11", "L4: 5.83", "L5: 4.68", "L6: 3.15", "L7: 2.55", "L8: 0.98", "L9: 0.97", "L10: 0.36", "L11: 0.48", "L12: 0.10", "L13: 0.17"]
[2m2025-10-21T17:41:48.480430Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.78", "L1: 20.78", "L2: 11.00", "L3: 8.76", "L4: 5.68", "L5: 4.56", "L6: 3.11", "L7: 2.51", "L8: 0.99", "L9: 0.98", "L10: 0.39", "L11: 0.51", "L12: 0.10", "L13: 0.19"]
[2m2025-10-21T17:41:48.675021Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.59", "L1: 10.59", "L2: 5.45", "L3: 4.33", "L4: 2.70", "L5: 2.18", "L6: 1.45", "L7: 1.19", "L8: 0.45", "L9: 0.46", "L10: 0.19", "L11: 0.26", "L12: 0.05", "L13: 0.09"]
[2m2025-10-21T17:41:48.688794Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.64h@0.56p | L5: 4.51h@0.53p | L9: 3.98h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 5.3422 | Complexity: 0.482 [0.196-0.714] | Temp: 5.33 [2.65-6.45] [3mepoch[0m[2m=[0m29 [3mloss[0m[2m=[0m0.39778032898902893 [3mgrad_norm[0m[2m=[0m5.307311534881592 [3mlearning_rate[0m[2m=[0m0.000941090052947402
[2m2025-10-21T17:41:48.996992Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.16", "L1: 17.16", "L2: 8.53", "L3: 6.78", "L4: 4.06", "L5: 3.29", "L6: 2.05", "L7: 1.71", "L8: 0.58", "L9: 0.66", "L10: 0.26", "L11: 0.38", "L12: 0.07", "L13: 0.14"]
[2m2025-10-21T17:41:49.342281Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.60", "L1: 25.60", "L2: 13.07", "L3: 10.38", "L4: 6.44", "L5: 5.20", "L6: 3.37", "L7: 2.77", "L8: 0.97", "L9: 1.07", "L10: 0.39", "L11: 0.55", "L12: 0.11", "L13: 0.20"]
[2m2025-10-21T17:41:49.706828Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.19", "L1: 22.19", "L2: 11.24", "L3: 8.93", "L4: 5.48", "L5: 4.42", "L6: 2.87", "L7: 2.35", "L8: 0.88", "L9: 0.90", "L10: 0.35", "L11: 0.48", "L12: 0.10", "L13: 0.17"]
[2m2025-10-21T17:41:50.054652Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.51", "L1: 23.51", "L2: 11.90", "L3: 9.44", "L4: 5.87", "L5: 4.73", "L6: 3.10", "L7: 2.53", "L8: 0.94", "L9: 0.97", "L10: 0.37", "L11: 0.51", "L12: 0.10", "L13: 0.18"]
[2m2025-10-21T17:41:50.458424Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.15", "L1: 22.15", "L2: 11.36", "L3: 9.01", "L4: 5.79", "L5: 4.62", "L6: 3.13", "L7: 2.54", "L8: 0.96", "L9: 0.97", "L10: 0.37", "L11: 0.49", "L12: 0.10", "L13: 0.17"]
[2m2025-10-21T17:41:50.832943Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.00", "L1: 21.00", "L2: 10.77", "L3: 8.54", "L4: 5.44", "L5: 4.36", "L6: 2.93", "L7: 2.37", "L8: 0.92", "L9: 0.91", "L10: 0.37", "L11: 0.49", "L12: 0.10", "L13: 0.17"]
[2m2025-10-21T17:41:51.035919Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.50h@0.53p | L9: 4.11h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 5.5002 | Complexity: 0.488 [0.190-0.745] | Temp: 5.30 [2.58-6.54] [3mepoch[0m[2m=[0m30 [3mloss[0m[2m=[0m0.3543940484523773 [3mgrad_norm[0m[2m=[0m5.198784828186035 [3mlearning_rate[0m[2m=[0m0.0009325977298431098
[2m2025-10-21T17:41:51.331316Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.75", "L1: 16.75", "L2: 8.27", "L3: 6.56", "L4: 3.96", "L5: 3.20", "L6: 1.99", "L7: 1.66", "L8: 0.56", "L9: 0.63", "L10: 0.25", "L11: 0.37", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:41:51.674085Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.23", "L1: 25.23", "L2: 12.74", "L3: 10.07", "L4: 6.23", "L5: 4.99", "L6: 3.17", "L7: 2.60", "L8: 0.89", "L9: 0.97", "L10: 0.36", "L11: 0.53", "L12: 0.10", "L13: 0.20"]
[2m2025-10-21T17:41:52.064530Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.13", "L1: 23.13", "L2: 11.75", "L3: 9.29", "L4: 5.80", "L5: 4.65", "L6: 3.05", "L7: 2.50", "L8: 0.94", "L9: 0.98", "L10: 0.36", "L11: 0.50", "L12: 0.09", "L13: 0.17"]
[2m2025-10-21T17:41:52.410633Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.46", "L1: 22.46", "L2: 11.39", "L3: 9.00", "L4: 5.55", "L5: 4.45", "L6: 2.88", "L7: 2.35", "L8: 0.86", "L9: 0.89", "L10: 0.35", "L11: 0.49", "L12: 0.09", "L13: 0.18"]
[2m2025-10-21T17:41:52.811518Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.77", "L1: 20.77", "L2: 10.59", "L3: 8.36", "L4: 5.31", "L5: 4.24", "L6: 2.77", "L7: 2.25", "L8: 0.86", "L9: 0.86", "L10: 0.34", "L11: 0.46", "L12: 0.09", "L13: 0.16"]
[2m2025-10-21T17:41:53.221526Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.39", "L1: 21.39", "L2: 11.11", "L3: 8.77", "L4: 5.57", "L5: 4.45", "L6: 2.97", "L7: 2.40", "L8: 0.92", "L9: 0.91", "L10: 0.34", "L11: 0.46", "L12: 0.09", "L13: 0.16"]
[2m2025-10-21T17:41:53.414828Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.51h@0.53p | L9: 4.18h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 5.6631 | Complexity: 0.492 [0.219-0.758] | Temp: 5.32 [2.61-6.52] [3mepoch[0m[2m=[0m31 [3mloss[0m[2m=[0m0.3244463801383972 [3mgrad_norm[0m[2m=[0m5.058915615081787 [3mlearning_rate[0m[2m=[0m0.0009235828183591366
[2m2025-10-21T17:41:53.703790Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.21", "L1: 18.21", "L2: 8.91", "L3: 7.03", "L4: 4.17", "L5: 3.34", "L6: 2.03", "L7: 1.69", "L8: 0.57", "L9: 0.63", "L10: 0.24", "L11: 0.36", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:41:54.037188Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.17", "L1: 25.17", "L2: 12.81", "L3: 10.08", "L4: 6.13", "L5: 4.90", "L6: 3.10", "L7: 2.54", "L8: 0.87", "L9: 0.94", "L10: 0.36", "L11: 0.53", "L12: 0.10", "L13: 0.19"]
[2m2025-10-21T17:41:54.463015Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.70", "L1: 19.70", "L2: 10.06", "L3: 7.92", "L4: 4.87", "L5: 3.88", "L6: 2.51", "L7: 2.06", "L8: 0.75", "L9: 0.80", "L10: 0.32", "L11: 0.45", "L12: 0.09", "L13: 0.16"]
[2m2025-10-21T17:41:54.830687Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.92", "L1: 20.92", "L2: 10.69", "L3: 8.41", "L4: 5.24", "L5: 4.18", "L6: 2.71", "L7: 2.22", "L8: 0.79", "L9: 0.82", "L10: 0.31", "L11: 0.44", "L12: 0.09", "L13: 0.16"]
[2m2025-10-21T17:41:55.235105Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.20", "L1: 20.20", "L2: 10.48", "L3: 8.25", "L4: 5.17", "L5: 4.12", "L6: 2.69", "L7: 2.19", "L8: 0.81", "L9: 0.80", "L10: 0.30", "L11: 0.42", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:41:55.620314Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.60", "L1: 18.60", "L2: 9.66", "L3: 7.59", "L4: 4.77", "L5: 3.79", "L6: 2.51", "L7: 2.04", "L8: 0.77", "L9: 0.77", "L10: 0.30", "L11: 0.42", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:41:55.808299Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.35", "L1: 10.35", "L2: 5.27", "L3: 4.14", "L4: 2.53", "L5: 2.02", "L6: 1.33", "L7: 1.09", "L8: 0.39", "L9: 0.42", "L10: 0.17", "L11: 0.24", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:41:55.821436Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.54h@0.53p | L9: 4.10h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 5.8209 | Complexity: 0.489 [0.196-0.767] | Temp: 5.30 [2.52-6.52] [3mepoch[0m[2m=[0m32 [3mloss[0m[2m=[0m0.29239118099212646 [3mgrad_norm[0m[2m=[0m4.88675594329834 [3mlearning_rate[0m[2m=[0m0.0009140576585195959
[2m2025-10-21T17:41:56.120138Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.40", "L1: 17.40", "L2: 8.68", "L3: 6.82", "L4: 4.13", "L5: 3.29", "L6: 2.04", "L7: 1.69", "L8: 0.57", "L9: 0.63", "L10: 0.23", "L11: 0.35", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:41:56.455972Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.00", "L1: 25.00", "L2: 12.93", "L3: 10.15", "L4: 6.19", "L5: 4.93", "L6: 3.12", "L7: 2.56", "L8: 0.87", "L9: 0.96", "L10: 0.34", "L11: 0.50", "L12: 0.09", "L13: 0.18"]
[2m2025-10-21T17:41:56.832628Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.20", "L1: 21.20", "L2: 10.95", "L3: 8.60", "L4: 5.34", "L5: 4.24", "L6: 2.72", "L7: 2.22", "L8: 0.80", "L9: 0.84", "L10: 0.32", "L11: 0.45", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:41:57.198607Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.51", "L1: 22.51", "L2: 11.53", "L3: 9.05", "L4: 5.61", "L5: 4.45", "L6: 2.88", "L7: 2.35", "L8: 0.83", "L9: 0.87", "L10: 0.33", "L11: 0.48", "L12: 0.09", "L13: 0.17"]
[2m2025-10-21T17:41:57.608089Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.89", "L1: 18.89", "L2: 9.70", "L3: 7.61", "L4: 4.80", "L5: 3.79", "L6: 2.51", "L7: 2.03", "L8: 0.75", "L9: 0.76", "L10: 0.29", "L11: 0.41", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:41:58.022500Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.12", "L1: 19.12", "L2: 9.99", "L3: 7.83", "L4: 4.91", "L5: 3.89", "L6: 2.60", "L7: 2.10", "L8: 0.79", "L9: 0.80", "L10: 0.30", "L11: 0.43", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:41:58.224666Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.50h@0.53p | L9: 4.13h@0.51p | ThreshRange: [0.51-0.56] | PredNorm: 5.9798 | Complexity: 0.489 [0.188-0.796] | Temp: 5.34 [2.52-6.59] [3mepoch[0m[2m=[0m33 [3mloss[0m[2m=[0m0.2729123532772064 [3mgrad_norm[0m[2m=[0m4.717140197753906 [3mlearning_rate[0m[2m=[0m0.0009040352888405323
[2m2025-10-21T17:41:58.532668Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.83", "L1: 16.83", "L2: 8.33", "L3: 6.53", "L4: 3.81", "L5: 3.05", "L6: 1.87", "L7: 1.57", "L8: 0.51", "L9: 0.57", "L10: 0.22", "L11: 0.34", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:41:58.873743Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.23", "L1: 25.23", "L2: 12.68", "L3: 9.90", "L4: 5.99", "L5: 4.76", "L6: 2.97", "L7: 2.45", "L8: 0.82", "L9: 0.91", "L10: 0.33", "L11: 0.51", "L12: 0.09", "L13: 0.18"]
[2m2025-10-21T17:41:59.270014Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.23", "L1: 19.23", "L2: 9.82", "L3: 7.68", "L4: 4.75", "L5: 3.77", "L6: 2.44", "L7: 2.00", "L8: 0.71", "L9: 0.75", "L10: 0.28", "L11: 0.41", "L12: 0.08", "L13: 0.14"]
[2m2025-10-21T17:41:59.639806Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.48", "L1: 22.48", "L2: 11.63", "L3: 9.08", "L4: 5.59", "L5: 4.44", "L6: 2.82", "L7: 2.31", "L8: 0.79", "L9: 0.84", "L10: 0.30", "L11: 0.44", "L12: 0.09", "L13: 0.16"]
[2m2025-10-21T17:42:00.058551Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.39", "L1: 22.39", "L2: 11.42", "L3: 8.91", "L4: 5.46", "L5: 4.30", "L6: 2.82", "L7: 2.29", "L8: 0.83", "L9: 0.84", "L10: 0.29", "L11: 0.42", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:42:00.456102Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.19", "L1: 20.19", "L2: 10.64", "L3: 8.30", "L4: 5.19", "L5: 4.09", "L6: 2.72", "L7: 2.20", "L8: 0.81", "L9: 0.82", "L10: 0.30", "L11: 0.43", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:42:00.643063Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.36", "L1: 10.36", "L2: 5.52", "L3: 4.30", "L4: 2.69", "L5: 2.12", "L6: 1.41", "L7: 1.14", "L8: 0.40", "L9: 0.42", "L10: 0.15", "L11: 0.22", "L12: 0.04", "L13: 0.07"]
[2m2025-10-21T17:42:00.656061Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.50h@0.53p | L9: 4.27h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 6.1319 | Complexity: 0.495 [0.218-0.840] | Temp: 5.31 [2.48-6.61] [3mepoch[0m[2m=[0m34 [3mloss[0m[2m=[0m0.26244768500328064 [3mgrad_norm[0m[2m=[0m4.99062442779541 [3mlearning_rate[0m[2m=[0m0.0008935292717069387
[2m2025-10-21T17:42:00.933237Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.18", "L1: 17.18", "L2: 8.42", "L3: 6.57", "L4: 3.82", "L5: 3.05", "L6: 1.83", "L7: 1.55", "L8: 0.50", "L9: 0.59", "L10: 0.22", "L11: 0.34", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:01.276836Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.89", "L1: 23.89", "L2: 12.31", "L3: 9.59", "L4: 5.79", "L5: 4.58", "L6: 2.91", "L7: 2.41", "L8: 0.81", "L9: 0.90", "L10: 0.31", "L11: 0.47", "L12: 0.09", "L13: 0.18"]
[2m2025-10-21T17:42:01.640159Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.70", "L1: 18.70", "L2: 9.57", "L3: 7.46", "L4: 4.63", "L5: 3.66", "L6: 2.39", "L7: 1.96", "L8: 0.69", "L9: 0.73", "L10: 0.27", "L11: 0.40", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:42:01.978191Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.12", "L1: 21.12", "L2: 10.89", "L3: 8.48", "L4: 5.15", "L5: 4.09", "L6: 2.60", "L7: 2.15", "L8: 0.74", "L9: 0.80", "L10: 0.30", "L11: 0.46", "L12: 0.09", "L13: 0.16"]
[2m2025-10-21T17:42:02.364615Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.29", "L1: 22.29", "L2: 11.59", "L3: 9.02", "L4: 5.68", "L5: 4.45", "L6: 2.94", "L7: 2.38", "L8: 0.87", "L9: 0.87", "L10: 0.32", "L11: 0.46", "L12: 0.09", "L13: 0.15"]
[2m2025-10-21T17:42:02.734887Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.49", "L1: 20.49", "L2: 10.84", "L3: 8.43", "L4: 5.23", "L5: 4.11", "L6: 2.72", "L7: 2.20", "L8: 0.80", "L9: 0.80", "L10: 0.29", "L11: 0.42", "L12: 0.08", "L13: 0.14"]
[2m2025-10-21T17:42:02.926551Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.51h@0.53p | L9: 4.12h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 6.2859 | Complexity: 0.489 [0.222-0.793] | Temp: 5.29 [2.45-6.82] [3mepoch[0m[2m=[0m35 [3mloss[0m[2m=[0m0.2511119842529297 [3mgrad_norm[0m[2m=[0m4.293120861053467 [3mlearning_rate[0m[2m=[0m0.0008825540426187217
[2m2025-10-21T17:42:03.207079Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.57", "L1: 13.57", "L2: 6.72", "L3: 5.23", "L4: 3.09", "L5: 2.45", "L6: 1.50", "L7: 1.26", "L8: 0.42", "L9: 0.47", "L10: 0.18", "L11: 0.29", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:03.525452Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.18", "L1: 22.18", "L2: 11.22", "L3: 8.72", "L4: 5.19", "L5: 4.09", "L6: 2.51", "L7: 2.09", "L8: 0.69", "L9: 0.78", "L10: 0.29", "L11: 0.46", "L12: 0.08", "L13: 0.17"]
[2m2025-10-21T17:42:03.881777Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.87", "L1: 18.87", "L2: 9.94", "L3: 7.72", "L4: 4.84", "L5: 3.78", "L6: 2.48", "L7: 2.01", "L8: 0.72", "L9: 0.73", "L10: 0.25", "L11: 0.38", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:42:04.241925Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.87", "L1: 21.87", "L2: 11.34", "L3: 8.80", "L4: 5.43", "L5: 4.27", "L6: 2.74", "L7: 2.24", "L8: 0.80", "L9: 0.82", "L10: 0.30", "L11: 0.46", "L12: 0.09", "L13: 0.16"]
[2m2025-10-21T17:42:04.665497Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.61", "L1: 20.61", "L2: 10.78", "L3: 8.37", "L4: 5.25", "L5: 4.11", "L6: 2.66", "L7: 2.17", "L8: 0.78", "L9: 0.78", "L10: 0.27", "L11: 0.39", "L12: 0.08", "L13: 0.14"]
[2m2025-10-21T17:42:05.048276Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.81", "L1: 24.81", "L2: 13.01", "L3: 10.10", "L4: 6.25", "L5: 4.92", "L6: 3.09", "L7: 2.51", "L8: 0.89", "L9: 0.87", "L10: 0.30", "L11: 0.43", "L12: 0.08", "L13: 0.14"]
[2m2025-10-21T17:42:05.247021Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.23", "L1: 10.23", "L2: 5.16", "L3: 4.00", "L4: 2.38", "L5: 1.87", "L6: 1.19", "L7: 0.99", "L8: 0.34", "L9: 0.36", "L10: 0.12", "L11: 0.18", "L12: 0.03", "L13: 0.06"]
[2m2025-10-21T17:42:05.260802Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.49h@0.53p | L9: 4.12h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 6.4335 | Complexity: 0.488 [0.217-0.751] | Temp: 5.29 [2.48-7.01] [3mepoch[0m[2m=[0m36 [3mloss[0m[2m=[0m0.23095759749412537 [3mgrad_norm[0m[2m=[0m4.850478649139404 [3mlearning_rate[0m[2m=[0m0.0008711246191523969
[2m2025-10-21T17:42:05.570714Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.42", "L1: 17.42", "L2: 8.82", "L3: 6.84", "L4: 3.97", "L5: 3.16", "L6: 1.87", "L7: 1.57", "L8: 0.50", "L9: 0.51", "L10: 0.17", "L11: 0.26", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:05.933035Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.55", "L1: 20.55", "L2: 10.57", "L3: 8.19", "L4: 4.90", "L5: 3.87", "L6: 2.42", "L7: 2.01", "L8: 0.66", "L9: 0.74", "L10: 0.27", "L11: 0.43", "L12: 0.08", "L13: 0.16"]
[2m2025-10-21T17:42:06.316564Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.44", "L1: 17.44", "L2: 9.01", "L3: 6.98", "L4: 4.34", "L5: 3.39", "L6: 2.17", "L7: 1.78", "L8: 0.64", "L9: 0.68", "L10: 0.25", "L11: 0.37", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:42:06.668361Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.62", "L1: 23.62", "L2: 12.03", "L3: 9.32", "L4: 5.70", "L5: 4.48", "L6: 2.79", "L7: 2.29", "L8: 0.79", "L9: 0.83", "L10: 0.29", "L11: 0.44", "L12: 0.08", "L13: 0.15"]
[2m2025-10-21T17:42:07.077830Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.43", "L1: 22.43", "L2: 11.58", "L3: 8.96", "L4: 5.53", "L5: 4.31", "L6: 2.79", "L7: 2.29", "L8: 0.80", "L9: 0.82", "L10: 0.29", "L11: 0.43", "L12: 0.08", "L13: 0.14"]
[2m2025-10-21T17:42:07.469664Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.55", "L1: 21.55", "L2: 11.19", "L3: 8.66", "L4: 5.38", "L5: 4.22", "L6: 2.70", "L7: 2.20", "L8: 0.79", "L9: 0.79", "L10: 0.28", "L11: 0.42", "L12: 0.08", "L13: 0.14"]
[2m2025-10-21T17:42:07.664952Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.66", "L1: 12.66", "L2: 6.38", "L3: 4.94", "L4: 2.98", "L5: 2.36", "L6: 1.44", "L7: 1.21", "L8: 0.39", "L9: 0.41", "L10: 0.15", "L11: 0.23", "L12: 0.04", "L13: 0.07"]
[2m2025-10-21T17:42:07.678924Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.48h@0.53p | L9: 4.11h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 6.5787 | Complexity: 0.487 [0.208-0.809] | Temp: 5.30 [2.38-7.06] [3mepoch[0m[2m=[0m37 [3mloss[0m[2m=[0m0.22005225718021393 [3mgrad_norm[0m[2m=[0m5.218510150909424 [3mlearning_rate[0m[2m=[0m0.0008592565427534282
[2m2025-10-21T17:42:07.987854Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.50", "L1: 17.50", "L2: 8.60", "L3: 6.66", "L4: 3.94", "L5: 3.12", "L6: 1.87", "L7: 1.57", "L8: 0.52", "L9: 0.56", "L10: 0.19", "L11: 0.30", "L12: 0.06", "L13: 0.10"]
[2m2025-10-21T17:42:08.349921Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.09", "L1: 24.09", "L2: 11.94", "L3: 9.23", "L4: 5.31", "L5: 4.23", "L6: 2.55", "L7: 2.17", "L8: 0.70", "L9: 0.82", "L10: 0.28", "L11: 0.45", "L12: 0.08", "L13: 0.16"]
[2m2025-10-21T17:42:08.750902Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.07", "L1: 17.07", "L2: 8.87", "L3: 6.86", "L4: 4.19", "L5: 3.29", "L6: 2.02", "L7: 1.68", "L8: 0.58", "L9: 0.63", "L10: 0.23", "L11: 0.35", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:09.155886Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.36", "L1: 20.36", "L2: 10.62", "L3: 8.19", "L4: 5.08", "L5: 3.98", "L6: 2.50", "L7: 2.06", "L8: 0.71", "L9: 0.76", "L10: 0.26", "L11: 0.41", "L12: 0.08", "L13: 0.14"]
[2m2025-10-21T17:42:09.610753Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.63", "L1: 19.63", "L2: 10.14", "L3: 7.81", "L4: 4.82", "L5: 3.75", "L6: 2.35", "L7: 1.94", "L8: 0.68", "L9: 0.70", "L10: 0.26", "L11: 0.39", "L12: 0.08", "L13: 0.14"]
[2m2025-10-21T17:42:09.996907Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.43", "L1: 21.43", "L2: 11.20", "L3: 8.63", "L4: 5.24", "L5: 4.10", "L6: 2.57", "L7: 2.11", "L8: 0.75", "L9: 0.76", "L10: 0.26", "L11: 0.39", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:42:10.181321Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.38", "L1: 12.38", "L2: 6.37", "L3: 4.91", "L4: 3.02", "L5: 2.34", "L6: 1.51", "L7: 1.23", "L8: 0.41", "L9: 0.43", "L10: 0.14", "L11: 0.21", "L12: 0.04", "L13: 0.07"]
[2m2025-10-21T17:42:10.194666Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.43h@0.53p | L9: 4.26h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 6.7208 | Complexity: 0.492 [0.196-0.806] | Temp: 5.34 [2.34-7.15] [3mepoch[0m[2m=[0m38 [3mloss[0m[2m=[0m0.20963013172149658 [3mgrad_norm[0m[2m=[0m5.036667823791504 [3mlearning_rate[0m[2m=[0m0.0008469660533592105
[2m2025-10-21T17:42:10.503929Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.30", "L1: 19.30", "L2: 9.66", "L3: 7.44", "L4: 4.33", "L5: 3.42", "L6: 1.99", "L7: 1.70", "L8: 0.51", "L9: 0.57", "L10: 0.18", "L11: 0.30", "L12: 0.06", "L13: 0.10"]
[2m2025-10-21T17:42:10.849089Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.51", "L1: 23.51", "L2: 11.99", "L3: 9.24", "L4: 5.47", "L5: 4.29", "L6: 2.55", "L7: 2.14", "L8: 0.68", "L9: 0.77", "L10: 0.28", "L11: 0.45", "L12: 0.08", "L13: 0.16"]
[2m2025-10-21T17:42:11.223406Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.11", "L1: 17.11", "L2: 9.01", "L3: 6.94", "L4: 4.25", "L5: 3.31", "L6: 2.10", "L7: 1.74", "L8: 0.60", "L9: 0.62", "L10: 0.23", "L11: 0.35", "L12: 0.07", "L13: 0.12"]
[2m2025-10-21T17:42:11.593376Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.98", "L1: 18.98", "L2: 9.70", "L3: 7.46", "L4: 4.50", "L5: 3.51", "L6: 2.17", "L7: 1.80", "L8: 0.61", "L9: 0.68", "L10: 0.25", "L11: 0.39", "L12: 0.07", "L13: 0.14"]
[2m2025-10-21T17:42:12.005501Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.72", "L1: 20.72", "L2: 10.97", "L3: 8.42", "L4: 5.24", "L5: 4.06", "L6: 2.54", "L7: 2.08", "L8: 0.73", "L9: 0.76", "L10: 0.25", "L11: 0.38", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:42:12.384059Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.46", "L1: 20.46", "L2: 10.77", "L3: 8.29", "L4: 5.11", "L5: 3.98", "L6: 2.57", "L7: 2.11", "L8: 0.75", "L9: 0.74", "L10: 0.26", "L11: 0.39", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:42:12.584422Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.46h@0.53p | L9: 4.37h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 6.8619 | Complexity: 0.498 [0.197-0.845] | Temp: 5.30 [2.24-6.82] [3mepoch[0m[2m=[0m39 [3mloss[0m[2m=[0m0.20214340090751648 [3mgrad_norm[0m[2m=[0m4.500226020812988 [3mlearning_rate[0m[2m=[0m0.0008342699729837477
[2m2025-10-21T17:42:12.868085Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.93", "L1: 16.93", "L2: 8.39", "L3: 6.45", "L4: 3.74", "L5: 2.93", "L6: 1.75", "L7: 1.48", "L8: 0.46", "L9: 0.54", "L10: 0.20", "L11: 0.32", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:13.191596Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.97", "L1: 21.97", "L2: 11.26", "L3: 8.66", "L4: 5.20", "L5: 4.08", "L6: 2.48", "L7: 2.09", "L8: 0.67", "L9: 0.78", "L10: 0.29", "L11: 0.47", "L12: 0.09", "L13: 0.17"]
[2m2025-10-21T17:42:13.557938Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.02", "L1: 19.02", "L2: 9.73", "L3: 7.48", "L4: 4.45", "L5: 3.47", "L6: 2.16", "L7: 1.81", "L8: 0.62", "L9: 0.70", "L10: 0.24", "L11: 0.36", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:42:13.932643Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.20", "L1: 21.20", "L2: 10.78", "L3: 8.27", "L4: 4.94", "L5: 3.86", "L6: 2.36", "L7: 1.98", "L8: 0.66", "L9: 0.70", "L10: 0.24", "L11: 0.39", "L12: 0.07", "L13: 0.14"]
[2m2025-10-21T17:42:14.386207Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.51", "L1: 19.51", "L2: 9.96", "L3: 7.64", "L4: 4.64", "L5: 3.59", "L6: 2.23", "L7: 1.85", "L8: 0.63", "L9: 0.69", "L10: 0.23", "L11: 0.35", "L12: 0.07", "L13: 0.12"]
[2m2025-10-21T17:42:14.804918Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.94", "L1: 19.94", "L2: 10.53", "L3: 8.09", "L4: 4.94", "L5: 3.84", "L6: 2.45", "L7: 2.01", "L8: 0.70", "L9: 0.70", "L10: 0.24", "L11: 0.36", "L12: 0.07", "L13: 0.12"]
[2m2025-10-21T17:42:15.011702Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.48h@0.53p | L9: 4.28h@0.51p | ThreshRange: [0.51-0.56] | PredNorm: 7.0051 | Complexity: 0.495 [0.185-0.861] | Temp: 5.32 [2.25-7.03] [3mepoch[0m[2m=[0m40 [3mloss[0m[2m=[0m0.1953074187040329 [3mgrad_norm[0m[2m=[0m4.024664878845215 [3mlearning_rate[0m[2m=[0m0.0008211856475099921
[2m2025-10-21T17:42:15.303545Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.73", "L1: 16.73", "L2: 8.11", "L3: 6.22", "L4: 3.49", "L5: 2.73", "L6: 1.58", "L7: 1.36", "L8: 0.41", "L9: 0.48", "L10: 0.18", "L11: 0.30", "L12: 0.06", "L13: 0.10"]
[2m2025-10-21T17:42:15.656590Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.30", "L1: 19.30", "L2: 9.86", "L3: 7.56", "L4: 4.58", "L5: 3.59", "L6: 2.18", "L7: 1.84", "L8: 0.60", "L9: 0.69", "L10: 0.25", "L11: 0.41", "L12: 0.07", "L13: 0.15"]
[2m2025-10-21T17:42:16.043900Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.45", "L1: 18.45", "L2: 9.34", "L3: 7.15", "L4: 4.25", "L5: 3.30", "L6: 2.04", "L7: 1.69", "L8: 0.57", "L9: 0.61", "L10: 0.22", "L11: 0.35", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:16.444601Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.75", "L1: 19.75", "L2: 10.28", "L3: 7.86", "L4: 4.79", "L5: 3.72", "L6: 2.33", "L7: 1.94", "L8: 0.66", "L9: 0.70", "L10: 0.24", "L11: 0.38", "L12: 0.07", "L13: 0.14"]
[2m2025-10-21T17:42:16.866966Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.62", "L1: 18.62", "L2: 9.45", "L3: 7.23", "L4: 4.40", "L5: 3.40", "L6: 2.15", "L7: 1.78", "L8: 0.63", "L9: 0.66", "L10: 0.23", "L11: 0.35", "L12: 0.07", "L13: 0.12"]
[2m2025-10-21T17:42:17.273892Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.87", "L1: 18.87", "L2: 10.09", "L3: 7.71", "L4: 4.68", "L5: 3.63", "L6: 2.31", "L7: 1.91", "L8: 0.65", "L9: 0.66", "L10: 0.23", "L11: 0.35", "L12: 0.07", "L13: 0.12"]
[2m2025-10-21T17:42:17.476727Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.50h@0.53p | L9: 4.26h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 7.1457 | Complexity: 0.495 [0.196-0.794] | Temp: 5.28 [2.23-6.90] [3mepoch[0m[2m=[0m41 [3mloss[0m[2m=[0m0.1811228096485138 [3mgrad_norm[0m[2m=[0m3.786433696746826 [3mlearning_rate[0m[2m=[0m0.0008077308302745223
[2m2025-10-21T17:42:17.764748Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.68", "L1: 11.68", "L2: 5.72", "L3: 4.37", "L4: 2.57", "L5: 1.99", "L6: 1.14", "L7: 0.97", "L8: 0.31", "L9: 0.35", "L10: 0.13", "L11: 0.22", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:42:18.114343Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.59", "L1: 18.59", "L2: 9.42", "L3: 7.19", "L4: 4.23", "L5: 3.28", "L6: 1.98", "L7: 1.66", "L8: 0.54", "L9: 0.61", "L10: 0.22", "L11: 0.37", "L12: 0.07", "L13: 0.14"]
[2m2025-10-21T17:42:18.484681Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.00", "L1: 20.00", "L2: 10.28", "L3: 7.82", "L4: 4.68", "L5: 3.62", "L6: 2.25", "L7: 1.86", "L8: 0.63", "L9: 0.68", "L10: 0.21", "L11: 0.33", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:18.837080Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.96", "L1: 18.96", "L2: 9.73", "L3: 7.42", "L4: 4.47", "L5: 3.47", "L6: 2.18", "L7: 1.82", "L8: 0.61", "L9: 0.64", "L10: 0.23", "L11: 0.37", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:42:19.248838Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.51", "L1: 16.51", "L2: 8.49", "L3: 6.48", "L4: 3.95", "L5: 3.04", "L6: 1.91", "L7: 1.59", "L8: 0.55", "L9: 0.58", "L10: 0.21", "L11: 0.33", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:19.658607Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.60", "L1: 16.60", "L2: 8.77", "L3: 6.69", "L4: 4.09", "L5: 3.16", "L6: 1.97", "L7: 1.64", "L8: 0.58", "L9: 0.59", "L10: 0.21", "L11: 0.32", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:19.866786Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.51h@0.53p | L9: 4.26h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 7.2841 | Complexity: 0.495 [0.185-0.764] | Temp: 5.28 [2.22-6.75] [3mepoch[0m[2m=[0m42 [3mloss[0m[2m=[0m0.16691450774669647 [3mgrad_norm[0m[2m=[0m3.6783366203308105 [3mlearning_rate[0m[2m=[0m0.0007939240313135087
[2m2025-10-21T17:42:20.186395Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.98", "L1: 10.98", "L2: 5.50", "L3: 4.19", "L4: 2.46", "L5: 1.91", "L6: 1.13", "L7: 0.97", "L8: 0.31", "L9: 0.35", "L10: 0.13", "L11: 0.22", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:42:20.524488Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.78", "L1: 19.78", "L2: 10.17", "L3: 7.73", "L4: 4.60", "L5: 3.55", "L6: 2.12", "L7: 1.79", "L8: 0.58", "L9: 0.66", "L10: 0.23", "L11: 0.39", "L12: 0.07", "L13: 0.15"]
[2m2025-10-21T17:42:20.906075Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.92", "L1: 17.92", "L2: 9.17", "L3: 6.97", "L4: 4.21", "L5: 3.24", "L6: 1.99", "L7: 1.66", "L8: 0.57", "L9: 0.62", "L10: 0.22", "L11: 0.34", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:21.265294Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.95", "L1: 18.95", "L2: 9.76", "L3: 7.42", "L4: 4.59", "L5: 3.53", "L6: 2.15", "L7: 1.80", "L8: 0.60", "L9: 0.65", "L10: 0.23", "L11: 0.36", "L12: 0.07", "L13: 0.13"]
[2m2025-10-21T17:42:21.785143Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.75", "L1: 16.75", "L2: 8.61", "L3: 6.55", "L4: 4.03", "L5: 3.09", "L6: 1.92", "L7: 1.60", "L8: 0.55", "L9: 0.59", "L10: 0.21", "L11: 0.34", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:22.216733Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.82", "L1: 16.82", "L2: 8.82", "L3: 6.70", "L4: 4.12", "L5: 3.17", "L6: 1.96", "L7: 1.63", "L8: 0.58", "L9: 0.59", "L10: 0.21", "L11: 0.32", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:22.430282Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.53p | L9: 4.15h@0.52p | ThreshRange: [0.52-0.56] | PredNorm: 7.4208 | Complexity: 0.492 [0.163-0.744] | Temp: 5.27 [2.24-6.60] [3mepoch[0m[2m=[0m43 [3mloss[0m[2m=[0m0.16223743557929993 [3mgrad_norm[0m[2m=[0m3.4712414741516113 [3mlearning_rate[0m[2m=[0m0.0007797840517014265
[2m2025-10-21T17:42:22.747921Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.88", "L1: 13.88", "L2: 6.65", "L3: 5.05", "L4: 2.88", "L5: 2.23", "L6: 1.27", "L7: 1.09", "L8: 0.34", "L9: 0.38", "L10: 0.14", "L11: 0.24", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:42:23.081276Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.07", "L1: 18.07", "L2: 9.15", "L3: 6.94", "L4: 4.03", "L5: 3.12", "L6: 1.87", "L7: 1.59", "L8: 0.50", "L9: 0.58", "L10: 0.20", "L11: 0.34", "L12: 0.06", "L13: 0.14"]
[2m2025-10-21T17:42:23.490236Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.66", "L1: 16.66", "L2: 8.60", "L3: 6.52", "L4: 3.97", "L5: 3.03", "L6: 1.89", "L7: 1.59", "L8: 0.53", "L9: 0.57", "L10: 0.19", "L11: 0.30", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:23.882972Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.28", "L1: 15.28", "L2: 7.74", "L3: 5.87", "L4: 3.48", "L5: 2.69", "L6: 1.62", "L7: 1.38", "L8: 0.46", "L9: 0.52", "L10: 0.19", "L11: 0.32", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:24.298844Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.48", "L1: 16.48", "L2: 8.28", "L3: 6.26", "L4: 3.83", "L5: 2.92", "L6: 1.80", "L7: 1.50", "L8: 0.51", "L9: 0.52", "L10: 0.18", "L11: 0.29", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:24.685200Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.73", "L1: 17.73", "L2: 9.19", "L3: 6.96", "L4: 4.14", "L5: 3.19", "L6: 1.93", "L7: 1.61", "L8: 0.56", "L9: 0.58", "L10: 0.19", "L11: 0.31", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:24.883625Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.53p | L9: 4.13h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 7.5537 | Complexity: 0.492 [0.146-0.777] | Temp: 5.29 [2.26-6.62] [3mepoch[0m[2m=[0m44 [3mloss[0m[2m=[0m0.1512688547372818 [3mgrad_norm[0m[2m=[0m3.5536341667175293 [3mlearning_rate[0m[2m=[0m0.0007653302163816988
[2m2025-10-21T17:42:25.179700Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.99", "L1: 11.99", "L2: 5.98", "L3: 4.52", "L4: 2.65", "L5: 2.05", "L6: 1.20", "L7: 1.02", "L8: 0.31", "L9: 0.36", "L10: 0.13", "L11: 0.23", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:42:25.517160Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.73", "L1: 18.73", "L2: 9.59", "L3: 7.25", "L4: 4.27", "L5: 3.29", "L6: 1.95", "L7: 1.65", "L8: 0.52", "L9: 0.59", "L10: 0.20", "L11: 0.34", "L12: 0.06", "L13: 0.14"]
[2m2025-10-21T17:42:25.899989Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.99", "L1: 14.99", "L2: 7.95", "L3: 6.00", "L4: 3.68", "L5: 2.80", "L6: 1.74", "L7: 1.45", "L8: 0.49", "L9: 0.51", "L10: 0.17", "L11: 0.27", "L12: 0.05", "L13: 0.11"]
[2m2025-10-21T17:42:26.243613Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.19", "L1: 15.19", "L2: 7.78", "L3: 5.87", "L4: 3.53", "L5: 2.70", "L6: 1.63", "L7: 1.37", "L8: 0.46", "L9: 0.51", "L10: 0.19", "L11: 0.30", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:26.648888Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.35", "L1: 16.35", "L2: 8.56", "L3: 6.46", "L4: 4.01", "L5: 3.04", "L6: 1.86", "L7: 1.54", "L8: 0.53", "L9: 0.55", "L10: 0.19", "L11: 0.30", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:27.031033Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.62", "L1: 15.62", "L2: 7.99", "L3: 6.03", "L4: 3.60", "L5: 2.76", "L6: 1.71", "L7: 1.44", "L8: 0.49", "L9: 0.55", "L10: 0.19", "L11: 0.30", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:27.231750Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.54h@0.53p | L9: 4.13h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 7.6861 | Complexity: 0.490 [0.138-0.787] | Temp: 5.34 [2.37-6.80] [3mepoch[0m[2m=[0m45 [3mloss[0m[2m=[0m0.14607957005500793 [3mgrad_norm[0m[2m=[0m3.2767202854156494 [3mlearning_rate[0m[2m=[0m0.0007505822577513754
[2m2025-10-21T17:42:27.882529Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.67", "L1: 15.67", "L2: 7.80", "L3: 5.87", "L4: 3.37", "L5: 2.60", "L6: 1.53", "L7: 1.31", "L8: 0.41", "L9: 0.50", "L10: 0.18", "L11: 0.31", "L12: 0.06", "L13: 0.13"]
[2m2025-10-21T17:42:28.351343Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.45", "L1: 13.45", "L2: 7.15", "L3: 5.39", "L4: 3.29", "L5: 2.52", "L6: 1.53", "L7: 1.28", "L8: 0.43", "L9: 0.46", "L10: 0.16", "L11: 0.26", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:28.745449Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.74", "L1: 14.74", "L2: 7.69", "L3: 5.79", "L4: 3.47", "L5: 2.66", "L6: 1.60", "L7: 1.35", "L8: 0.45", "L9: 0.50", "L10: 0.18", "L11: 0.30", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:29.165136Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.08", "L1: 15.08", "L2: 7.83", "L3: 5.89", "L4: 3.56", "L5: 2.71", "L6: 1.67", "L7: 1.41", "L8: 0.47", "L9: 0.50", "L10: 0.18", "L11: 0.29", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:29.611894Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.68", "L1: 16.68", "L2: 8.86", "L3: 6.66", "L4: 4.05", "L5: 3.09", "L6: 1.87", "L7: 1.55", "L8: 0.54", "L9: 0.56", "L10: 0.18", "L11: 0.29", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:29.836681Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.62h@0.56p | L5: 4.55h@0.53p | L9: 4.14h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 7.8180 | Complexity: 0.491 [0.139-0.814] | Temp: 5.33 [2.38-6.64] [3mepoch[0m[2m=[0m46 [3mloss[0m[2m=[0m0.13781532645225525 [3mgrad_norm[0m[2m=[0m2.7907161712646484 [3mlearning_rate[0m[2m=[0m0.0007355604320764542
[2m2025-10-21T17:42:30.131563Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.55", "L1: 10.55", "L2: 5.12", "L3: 3.85", "L4: 2.15", "L5: 1.66", "L6: 0.94", "L7: 0.81", "L8: 0.25", "L9: 0.30", "L10: 0.12", "L11: 0.21", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:42:30.460056Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.73", "L1: 13.73", "L2: 6.89", "L3: 5.18", "L4: 2.98", "L5: 2.28", "L6: 1.34", "L7: 1.15", "L8: 0.36", "L9: 0.46", "L10: 0.18", "L11: 0.31", "L12: 0.06", "L13: 0.13"]
[2m2025-10-21T17:42:30.844005Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.54", "L1: 12.54", "L2: 6.57", "L3: 4.94", "L4: 2.96", "L5: 2.25", "L6: 1.35", "L7: 1.15", "L8: 0.38", "L9: 0.44", "L10: 0.16", "L11: 0.27", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:31.198877Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.19", "L1: 14.19", "L2: 7.36", "L3: 5.53", "L4: 3.29", "L5: 2.52", "L6: 1.51", "L7: 1.29", "L8: 0.42", "L9: 0.48", "L10: 0.18", "L11: 0.31", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:31.623833Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.46", "L1: 15.46", "L2: 8.05", "L3: 6.05", "L4: 3.73", "L5: 2.83", "L6: 1.69", "L7: 1.42", "L8: 0.48", "L9: 0.51", "L10: 0.18", "L11: 0.29", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:32.016839Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.25", "L1: 13.25", "L2: 6.80", "L3: 5.11", "L4: 3.05", "L5: 2.32", "L6: 1.40", "L7: 1.18", "L8: 0.41", "L9: 0.45", "L10: 0.16", "L11: 0.27", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:32.225604Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.62h@0.56p | L5: 4.56h@0.53p | L9: 4.16h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 7.9492 | Complexity: 0.492 [0.141-0.817] | Temp: 5.33 [2.38-6.67] [3mepoch[0m[2m=[0m47 [3mloss[0m[2m=[0m0.13254320621490479 [3mgrad_norm[0m[2m=[0m2.488616943359375 [3mlearning_rate[0m[2m=[0m0.000720284937415272
[2m2025-10-21T17:42:32.929218Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.70", "L1: 13.70", "L2: 7.05", "L3: 5.28", "L4: 3.08", "L5: 2.36", "L6: 1.37", "L7: 1.18", "L8: 0.37", "L9: 0.45", "L10: 0.17", "L11: 0.30", "L12: 0.05", "L13: 0.13"]
[2m2025-10-21T17:42:33.343008Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.16", "L1: 12.16", "L2: 6.36", "L3: 4.77", "L4: 2.89", "L5: 2.19", "L6: 1.32", "L7: 1.12", "L8: 0.37", "L9: 0.41", "L10: 0.14", "L11: 0.24", "L12: 0.04", "L13: 0.10"]
[2m2025-10-21T17:42:33.718819Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.35", "L1: 14.35", "L2: 7.43", "L3: 5.57", "L4: 3.30", "L5: 2.52", "L6: 1.47", "L7: 1.26", "L8: 0.41", "L9: 0.49", "L10: 0.17", "L11: 0.29", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:42:34.148054Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.82", "L1: 13.82", "L2: 7.23", "L3: 5.41", "L4: 3.25", "L5: 2.45", "L6: 1.47", "L7: 1.24", "L8: 0.42", "L9: 0.46", "L10: 0.17", "L11: 0.29", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:34.565258Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.65", "L1: 13.65", "L2: 7.20", "L3: 5.38", "L4: 3.28", "L5: 2.50", "L6: 1.53", "L7: 1.28", "L8: 0.44", "L9: 0.48", "L10: 0.16", "L11: 0.26", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:34.788818Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.62h@0.56p | L5: 4.58h@0.53p | L9: 4.21h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 8.0776 | Complexity: 0.496 [0.138-0.852] | Temp: 5.31 [2.34-6.72] [3mepoch[0m[2m=[0m48 [3mloss[0m[2m=[0m0.12674346566200256 [3mgrad_norm[0m[2m=[0m2.4379971027374268 [3mlearning_rate[0m[2m=[0m0.0007047770195640624
[2m2025-10-21T17:42:35.473904Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.29", "L1: 16.29", "L2: 8.43", "L3: 6.29", "L4: 3.74", "L5: 2.84", "L6: 1.65", "L7: 1.41", "L8: 0.45", "L9: 0.52", "L10: 0.18", "L11: 0.32", "L12: 0.06", "L13: 0.13"]
[2m2025-10-21T17:42:35.843499Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.33", "L1: 11.33", "L2: 5.70", "L3: 4.27", "L4: 2.49", "L5: 1.90", "L6: 1.12", "L7: 0.97", "L8: 0.32", "L9: 0.37", "L10: 0.15", "L11: 0.25", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:36.202757Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.03", "L1: 13.03", "L2: 6.63", "L3: 4.96", "L4: 2.86", "L5: 2.19", "L6: 1.32", "L7: 1.13", "L8: 0.36", "L9: 0.43", "L10: 0.17", "L11: 0.28", "L12: 0.05", "L13: 0.11"]
[2m2025-10-21T17:42:36.631464Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.28", "L1: 15.28", "L2: 8.05", "L3: 6.01", "L4: 3.61", "L5: 2.72", "L6: 1.63", "L7: 1.39", "L8: 0.46", "L9: 0.50", "L10: 0.17", "L11: 0.29", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:37.036595Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.53", "L1: 13.53", "L2: 7.17", "L3: 5.35", "L4: 3.21", "L5: 2.45", "L6: 1.46", "L7: 1.24", "L8: 0.41", "L9: 0.48", "L10: 0.17", "L11: 0.28", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:37.249776Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.62h@0.56p | L5: 4.57h@0.53p | L9: 4.06h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 8.2025 | Complexity: 0.489 [0.142-0.854] | Temp: 5.30 [2.25-6.71] [3mepoch[0m[2m=[0m49 [3mloss[0m[2m=[0m0.1254839152097702 [3mgrad_norm[0m[2m=[0m2.5314440727233887 [3mlearning_rate[0m[2m=[0m0.0006890576332807541
[2m2025-10-21T17:42:37.541726Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.89", "L1: 13.89", "L2: 7.01", "L3: 5.23", "L4: 3.00", "L5: 2.27", "L6: 1.31", "L7: 1.11", "L8: 0.35", "L9: 0.40", "L10: 0.10", "L11: 0.17", "L12: 0.03", "L13: 0.07"]
[2m2025-10-21T17:42:37.863416Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.23", "L1: 14.23", "L2: 7.30", "L3: 5.44", "L4: 3.06", "L5: 2.33", "L6: 1.33", "L7: 1.15", "L8: 0.35", "L9: 0.44", "L10: 0.17", "L11: 0.29", "L12: 0.05", "L13: 0.13"]
[2m2025-10-21T17:42:38.253731Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.17", "L1: 11.17", "L2: 5.75", "L3: 4.28", "L4: 2.53", "L5: 1.90", "L6: 1.12", "L7: 0.96", "L8: 0.32", "L9: 0.36", "L10: 0.14", "L11: 0.22", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:42:38.629858Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.77", "L1: 15.77", "L2: 8.32", "L3: 6.19", "L4: 3.62", "L5: 2.74", "L6: 1.65", "L7: 1.41", "L8: 0.44", "L9: 0.50", "L10: 0.16", "L11: 0.27", "L12: 0.05", "L13: 0.11"]
[2m2025-10-21T17:42:39.074229Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.47", "L1: 14.47", "L2: 7.37", "L3: 5.49", "L4: 3.24", "L5: 2.45", "L6: 1.47", "L7: 1.25", "L8: 0.41", "L9: 0.45", "L10: 0.16", "L11: 0.27", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:39.471488Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.58", "L1: 16.58", "L2: 8.72", "L3: 6.48", "L4: 3.78", "L5: 2.86", "L6: 1.72", "L7: 1.46", "L8: 0.47", "L9: 0.54", "L10: 0.17", "L11: 0.28", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:39.676161Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.58h@0.53p | L9: 3.93h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 8.3190 | Complexity: 0.482 [0.145-0.886] | Temp: 5.33 [2.29-6.72] [3mepoch[0m[2m=[0m50 [3mloss[0m[2m=[0m0.12227654457092285 [3mgrad_norm[0m[2m=[0m4.029362678527832 [3mlearning_rate[0m[2m=[0m0.0006731483153998852
[2m2025-10-21T17:42:39.974071Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.20", "L1: 15.20", "L2: 7.69", "L3: 5.70", "L4: 3.19", "L5: 2.42", "L6: 1.37", "L7: 1.18", "L8: 0.35", "L9: 0.43", "L10: 0.13", "L11: 0.22", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:42:40.323994Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.87", "L1: 16.87", "L2: 8.66", "L3: 6.45", "L4: 3.68", "L5: 2.80", "L6: 1.65", "L7: 1.41", "L8: 0.43", "L9: 0.52", "L10: 0.18", "L11: 0.32", "L12: 0.06", "L13: 0.13"]
[2m2025-10-21T17:42:40.694503Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.88", "L1: 13.88", "L2: 7.32", "L3: 5.43", "L4: 3.18", "L5: 2.40", "L6: 1.40", "L7: 1.19", "L8: 0.39", "L9: 0.43", "L10: 0.15", "L11: 0.25", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:41.074419Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.92", "L1: 16.92", "L2: 8.71", "L3: 6.48", "L4: 3.75", "L5: 2.84", "L6: 1.69", "L7: 1.45", "L8: 0.46", "L9: 0.53", "L10: 0.18", "L11: 0.31", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:41.497779Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.13", "L1: 15.13", "L2: 8.07", "L3: 5.99", "L4: 3.61", "L5: 2.69", "L6: 1.61", "L7: 1.36", "L8: 0.45", "L9: 0.48", "L10: 0.17", "L11: 0.28", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:41.879210Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.72", "L1: 18.72", "L2: 9.76", "L3: 7.26", "L4: 4.20", "L5: 3.19", "L6: 1.88", "L7: 1.61", "L8: 0.52", "L9: 0.56", "L10: 0.18", "L11: 0.30", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:42.079028Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.53p | L9: 3.80h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 8.4322 | Complexity: 0.475 [0.136-0.869] | Temp: 5.35 [2.30-7.01] [3mepoch[0m[2m=[0m51 [3mloss[0m[2m=[0m0.1332496553659439 [3mgrad_norm[0m[2m=[0m3.6115124225616455 [3mlearning_rate[0m[2m=[0m0.0006570708355866373
[2m2025-10-21T17:42:42.363155Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.37", "L1: 19.37", "L2: 9.57", "L3: 7.12", "L4: 3.85", "L5: 2.97", "L6: 1.65", "L7: 1.45", "L8: 0.42", "L9: 0.47", "L10: 0.15", "L11: 0.25", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:42:42.701712Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.46", "L1: 15.46", "L2: 7.93", "L3: 5.89", "L4: 3.34", "L5: 2.55", "L6: 1.45", "L7: 1.26", "L8: 0.39", "L9: 0.49", "L10: 0.19", "L11: 0.33", "L12: 0.06", "L13: 0.13"]
[2m2025-10-21T17:42:43.093820Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.90", "L1: 14.90", "L2: 7.62", "L3: 5.65", "L4: 3.24", "L5: 2.46", "L6: 1.42", "L7: 1.22", "L8: 0.38", "L9: 0.43", "L10: 0.14", "L11: 0.24", "L12: 0.04", "L13: 0.10"]
[2m2025-10-21T17:42:43.462042Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.99", "L1: 18.99", "L2: 9.81", "L3: 7.27", "L4: 4.19", "L5: 3.16", "L6: 1.88", "L7: 1.62", "L8: 0.51", "L9: 0.53", "L10: 0.18", "L11: 0.30", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:43.863948Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.02", "L1: 16.02", "L2: 8.40", "L3: 6.23", "L4: 3.74", "L5: 2.79", "L6: 1.67", "L7: 1.41", "L8: 0.47", "L9: 0.51", "L10: 0.17", "L11: 0.29", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:44.249743Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.31", "L1: 17.31", "L2: 9.08", "L3: 6.73", "L4: 4.00", "L5: 3.01", "L6: 1.77", "L7: 1.50", "L8: 0.50", "L9: 0.53", "L10: 0.18", "L11: 0.29", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:44.448359Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.53p | L9: 3.79h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 8.5393 | Complexity: 0.476 [0.089-0.875] | Temp: 5.31 [2.28-6.86] [3mepoch[0m[2m=[0m52 [3mloss[0m[2m=[0m0.13300998508930206 [3mgrad_norm[0m[2m=[0m4.124411106109619 [3mlearning_rate[0m[2m=[0m0.0006408470799215138
[2m2025-10-21T17:42:44.732348Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.76", "L1: 18.76", "L2: 9.47", "L3: 7.01", "L4: 3.92", "L5: 2.99", "L6: 1.68", "L7: 1.49", "L8: 0.44", "L9: 0.53", "L10: 0.18", "L11: 0.32", "L12: 0.06", "L13: 0.11"]
[2m2025-10-21T17:42:45.068999Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.23", "L1: 19.23", "L2: 9.87", "L3: 7.30", "L4: 4.15", "L5: 3.14", "L6: 1.74", "L7: 1.50", "L8: 0.46", "L9: 0.57", "L10: 0.20", "L11: 0.35", "L12: 0.06", "L13: 0.14"]
[2m2025-10-21T17:42:45.469470Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.06", "L1: 17.06", "L2: 9.08", "L3: 6.71", "L4: 3.97", "L5: 2.97", "L6: 1.66", "L7: 1.42", "L8: 0.46", "L9: 0.51", "L10: 0.16", "L11: 0.27", "L12: 0.05", "L13: 0.11"]
[2m2025-10-21T17:42:45.832400Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.15", "L1: 18.15", "L2: 9.55", "L3: 7.07", "L4: 4.01", "L5: 3.04", "L6: 1.72", "L7: 1.50", "L8: 0.47", "L9: 0.59", "L10: 0.19", "L11: 0.34", "L12: 0.06", "L13: 0.13"]
[2m2025-10-21T17:42:46.251748Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.22", "L1: 16.22", "L2: 8.47", "L3: 6.27", "L4: 3.68", "L5: 2.77", "L6: 1.61", "L7: 1.38", "L8: 0.46", "L9: 0.50", "L10: 0.17", "L11: 0.28", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:46.678540Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.69", "L1: 18.69", "L2: 9.99", "L3: 7.39", "L4: 4.42", "L5: 3.30", "L6: 1.80", "L7: 1.54", "L8: 0.50", "L9: 0.52", "L10: 0.17", "L11: 0.29", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:46.885371Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.78h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 8.6448 | Complexity: 0.475 [0.082-0.872] | Temp: 5.33 [2.32-6.95] [3mepoch[0m[2m=[0m53 [3mloss[0m[2m=[0m0.13912954926490784 [3mgrad_norm[0m[2m=[0m3.8080897331237793 [3mlearning_rate[0m[2m=[0m0.0006244992255233228
[2m2025-10-21T17:42:47.191820Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.76", "L1: 16.76", "L2: 8.42", "L3: 6.22", "L4: 3.45", "L5: 2.63", "L6: 1.49", "L7: 1.32", "L8: 0.38", "L9: 0.46", "L10: 0.15", "L11: 0.26", "L12: 0.05", "L13: 0.09"]
[2m2025-10-21T17:42:47.526605Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.68", "L1: 15.68", "L2: 7.87", "L3: 5.82", "L4: 3.23", "L5: 2.44", "L6: 1.40", "L7: 1.22", "L8: 0.37", "L9: 0.49", "L10: 0.18", "L11: 0.32", "L12: 0.06", "L13: 0.13"]
[2m2025-10-21T17:42:47.897579Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.70", "L1: 16.70", "L2: 8.66", "L3: 6.39", "L4: 3.72", "L5: 2.79", "L6: 1.56", "L7: 1.35", "L8: 0.43", "L9: 0.48", "L10: 0.16", "L11: 0.27", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:48.299209Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.15", "L1: 16.15", "L2: 8.34", "L3: 6.16", "L4: 3.56", "L5: 2.69", "L6: 1.54", "L7: 1.35", "L8: 0.42", "L9: 0.48", "L10: 0.18", "L11: 0.31", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:48.720783Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.35", "L1: 15.35", "L2: 7.92", "L3: 5.85", "L4: 3.41", "L5: 2.55", "L6: 1.53", "L7: 1.32", "L8: 0.43", "L9: 0.46", "L10: 0.16", "L11: 0.28", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:49.102679Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.60", "L1: 15.60", "L2: 8.61", "L3: 6.36", "L4: 3.91", "L5: 2.89", "L6: 1.60", "L7: 1.36", "L8: 0.47", "L9: 0.49", "L10: 0.17", "L11: 0.28", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:49.321442Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.59h@0.54p | L9: 3.80h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 8.7478 | Complexity: 0.477 [0.079-0.861] | Temp: 5.33 [2.38-6.90] [3mepoch[0m[2m=[0m54 [3mloss[0m[2m=[0m0.12722128629684448 [3mgrad_norm[0m[2m=[0m3.3041272163391113 [3mlearning_rate[0m[2m=[0m0.000608049682341516
[2m2025-10-21T17:42:49.613282Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.78", "L1: 15.78", "L2: 7.76", "L3: 5.73", "L4: 3.20", "L5: 2.42", "L6: 1.37", "L7: 1.21", "L8: 0.36", "L9: 0.42", "L10: 0.15", "L11: 0.26", "L12: 0.05", "L13: 0.09"]
[2m2025-10-21T17:42:49.947007Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.05", "L1: 16.05", "L2: 7.94", "L3: 5.86", "L4: 3.20", "L5: 2.42", "L6: 1.34", "L7: 1.19", "L8: 0.36", "L9: 0.46", "L10: 0.17", "L11: 0.31", "L12: 0.06", "L13: 0.13"]
[2m2025-10-21T17:42:50.354417Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.43", "L1: 17.43", "L2: 8.93", "L3: 6.59", "L4: 3.82", "L5: 2.87", "L6: 1.66", "L7: 1.46", "L8: 0.47", "L9: 0.52", "L10: 0.17", "L11: 0.29", "L12: 0.05", "L13: 0.11"]
[2m2025-10-21T17:42:50.725378Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.95", "L1: 17.95", "L2: 9.36", "L3: 6.91", "L4: 3.98", "L5: 2.98", "L6: 1.72", "L7: 1.49", "L8: 0.48", "L9: 0.54", "L10: 0.18", "L11: 0.31", "L12: 0.06", "L13: 0.12"]
[2m2025-10-21T17:42:51.239717Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.75", "L1: 13.75", "L2: 7.08", "L3: 5.23", "L4: 3.06", "L5: 2.30", "L6: 1.33", "L7: 1.14", "L8: 0.37", "L9: 0.41", "L10: 0.15", "L11: 0.25", "L12: 0.05", "L13: 0.09"]
[2m2025-10-21T17:42:51.748168Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.72", "L1: 14.72", "L2: 7.91", "L3: 5.84", "L4: 3.53", "L5: 2.64", "L6: 1.50", "L7: 1.28", "L8: 0.43", "L9: 0.47", "L10: 0.16", "L11: 0.26", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:52.009940Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.54p | L9: 3.74h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 8.8478 | Complexity: 0.473 [0.080-0.857] | Temp: 5.34 [2.38-6.90] [3mepoch[0m[2m=[0m55 [3mloss[0m[2m=[0m0.12476032972335815 [3mgrad_norm[0m[2m=[0m3.2279248237609863 [3mlearning_rate[0m[2m=[0m0.0005915207439102232
[2m2025-10-21T17:42:52.395693Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.53", "L1: 11.53", "L2: 5.63", "L3: 4.16", "L4: 2.30", "L5: 1.75", "L6: 1.00", "L7: 0.89", "L8: 0.26", "L9: 0.33", "L10: 0.13", "L11: 0.23", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:42:52.841425Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.15", "L1: 16.15", "L2: 7.94", "L3: 5.86", "L4: 3.23", "L5: 2.47", "L6: 1.35", "L7: 1.21", "L8: 0.35", "L9: 0.46", "L10: 0.17", "L11: 0.31", "L12: 0.06", "L13: 0.13"]
[2m2025-10-21T17:42:53.279602Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.63", "L1: 15.63", "L2: 7.88", "L3: 5.81", "L4: 3.35", "L5: 2.51", "L6: 1.41", "L7: 1.25", "L8: 0.38", "L9: 0.45", "L10: 0.15", "L11: 0.26", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:53.759880Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.09", "L1: 15.09", "L2: 7.63", "L3: 5.62", "L4: 3.23", "L5: 2.42", "L6: 1.35", "L7: 1.19", "L8: 0.37", "L9: 0.43", "L10: 0.16", "L11: 0.29", "L12: 0.05", "L13: 0.11"]
[2m2025-10-21T17:42:54.247323Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.22", "L1: 16.22", "L2: 8.09", "L3: 5.96", "L4: 3.45", "L5: 2.58", "L6: 1.51", "L7: 1.32", "L8: 0.42", "L9: 0.47", "L10: 0.16", "L11: 0.27", "L12: 0.05", "L13: 0.09"]
[2m2025-10-21T17:42:54.787849Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.80", "L1: 13.80", "L2: 7.09", "L3: 5.23", "L4: 3.05", "L5: 2.28", "L6: 1.32", "L7: 1.15", "L8: 0.38", "L9: 0.42", "L10: 0.15", "L11: 0.25", "L12: 0.05", "L13: 0.09"]
[2m2025-10-21T17:42:55.071247Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.73h@0.54p | ThreshRange: [0.54-0.56] | PredNorm: 8.9455 | Complexity: 0.473 [0.088-0.874] | Temp: 5.34 [2.32-6.90] [3mepoch[0m[2m=[0m56 [3mloss[0m[2m=[0m0.11784493923187256 [3mgrad_norm[0m[2m=[0m3.03902268409729 [3mlearning_rate[0m[2m=[0m0.0005749351694248617
[2m2025-10-21T17:42:55.467559Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.40", "L1: 12.40", "L2: 5.84", "L3: 4.30", "L4: 2.27", "L5: 1.74", "L6: 0.95", "L7: 0.86", "L8: 0.24", "L9: 0.30", "L10: 0.10", "L11: 0.18", "L12: 0.03", "L13: 0.07"]
[2m2025-10-21T17:42:55.872737Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.20", "L1: 17.20", "L2: 8.38", "L3: 6.18", "L4: 3.34", "L5: 2.54", "L6: 1.41", "L7: 1.28", "L8: 0.37", "L9: 0.47", "L10: 0.17", "L11: 0.32", "L12: 0.06", "L13: 0.13"]
[2m2025-10-21T17:42:56.354464Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.82", "L1: 13.82", "L2: 6.93", "L3: 5.10", "L4: 2.86", "L5: 2.16", "L6: 1.22", "L7: 1.08", "L8: 0.34", "L9: 0.39", "L10: 0.15", "L11: 0.26", "L12: 0.05", "L13: 0.10"]
[2m2025-10-21T17:42:56.792224Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.99", "L1: 15.99", "L2: 8.25", "L3: 6.07", "L4: 3.50", "L5: 2.61", "L6: 1.48", "L7: 1.29", "L8: 0.41", "L9: 0.47", "L10: 0.16", "L11: 0.28", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:42:57.427935Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.60", "L1: 12.60", "L2: 6.44", "L3: 4.74", "L4: 2.77", "L5: 2.06", "L6: 1.18", "L7: 1.03", "L8: 0.33", "L9: 0.39", "L10: 0.14", "L11: 0.25", "L12: 0.05", "L13: 0.09"]
[2m2025-10-21T17:42:57.932998Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.31", "L1: 13.31", "L2: 7.09", "L3: 5.21", "L4: 3.07", "L5: 2.29", "L6: 1.35", "L7: 1.16", "L8: 0.38", "L9: 0.41", "L10: 0.14", "L11: 0.24", "L12: 0.05", "L13: 0.09"]
[2m2025-10-21T17:42:58.191568Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.59h@0.54p | L9: 3.79h@0.54p | ThreshRange: [0.54-0.56] | PredNorm: 9.0410 | Complexity: 0.476 [0.095-0.872] | Temp: 5.32 [2.28-6.74] [3mepoch[0m[2m=[0m57 [3mloss[0m[2m=[0m0.11345888674259186 [3mgrad_norm[0m[2m=[0m2.744495391845703 [3mlearning_rate[0m[2m=[0m0.0005583155434578657
[2m2025-10-21T17:42:58.950980Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.31", "L1: 14.31", "L2: 7.09", "L3: 5.22", "L4: 2.87", "L5: 2.16", "L6: 1.20", "L7: 1.07", "L8: 0.32", "L9: 0.41", "L10: 0.15", "L11: 0.28", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:42:59.451136Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.05", "L1: 13.05", "L2: 6.65", "L3: 4.89", "L4: 2.82", "L5: 2.12", "L6: 1.23", "L7: 1.07", "L8: 0.35", "L9: 0.41", "L10: 0.14", "L11: 0.24", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:42:59.917131Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.61", "L1: 14.61", "L2: 7.28", "L3: 5.35", "L4: 3.07", "L5: 2.31", "L6: 1.32", "L7: 1.18", "L8: 0.37", "L9: 0.42", "L10: 0.15", "L11: 0.27", "L12: 0.05", "L13: 0.11"]
[2m2025-10-21T17:43:00.443794Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.11", "L1: 12.11", "L2: 6.32", "L3: 4.64", "L4: 2.72", "L5: 2.01", "L6: 1.18", "L7: 1.02", "L8: 0.33", "L9: 0.36", "L10: 0.12", "L11: 0.21", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:00.989576Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.96", "L1: 11.96", "L2: 6.21", "L3: 4.57", "L4: 2.66", "L5: 1.98", "L6: 1.16", "L7: 1.00", "L8: 0.34", "L9: 0.37", "L10: 0.14", "L11: 0.24", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:01.252288Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.58h@0.54p | L9: 3.76h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 9.1338 | Complexity: 0.475 [0.088-0.872] | Temp: 5.31 [2.32-6.73] [3mepoch[0m[2m=[0m58 [3mloss[0m[2m=[0m0.106061190366745 [3mgrad_norm[0m[2m=[0m2.4543628692626953 [3mlearning_rate[0m[2m=[0m0.0005416845087893307
[2m2025-10-21T17:43:02.070552Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.96", "L1: 11.96", "L2: 5.82", "L3: 4.27", "L4: 2.32", "L5: 1.76", "L6: 1.00", "L7: 0.90", "L8: 0.27", "L9: 0.36", "L10: 0.14", "L11: 0.26", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:43:02.553097Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.82", "L1: 10.82", "L2: 5.47", "L3: 4.02", "L4: 2.28", "L5: 1.70", "L6: 0.97", "L7: 0.86", "L8: 0.27", "L9: 0.32", "L10: 0.12", "L11: 0.22", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:03.046024Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.47", "L1: 13.47", "L2: 6.86", "L3: 5.04", "L4: 2.86", "L5: 2.15", "L6: 1.20", "L7: 1.06", "L8: 0.34", "L9: 0.39", "L10: 0.15", "L11: 0.26", "L12: 0.05", "L13: 0.11"]
[2m2025-10-21T17:43:03.576963Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.37", "L1: 11.37", "L2: 5.83", "L3: 4.28", "L4: 2.48", "L5: 1.84", "L6: 1.07", "L7: 0.93", "L8: 0.30", "L9: 0.34", "L10: 0.12", "L11: 0.21", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:43:04.105326Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.21", "L1: 12.21", "L2: 6.25", "L3: 4.59", "L4: 2.61", "L5: 1.95", "L6: 1.11", "L7: 0.97", "L8: 0.32", "L9: 0.36", "L10: 0.13", "L11: 0.23", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:04.376706Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.60h@0.54p | L9: 3.78h@0.54p | ThreshRange: [0.54-0.56] | PredNorm: 9.2235 | Complexity: 0.477 [0.086-0.890] | Temp: 5.30 [2.33-6.72] [3mepoch[0m[2m=[0m59 [3mloss[0m[2m=[0m0.10142441838979721 [3mgrad_norm[0m[2m=[0m2.2469961643218994 [3mlearning_rate[0m[2m=[0m0.0005250648246146739
[2m2025-10-21T17:43:05.283154Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.68", "L1: 14.68", "L2: 7.21", "L3: 5.29", "L4: 2.80", "L5: 2.13", "L6: 1.18", "L7: 1.07", "L8: 0.32", "L9: 0.39", "L10: 0.14", "L11: 0.27", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:43:05.829346Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.40", "L1: 11.40", "L2: 5.81", "L3: 4.26", "L4: 2.39", "L5: 1.78", "L6: 1.00", "L7: 0.89", "L8: 0.27", "L9: 0.33", "L10: 0.12", "L11: 0.21", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:06.366594Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.92", "L1: 12.92", "L2: 6.46", "L3: 4.74", "L4: 2.68", "L5: 2.00", "L6: 1.11", "L7: 0.99", "L8: 0.31", "L9: 0.38", "L10: 0.13", "L11: 0.23", "L12: 0.04", "L13: 0.10"]
[2m2025-10-21T17:43:06.958614Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.35", "L1: 10.35", "L2: 5.27", "L3: 3.86", "L4: 2.26", "L5: 1.67", "L6: 0.98", "L7: 0.85", "L8: 0.28", "L9: 0.32", "L10: 0.12", "L11: 0.22", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:07.462430Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.77", "L1: 11.77", "L2: 6.15", "L3: 4.51", "L4: 2.62", "L5: 1.96", "L6: 1.11", "L7: 0.97", "L8: 0.31", "L9: 0.36", "L10: 0.13", "L11: 0.22", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:07.696954Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.58h@0.54p | L9: 3.79h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 9.3100 | Complexity: 0.476 [0.083-0.890] | Temp: 5.30 [2.31-6.72] [3mepoch[0m[2m=[0m60 [3mloss[0m[2m=[0m0.0996592715382576 [3mgrad_norm[0m[2m=[0m2.462339401245117 [3mlearning_rate[0m[2m=[0m0.0005084791919216514
[2m2025-10-21T17:43:08.475333Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.21", "L1: 13.21", "L2: 6.70", "L3: 4.91", "L4: 2.80", "L5: 2.10", "L6: 1.18", "L7: 1.04", "L8: 0.31", "L9: 0.40", "L10: 0.14", "L11: 0.26", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:43:08.966941Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.56", "L1: 11.56", "L2: 5.75", "L3: 4.22", "L4: 2.36", "L5: 1.76", "L6: 0.99", "L7: 0.88", "L8: 0.27", "L9: 0.32", "L10: 0.12", "L11: 0.22", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:09.396142Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.74", "L1: 10.74", "L2: 5.32", "L3: 3.89", "L4: 2.19", "L5: 1.65", "L6: 0.93", "L7: 0.84", "L8: 0.26", "L9: 0.32", "L10: 0.12", "L11: 0.23", "L12: 0.04", "L13: 0.10"]
[2m2025-10-21T17:43:09.924465Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.05", "L1: 11.05", "L2: 5.59", "L3: 4.09", "L4: 2.39", "L5: 1.76", "L6: 1.04", "L7: 0.91", "L8: 0.29", "L9: 0.33", "L10: 0.12", "L11: 0.21", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:43:10.465329Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.10", "L1: 11.10", "L2: 5.85", "L3: 4.28", "L4: 2.52", "L5: 1.86", "L6: 1.12", "L7: 0.97", "L8: 0.32", "L9: 0.35", "L10: 0.11", "L11: 0.20", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:43:10.752653Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.78h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 9.3937 | Complexity: 0.475 [0.078-0.887] | Temp: 5.30 [2.34-6.73] [3mepoch[0m[2m=[0m61 [3mloss[0m[2m=[0m0.09782162308692932 [3mgrad_norm[0m[2m=[0m2.2346112728118896 [3mlearning_rate[0m[2m=[0m0.0004919504281133413
[2m2025-10-21T17:43:11.510520Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.50", "L1: 11.50", "L2: 5.79", "L3: 4.24", "L4: 2.38", "L5: 1.79", "L6: 1.00", "L7: 0.90", "L8: 0.26", "L9: 0.35", "L10: 0.13", "L11: 0.24", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:43:12.951762Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.10", "L1: 11.10", "L2: 5.63", "L3: 4.11", "L4: 2.33", "L5: 1.75", "L6: 0.99", "L7: 0.89", "L8: 0.28", "L9: 0.32", "L10: 0.12", "L11: 0.23", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:13.731010Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.54p | L9: 3.83h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 9.4745 | Complexity: 0.477 [0.085-0.887] | Temp: 5.32 [2.38-6.76] [3mepoch[0m[2m=[0m62 [3mloss[0m[2m=[0m0.09361216425895691 [3mgrad_norm[0m[2m=[0m1.879309058189392 [3mlearning_rate[0m[2m=[0m0.0004755007685162127
[2m2025-10-21T17:43:14.470711Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.99", "L1: 11.99", "L2: 5.81", "L3: 4.24", "L4: 2.30", "L5: 1.73", "L6: 0.95", "L7: 0.86", "L8: 0.25", "L9: 0.36", "L10: 0.14", "L11: 0.26", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:43:14.971354Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.11", "L1: 12.11", "L2: 6.40", "L3: 4.68", "L4: 2.66", "L5: 1.96", "L6: 1.08", "L7: 0.95", "L8: 0.30", "L9: 0.35", "L10: 0.11", "L11: 0.20", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:15.435362Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.03", "L1: 11.03", "L2: 5.48", "L3: 4.00", "L4: 2.26", "L5: 1.69", "L6: 0.92", "L7: 0.82", "L8: 0.25", "L9: 0.30", "L10: 0.11", "L11: 0.20", "L12: 0.04", "L13: 0.10"]
[2m2025-10-21T17:43:15.993336Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.10", "L1: 11.10", "L2: 5.69", "L3: 4.15", "L4: 2.45", "L5: 1.81", "L6: 1.01", "L7: 0.89", "L8: 0.28", "L9: 0.33", "L10: 0.12", "L11: 0.23", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:43:16.521099Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.36", "L1: 10.36", "L2: 5.44", "L3: 3.98", "L4: 2.26", "L5: 1.68", "L6: 0.95", "L7: 0.83", "L8: 0.27", "L9: 0.31", "L10: 0.11", "L11: 0.20", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:43:16.772703Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.58h@0.54p | L9: 3.89h@0.53p | ThreshRange: [0.53-0.56] | PredNorm: 9.5526 | Complexity: 0.481 [0.095-0.885] | Temp: 5.32 [2.36-6.80] [3mepoch[0m[2m=[0m63 [3mloss[0m[2m=[0m0.09387362003326416 [3mgrad_norm[0m[2m=[0m2.5461297035217285 [3mlearning_rate[0m[2m=[0m0.0004591529432218522
[2m2025-10-21T17:43:17.566697Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.44", "L1: 11.44", "L2: 5.66", "L3: 4.13", "L4: 2.25", "L5: 1.70", "L6: 0.92", "L7: 0.84", "L8: 0.24", "L9: 0.33", "L10: 0.13", "L11: 0.25", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:43:18.047814Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.06", "L1: 11.06", "L2: 5.75", "L3: 4.19", "L4: 2.46", "L5: 1.82", "L6: 1.07", "L7: 0.93", "L8: 0.29", "L9: 0.33", "L10: 0.11", "L11: 0.20", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:18.548577Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.35", "L1: 11.35", "L2: 5.88", "L3: 4.29", "L4: 2.44", "L5: 1.82", "L6: 1.01", "L7: 0.89", "L8: 0.28", "L9: 0.32", "L10: 0.12", "L11: 0.22", "L12: 0.04", "L13: 0.10"]
[2m2025-10-21T17:43:19.145973Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.19", "L1: 10.19", "L2: 5.23", "L3: 3.81", "L4: 2.19", "L5: 1.62", "L6: 0.94", "L7: 0.83", "L8: 0.26", "L9: 0.31", "L10: 0.11", "L11: 0.20", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:43:19.725404Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.81", "L1: 11.81", "L2: 6.21", "L3: 4.53", "L4: 2.64", "L5: 1.96", "L6: 1.04", "L7: 0.91", "L8: 0.30", "L9: 0.34", "L10: 0.12", "L11: 0.21", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:43:20.002206Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.58h@0.54p | L9: 3.84h@0.54p | ThreshRange: [0.54-0.56] | PredNorm: 9.6285 | Complexity: 0.479 [0.093-0.880] | Temp: 5.31 [2.33-6.84] [3mepoch[0m[2m=[0m64 [3mloss[0m[2m=[0m0.09428361058235168 [3mgrad_norm[0m[2m=[0m2.133200168609619 [3mlearning_rate[0m[2m=[0m0.0004429291293490678
[2m2025-10-21T17:43:20.863839Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.80", "L1: 10.80", "L2: 5.44", "L3: 3.97", "L4: 2.17", "L5: 1.63", "L6: 0.91", "L7: 0.82", "L8: 0.24", "L9: 0.33", "L10: 0.12", "L11: 0.24", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:43:21.865326Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.52", "L1: 13.52", "L2: 6.69", "L3: 4.86", "L4: 2.78", "L5: 2.03", "L6: 1.15", "L7: 1.00", "L8: 0.31", "L9: 0.34", "L10: 0.11", "L11: 0.21", "L12: 0.04", "L13: 0.10"]
[2m2025-10-21T17:43:22.892299Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.33", "L1: 10.33", "L2: 5.46", "L3: 3.98", "L4: 2.29", "L5: 1.70", "L6: 0.97", "L7: 0.85", "L8: 0.28", "L9: 0.32", "L10: 0.12", "L11: 0.22", "L12: 0.04", "L13: 0.09"]
[2m2025-10-21T17:43:23.152963Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.73h@0.54p | ThreshRange: [0.54-0.56] | PredNorm: 9.7010 | Complexity: 0.473 [0.094-0.885] | Temp: 5.31 [2.31-6.86] [3mepoch[0m[2m=[0m65 [3mloss[0m[2m=[0m0.08922053873538971 [3mgrad_norm[0m[2m=[0m2.3019931316375732 [3mlearning_rate[0m[2m=[0m0.0004268516495358199
[2m2025-10-21T17:43:23.940415Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.52", "L1: 10.52", "L2: 5.45", "L3: 3.96", "L4: 2.23", "L5: 1.65", "L6: 0.85", "L7: 0.76", "L8: 0.23", "L9: 0.30", "L10: 0.11", "L11: 0.21", "L12: 0.04", "L13: 0.11"]
[2m2025-10-21T17:43:25.446543Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.26", "L1: 10.26", "L2: 5.19", "L3: 3.77", "L4: 2.17", "L5: 1.60", "L6: 0.88", "L7: 0.78", "L8: 0.24", "L9: 0.29", "L10: 0.11", "L11: 0.19", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:43:25.961130Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.15", "L1: 10.15", "L2: 5.34", "L3: 3.89", "L4: 2.22", "L5: 1.64", "L6: 0.93", "L7: 0.82", "L8: 0.26", "L9: 0.32", "L10: 0.11", "L11: 0.20", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:43:26.240081Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.53p | L9: 3.71h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 9.7705 | Complexity: 0.472 [0.094-0.891] | Temp: 5.31 [2.29-6.88] [3mepoch[0m[2m=[0m66 [3mloss[0m[2m=[0m0.08697398751974106 [3mgrad_norm[0m[2m=[0m1.7788660526275635 [3mlearning_rate[0m[2m=[0m0.00041094227344729006
[2m2025-10-21T17:43:27.937731Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.52", "L1: 10.52", "L2: 5.28", "L3: 3.83", "L4: 2.14", "L5: 1.59", "L6: 0.85", "L7: 0.77", "L8: 0.23", "L9: 0.29", "L10: 0.11", "L11: 0.21", "L12: 0.04", "L13: 0.10"]
[2m2025-10-21T17:43:28.508889Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.69", "L1: 10.69", "L2: 5.47", "L3: 3.96", "L4: 2.29", "L5: 1.68", "L6: 0.96", "L7: 0.85", "L8: 0.26", "L9: 0.30", "L10: 0.10", "L11: 0.19", "L12: 0.04", "L13: 0.08"]
[2m2025-10-21T17:43:28.980991Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.00", "L1: 10.00", "L2: 5.31", "L3: 3.86", "L4: 2.22", "L5: 1.64", "L6: 0.90", "L7: 0.79", "L8: 0.25", "L9: 0.29", "L10: 0.10", "L11: 0.18", "L12: 0.03", "L13: 0.08"]
[2m2025-10-21T17:43:29.220561Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.53p | L9: 3.73h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 9.8373 | Complexity: 0.473 [0.090-0.891] | Temp: 5.31 [2.27-6.87] [3mepoch[0m[2m=[0m67 [3mloss[0m[2m=[0m0.08494529128074646 [3mgrad_norm[0m[2m=[0m1.7562247514724731 [3mlearning_rate[0m[2m=[0m0.00039522297447547317
[2m2025-10-21T17:43:32.246751Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.55h@0.53p | L9: 3.72h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 9.9014 | Complexity: 0.471 [0.087-0.896] | Temp: 5.31 [2.25-6.84] [3mepoch[0m[2m=[0m68 [3mloss[0m[2m=[0m0.08390455693006516 [3mgrad_norm[0m[2m=[0m1.618193507194519 [3mlearning_rate[0m[2m=[0m0.0003797150566242635
[2m2025-10-21T17:43:35.398966Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.54h@0.54p | L9: 3.68h@0.54p | ThreshRange: [0.54-0.56] | PredNorm: 9.9627 | Complexity: 0.469 [0.089-0.891] | Temp: 5.30 [2.21-6.82] [3mepoch[0m[2m=[0m69 [3mloss[0m[2m=[0m0.0788477435708046 [3mgrad_norm[0m[2m=[0m1.2536808252334595 [3mlearning_rate[0m[2m=[0m0.00036443964927457273
[2m2025-10-21T17:43:38.609831Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.55h@0.53p | L9: 3.70h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.0216 | Complexity: 0.470 [0.088-0.890] | Temp: 5.28 [2.18-6.81] [3mepoch[0m[2m=[0m70 [3mloss[0m[2m=[0m0.07748005539178848 [3mgrad_norm[0m[2m=[0m1.2536041736602783 [3mlearning_rate[0m[2m=[0m0.00034941776539199054
[2m2025-10-21T17:43:41.865811Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.55h@0.53p | L9: 3.71h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.0781 | Complexity: 0.471 [0.088-0.894] | Temp: 5.28 [2.14-6.81] [3mepoch[0m[2m=[0m71 [3mloss[0m[2m=[0m0.07599551975727081 [3mgrad_norm[0m[2m=[0m1.1955324411392212 [3mlearning_rate[0m[2m=[0m0.0003346697485540062
[2m2025-10-21T17:43:44.940555Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.55h@0.53p | L9: 3.72h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.1322 | Complexity: 0.471 [0.091-0.897] | Temp: 5.28 [2.13-6.80] [3mepoch[0m[2m=[0m72 [3mloss[0m[2m=[0m0.07442042231559753 [3mgrad_norm[0m[2m=[0m1.1571816205978394 [3mlearning_rate[0m[2m=[0m0.00032021591323427856
[2m2025-10-21T17:43:48.178075Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.55h@0.53p | L9: 3.75h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.1838 | Complexity: 0.473 [0.092-0.902] | Temp: 5.27 [2.12-6.80] [3mepoch[0m[2m=[0m73 [3mloss[0m[2m=[0m0.07295176386833191 [3mgrad_norm[0m[2m=[0m1.0672342777252197 [3mlearning_rate[0m[2m=[0m0.0003060760791413486
[2m2025-10-21T17:43:51.451590Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.55h@0.53p | L9: 3.81h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.2333 | Complexity: 0.476 [0.092-0.906] | Temp: 5.27 [2.11-6.80] [3mepoch[0m[2m=[0m74 [3mloss[0m[2m=[0m0.07224429398775101 [3mgrad_norm[0m[2m=[0m1.0330840349197388 [3mlearning_rate[0m[2m=[0m0.0002922691637650132
[2m2025-10-21T17:43:54.584402Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.55h@0.53p | L9: 3.84h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.2804 | Complexity: 0.477 [0.092-0.909] | Temp: 5.27 [2.11-6.79] [3mepoch[0m[2m=[0m75 [3mloss[0m[2m=[0m0.07034534960985184 [3mgrad_norm[0m[2m=[0m1.020431399345398 [3mlearning_rate[0m[2m=[0m0.0002788144047372043
[2m2025-10-21T17:43:57.830889Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.55h@0.53p | L9: 3.85h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.3253 | Complexity: 0.477 [0.091-0.908] | Temp: 5.27 [2.10-6.78] [3mepoch[0m[2m=[0m76 [3mloss[0m[2m=[0m0.07054418325424194 [3mgrad_norm[0m[2m=[0m0.9896259307861328 [3mlearning_rate[0m[2m=[0m0.0002657300792634487
[2m2025-10-21T17:44:01.233427Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.54h@0.53p | L9: 3.84h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.3681 | Complexity: 0.477 [0.092-0.906] | Temp: 5.27 [2.10-6.78] [3mepoch[0m[2m=[0m77 [3mloss[0m[2m=[0m0.06882131844758987 [3mgrad_norm[0m[2m=[0m0.9307485222816467 [3mlearning_rate[0m[2m=[0m0.00025303399888798594
[2m2025-10-21T17:44:04.621805Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.55h@0.53p | L9: 3.84h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.4088 | Complexity: 0.478 [0.092-0.905] | Temp: 5.26 [2.09-6.78] [3mepoch[0m[2m=[0m78 [3mloss[0m[2m=[0m0.0684947818517685 [3mgrad_norm[0m[2m=[0m1.1413471698760986 [3mlearning_rate[0m[2m=[0m0.00024074343673419207
[2m2025-10-21T17:44:07.971363Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.53p | L9: 3.84h@0.55p | ThreshRange: [0.53-0.56] | PredNorm: 10.4473 | Complexity: 0.478 [0.092-0.905] | Temp: 5.27 [2.09-6.77] [3mepoch[0m[2m=[0m79 [3mloss[0m[2m=[0m0.06929542869329453 [3mgrad_norm[0m[2m=[0m1.6932545900344849 [3mlearning_rate[0m[2m=[0m0.0002288753748871386
[2m2025-10-21T17:44:11.418405Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.53p | L9: 3.84h@0.55p | ThreshRange: [0.53-0.56] | PredNorm: 10.4843 | Complexity: 0.478 [0.093-0.900] | Temp: 5.27 [2.12-6.75] [3mepoch[0m[2m=[0m80 [3mloss[0m[2m=[0m0.06923157721757889 [3mgrad_norm[0m[2m=[0m1.1681013107299805 [3mlearning_rate[0m[2m=[0m0.00021744603873230517
[2m2025-10-21T17:44:14.634917Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.53p | L9: 3.84h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.5194 | Complexity: 0.478 [0.091-0.903] | Temp: 5.27 [2.12-6.75] [3mepoch[0m[2m=[0m81 [3mloss[0m[2m=[0m0.06948783248662949 [3mgrad_norm[0m[2m=[0m1.1174976825714111 [3mlearning_rate[0m[2m=[0m0.00020647075143642724
[2m2025-10-21T17:44:17.859102Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.53p | L9: 3.84h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.5527 | Complexity: 0.478 [0.089-0.906] | Temp: 5.27 [2.11-6.75] [3mepoch[0m[2m=[0m82 [3mloss[0m[2m=[0m0.06911395490169525 [3mgrad_norm[0m[2m=[0m0.9840602278709412 [3mlearning_rate[0m[2m=[0m0.00019596476340666413
[2m2025-10-21T17:44:20.320475Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.53p | L9: 3.84h@0.54p | ThreshRange: [0.53-0.56] | PredNorm: 10.5842 | Complexity: 0.478 [0.089-0.908] | Temp: 5.27 [2.10-6.75] [3mepoch[0m[2m=[0m83 [3mloss[0m[2m=[0m0.06874986737966537 [3mgrad_norm[0m[2m=[0m1.1779662370681763 [3mlearning_rate[0m[2m=[0m0.00018594233551993966
[2m2025-10-21T17:44:22.790145Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.54p | L9: 3.84h@0.54p | ThreshRange: [0.54-0.56] | PredNorm: 10.6140 | Complexity: 0.478 [0.091-0.909] | Temp: 5.27 [2.11-6.75] [3mepoch[0m[2m=[0m84 [3mloss[0m[2m=[0m0.06789936125278473 [3mgrad_norm[0m[2m=[0m0.9191829562187195 [3mlearning_rate[0m[2m=[0m0.00017641719023231417
[2m2025-10-21T17:44:25.295877Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.54p | L9: 3.85h@0.54p | ThreshRange: [0.54-0.56] | PredNorm: 10.6422 | Complexity: 0.478 [0.091-0.910] | Temp: 5.27 [2.10-6.76] [3mepoch[0m[2m=[0m85 [3mloss[0m[2m=[0m0.06630224734544754 [3mgrad_norm[0m[2m=[0m0.8307848572731018 [3mlearning_rate[0m[2m=[0m0.00016740223509259522
[2m2025-10-21T17:44:27.751525Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.54p | L9: 3.86h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.6689 | Complexity: 0.478 [0.090-0.910] | Temp: 5.27 [2.10-6.76] [3mepoch[0m[2m=[0m86 [3mloss[0m[2m=[0m0.06617865711450577 [3mgrad_norm[0m[2m=[0m0.9517647624015808 [3mlearning_rate[0m[2m=[0m0.0001589099847478792
[2m2025-10-21T17:44:30.062428Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.54p | L9: 3.85h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.6943 | Complexity: 0.478 [0.089-0.909] | Temp: 5.27 [2.09-6.76] [3mepoch[0m[2m=[0m87 [3mloss[0m[2m=[0m0.06558448821306229 [3mgrad_norm[0m[2m=[0m0.8900639414787292 [3mlearning_rate[0m[2m=[0m0.0001509518624516204
[2m2025-10-21T17:44:32.329720Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.56h@0.54p | L9: 3.84h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.7184 | Complexity: 0.478 [0.088-0.908] | Temp: 5.27 [2.09-6.76] [3mepoch[0m[2m=[0m88 [3mloss[0m[2m=[0m0.06517983227968216 [3mgrad_norm[0m[2m=[0m0.7838837504386902 [3mlearning_rate[0m[2m=[0m0.00014353872393257916
[2m2025-10-21T17:44:34.628997Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.83h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.7413 | Complexity: 0.478 [0.087-0.906] | Temp: 5.27 [2.08-6.76] [3mepoch[0m[2m=[0m89 [3mloss[0m[2m=[0m0.06509862095117569 [3mgrad_norm[0m[2m=[0m1.0325449705123901 [3mlearning_rate[0m[2m=[0m0.00013668082829099149
[2m2025-10-21T17:44:36.971461Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.83h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.7632 | Complexity: 0.478 [0.087-0.903] | Temp: 5.27 [2.08-6.77] [3mepoch[0m[2m=[0m90 [3mloss[0m[2m=[0m0.06511946767568588 [3mgrad_norm[0m[2m=[0m0.7765477895736694 [3mlearning_rate[0m[2m=[0m0.0001303874742006883
[2m2025-10-21T17:44:39.389833Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.82h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.7842 | Complexity: 0.477 [0.086-0.904] | Temp: 5.27 [2.07-6.77] [3mepoch[0m[2m=[0m91 [3mloss[0m[2m=[0m0.06476452201604843 [3mgrad_norm[0m[2m=[0m0.8716827630996704 [3mlearning_rate[0m[2m=[0m0.00012466726184356958
[2m2025-10-21T17:44:41.685213Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.81h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.8042 | Complexity: 0.477 [0.086-0.906] | Temp: 5.27 [2.07-6.77] [3mepoch[0m[2m=[0m92 [3mloss[0m[2m=[0m0.06420843303203583 [3mgrad_norm[0m[2m=[0m0.7728416323661804 [3mlearning_rate[0m[2m=[0m0.00011952802015002817
[2m2025-10-21T17:44:43.885330Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.80h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.8235 | Complexity: 0.476 [0.085-0.908] | Temp: 5.27 [2.07-6.77] [3mepoch[0m[2m=[0m93 [3mloss[0m[2m=[0m0.06402507424354553 [3mgrad_norm[0m[2m=[0m0.7658979296684265 [3mlearning_rate[0m[2m=[0m0.00011497671221150085
[2m2025-10-21T17:44:46.134992Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.80h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.8420 | Complexity: 0.476 [0.084-0.909] | Temp: 5.27 [2.07-6.77] [3mepoch[0m[2m=[0m94 [3mloss[0m[2m=[0m0.06387320160865784 [3mgrad_norm[0m[2m=[0m0.8109642863273621 [3mlearning_rate[0m[2m=[0m0.00011101961717940867
[2m2025-10-21T17:44:48.404363Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.80h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.8600 | Complexity: 0.476 [0.083-0.910] | Temp: 5.27 [2.06-6.78] [3mepoch[0m[2m=[0m95 [3mloss[0m[2m=[0m0.06379521638154984 [3mgrad_norm[0m[2m=[0m0.7628405690193176 [3mlearning_rate[0m[2m=[0m0.00010766208288259804
[2m2025-10-21T17:44:50.768405Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.79h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.8775 | Complexity: 0.476 [0.082-0.911] | Temp: 5.27 [2.05-6.78] [3mepoch[0m[2m=[0m96 [3mloss[0m[2m=[0m0.06368843466043472 [3mgrad_norm[0m[2m=[0m0.7878019213676453 [3mlearning_rate[0m[2m=[0m0.0001049087950377725
[2m2025-10-21T17:44:53.131765Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.78h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.8946 | Complexity: 0.476 [0.080-0.911] | Temp: 5.27 [2.05-6.78] [3mepoch[0m[2m=[0m97 [3mloss[0m[2m=[0m0.06327982246875763 [3mgrad_norm[0m[2m=[0m0.7426978349685669 [3mlearning_rate[0m[2m=[0m0.00010276340617565438
[2m2025-10-21T17:44:55.496763Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.77h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.9114 | Complexity: 0.475 [0.079-0.911] | Temp: 5.27 [2.05-6.78] [3mepoch[0m[2m=[0m98 [3mloss[0m[2m=[0m0.06318222731351852 [3mgrad_norm[0m[2m=[0m0.7667531967163086 [3mlearning_rate[0m[2m=[0m0.0001012288557831198
[2m2025-10-21T17:44:57.868363Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.63h@0.56p | L5: 4.57h@0.54p | L9: 3.76h@0.55p | ThreshRange: [0.54-0.56] | PredNorm: 10.9281 | Complexity: 0.475 [0.079-0.912] | Temp: 5.27 [2.04-6.78] [3mepoch[0m[2m=[0m99 [3mloss[0m[2m=[0m0.06309202313423157 [3mgrad_norm[0m[2m=[0m0.779902994632721 [3mlearning_rate[0m[2m=[0m0.00010030733392341062

=== INSTRUCTION TUNING ===
Instruction tuning on 53 examples for 100 epochs with learning rate 0.0005
[2m2025-10-21T17:44:57.899971Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 191.25", "L1: 191.25", "L2: 105.84", "L3: 74.68", "L4: 44.73", "L5: 31.08", "L6: 20.16", "L7: 16.71", "L8: 5.78", "L9: 4.93", "L10: 1.04", "L11: 1.63", "L12: 0.27", "L13: 0.49"]
[2m2025-10-21T17:44:57.944830Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 181.82", "L1: 181.82", "L2: 105.53", "L3: 74.57", "L4: 45.42", "L5: 31.51", "L6: 20.71", "L7: 17.27", "L8: 6.18", "L9: 5.00", "L10: 1.05", "L11: 1.64", "L12: 0.29", "L13: 0.49"]
[2m2025-10-21T17:44:57.988688Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 193.42", "L1: 193.42", "L2: 114.38", "L3: 80.93", "L4: 51.55", "L5: 35.51", "L6: 22.67", "L7: 18.89", "L8: 6.66", "L9: 5.34", "L10: 1.04", "L11: 1.62", "L12: 0.28", "L13: 0.50"]
[2m2025-10-21T17:44:58.033282Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 209.11", "L1: 209.11", "L2: 116.79", "L3: 82.53", "L4: 49.72", "L5: 34.47", "L6: 22.94", "L7: 19.15", "L8: 6.57", "L9: 5.34", "L10: 1.08", "L11: 1.69", "L12: 0.28", "L13: 0.48"]
[2m2025-10-21T17:44:58.077045Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 203.98", "L1: 203.98", "L2: 116.99", "L3: 82.49", "L4: 50.64", "L5: 35.37", "L6: 22.36", "L7: 18.94", "L8: 6.39", "L9: 5.36", "L10: 1.09", "L11: 1.71", "L12: 0.29", "L13: 0.50"]
[2m2025-10-21T17:44:58.118421Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 195.53", "L1: 195.53", "L2: 108.93", "L3: 76.89", "L4: 46.35", "L5: 32.62", "L6: 19.78", "L7: 16.91", "L8: 5.67", "L9: 5.18", "L10: 1.09", "L11: 1.71", "L12: 0.29", "L13: 0.51"]
[2m2025-10-21T17:44:58.150155Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 234.83", "L1: 234.83", "L2: 120.47", "L3: 84.96", "L4: 48.50", "L5: 33.80", "L6: 21.21", "L7: 18.06", "L8: 6.06", "L9: 4.89", "L10: 1.05", "L11: 1.69", "L12: 0.29", "L13: 0.51"]
[2m2025-10-21T17:44:58.167269Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 1/15) | MoH L1: 4.55h@0.56p | L5: 4.46h@0.54p | L9: 3.83h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 10.9303 | Complexity: 0.468 [0.034-0.926] | Temp: 5.34 [2.81-7.34] [3mepoch[0m[2m=[0m0 [3mloss[0m[2m=[0m15.318265914916992 [3mgrad_norm[0m[2m=[0m100.0001220703125 [3mlearning_rate[0m[2m=[0m3.3333337341900915e-5
[2m2025-10-21T17:44:58.198910Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 167.17", "L1: 167.17", "L2: 99.64", "L3: 70.42", "L4: 43.25", "L5: 29.92", "L6: 19.06", "L7: 15.70", "L8: 5.53", "L9: 4.45", "L10: 1.01", "L11: 1.58", "L12: 0.27", "L13: 0.47"]
[2m2025-10-21T17:44:58.243384Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 176.50", "L1: 176.50", "L2: 94.03", "L3: 66.47", "L4: 39.05", "L5: 27.18", "L6: 18.15", "L7: 15.19", "L8: 5.40", "L9: 4.35", "L10: 1.02", "L11: 1.59", "L12: 0.28", "L13: 0.47"]
[2m2025-10-21T17:44:58.288078Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 171.20", "L1: 171.20", "L2: 101.11", "L3: 71.45", "L4: 44.73", "L5: 30.79", "L6: 21.11", "L7: 17.50", "L8: 6.40", "L9: 4.98", "L10: 1.00", "L11: 1.56", "L12: 0.28", "L13: 0.47"]
[2m2025-10-21T17:44:58.334572Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 182.86", "L1: 182.86", "L2: 97.49", "L3: 69.01", "L4: 40.82", "L5: 28.48", "L6: 18.80", "L7: 15.63", "L8: 5.48", "L9: 4.59", "L10: 1.02", "L11: 1.59", "L12: 0.28", "L13: 0.46"]
[2m2025-10-21T17:44:58.375451Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 173.90", "L1: 173.90", "L2: 94.30", "L3: 66.53", "L4: 39.34", "L5: 27.41", "L6: 17.74", "L7: 14.82", "L8: 5.24", "L9: 4.46", "L10: 1.08", "L11: 1.67", "L12: 0.29", "L13: 0.47"]
[2m2025-10-21T17:44:58.417402Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 182.72", "L1: 182.72", "L2: 96.70", "L3: 68.22", "L4: 38.81", "L5: 27.31", "L6: 16.25", "L7: 13.75", "L8: 4.86", "L9: 4.21", "L10: 1.01", "L11: 1.58", "L12: 0.27", "L13: 0.48"]
[2m2025-10-21T17:44:58.449034Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 172.90", "L1: 172.90", "L2: 96.73", "L3: 68.31", "L4: 41.82", "L5: 28.86", "L6: 18.50", "L7: 15.22", "L8: 5.36", "L9: 4.42", "L10: 1.02", "L11: 1.65", "L12: 0.28", "L13: 0.48"]
[2m2025-10-21T17:44:58.461857Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 2/15) | MoH L1: 4.55h@0.56p | L5: 4.47h@0.54p | L9: 3.80h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 10.9336 | Complexity: 0.467 [0.036-0.921] | Temp: 5.34 [2.79-7.30] [3mepoch[0m[2m=[0m1 [3mloss[0m[2m=[0m14.26433277130127 [3mgrad_norm[0m[2m=[0m99.30744934082031 [3mlearning_rate[0m[2m=[0m6.666667468380183e-5
[2m2025-10-21T17:44:58.491991Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 141.29", "L1: 141.29", "L2: 88.37", "L3: 62.35", "L4: 40.24", "L5: 27.80", "L6: 18.16", "L7: 14.91", "L8: 5.34", "L9: 4.50", "L10: 0.98", "L11: 1.51", "L12: 0.27", "L13: 0.45"]
[2m2025-10-21T17:44:58.534415Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 142.92", "L1: 142.92", "L2: 78.84", "L3: 55.59", "L4: 34.24", "L5: 23.86", "L6: 15.63", "L7: 13.04", "L8: 4.69", "L9: 3.95", "L10: 0.98", "L11: 1.52", "L12: 0.28", "L13: 0.45"]
[2m2025-10-21T17:44:58.575998Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 163.02", "L1: 163.02", "L2: 95.30", "L3: 67.11", "L4: 41.06", "L5: 28.41", "L6: 18.74", "L7: 15.46", "L8: 5.41", "L9: 4.54", "L10: 0.98", "L11: 1.53", "L12: 0.27", "L13: 0.45"]
[2m2025-10-21T17:44:58.618489Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 158.49", "L1: 158.49", "L2: 88.17", "L3: 62.29", "L4: 38.13", "L5: 26.25", "L6: 17.74", "L7: 14.54", "L8: 5.07", "L9: 4.35", "L10: 0.98", "L11: 1.54", "L12: 0.27", "L13: 0.44"]
[2m2025-10-21T17:44:58.662444Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 135.21", "L1: 135.21", "L2: 76.64", "L3: 54.04", "L4: 33.12", "L5: 22.84", "L6: 15.08", "L7: 12.46", "L8: 4.59", "L9: 3.76", "L10: 0.99", "L11: 1.51", "L12: 0.28", "L13: 0.45"]
[2m2025-10-21T17:44:58.705377Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 151.39", "L1: 151.39", "L2: 88.36", "L3: 62.24", "L4: 38.15", "L5: 26.53", "L6: 16.08", "L7: 13.28", "L8: 4.80", "L9: 4.09", "L10: 1.01", "L11: 1.56", "L12: 0.28", "L13: 0.46"]
[2m2025-10-21T17:44:58.737862Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 127.46", "L1: 127.46", "L2: 78.48", "L3: 55.37", "L4: 35.50", "L5: 24.44", "L6: 16.11", "L7: 13.35", "L8: 4.93", "L9: 4.05", "L10: 0.97", "L11: 1.54", "L12: 0.27", "L13: 0.46"]
[2m2025-10-21T17:44:58.751333Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 3/15) | MoH L1: 4.55h@0.56p | L5: 4.48h@0.54p | L9: 3.74h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 10.9381 | Complexity: 0.465 [0.036-0.908] | Temp: 5.33 [2.76-7.24] [3mepoch[0m[2m=[0m2 [3mloss[0m[2m=[0m12.761842727661133 [3mgrad_norm[0m[2m=[0m93.07915496826172 [3mlearning_rate[0m[2m=[0m0.00010000000474974513
[2m2025-10-21T17:44:58.782405Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 138.44", "L1: 138.44", "L2: 82.88", "L3: 58.40", "L4: 38.10", "L5: 26.35", "L6: 17.32", "L7: 14.23", "L8: 5.19", "L9: 4.37", "L10: 0.94", "L11: 1.47", "L12: 0.27", "L13: 0.43"]
[2m2025-10-21T17:44:58.825490Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 159.93", "L1: 159.93", "L2: 91.73", "L3: 64.65", "L4: 39.45", "L5: 27.29", "L6: 18.10", "L7: 14.95", "L8: 5.29", "L9: 4.45", "L10: 0.95", "L11: 1.47", "L12: 0.27", "L13: 0.43"]
[2m2025-10-21T17:44:58.869852Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 175.87", "L1: 175.87", "L2: 96.47", "L3: 68.03", "L4: 39.39", "L5: 27.57", "L6: 17.17", "L7: 14.31", "L8: 4.99", "L9: 4.18", "L10: 0.94", "L11: 1.48", "L12: 0.27", "L13: 0.43"]
[2m2025-10-21T17:44:58.913812Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 145.14", "L1: 145.14", "L2: 84.15", "L3: 59.32", "L4: 38.17", "L5: 26.27", "L6: 17.86", "L7: 14.50", "L8: 5.16", "L9: 4.07", "L10: 0.93", "L11: 1.44", "L12: 0.26", "L13: 0.42"]
[2m2025-10-21T17:44:58.957039Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 129.50", "L1: 129.50", "L2: 71.74", "L3: 50.51", "L4: 30.11", "L5: 20.98", "L6: 13.76", "L7: 11.46", "L8: 4.23", "L9: 3.55", "L10: 0.93", "L11: 1.44", "L12: 0.27", "L13: 0.43"]
[2m2025-10-21T17:44:58.997650Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 140.97", "L1: 140.97", "L2: 81.65", "L3: 57.59", "L4: 35.43", "L5: 24.59", "L6: 15.42", "L7: 12.80", "L8: 4.59", "L9: 3.95", "L10: 0.95", "L11: 1.47", "L12: 0.27", "L13: 0.43"]
[2m2025-10-21T17:44:59.028296Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 138.56", "L1: 138.56", "L2: 76.93", "L3: 54.31", "L4: 31.90", "L5: 22.56", "L6: 14.04", "L7: 11.76", "L8: 4.09", "L9: 3.59", "L10: 0.91", "L11: 1.44", "L12: 0.26", "L13: 0.42"]
[2m2025-10-21T17:44:59.041790Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 4/15) | MoH L1: 4.55h@0.56p | L5: 4.49h@0.54p | L9: 3.67h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 10.9440 | Complexity: 0.462 [0.035-0.898] | Temp: 5.33 [2.67-7.22] [3mepoch[0m[2m=[0m3 [3mloss[0m[2m=[0m11.238142967224121 [3mgrad_norm[0m[2m=[0m89.98173522949219 [3mlearning_rate[0m[2m=[0m0.00013333334936760366
[2m2025-10-21T17:44:59.072021Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 144.19", "L1: 144.19", "L2: 84.22", "L3: 59.35", "L4: 36.99", "L5: 25.73", "L6: 16.82", "L7: 13.82", "L8: 4.94", "L9: 4.23", "L10: 0.90", "L11: 1.39", "L12: 0.26", "L13: 0.41"]
[2m2025-10-21T17:44:59.114660Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 135.78", "L1: 135.78", "L2: 75.63", "L3: 53.23", "L4: 31.67", "L5: 21.88", "L6: 14.31", "L7: 11.78", "L8: 4.41", "L9: 3.78", "L10: 0.92", "L11: 1.42", "L12: 0.27", "L13: 0.40"]
[2m2025-10-21T17:44:59.157145Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 163.60", "L1: 163.60", "L2: 93.97", "L3: 66.01", "L4: 40.27", "L5: 27.58", "L6: 16.42", "L7: 13.49", "L8: 4.62", "L9: 3.96", "L10: 0.91", "L11: 1.41", "L12: 0.26", "L13: 0.41"]
[2m2025-10-21T17:44:59.207050Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 115.61", "L1: 115.61", "L2: 66.98", "L3: 47.21", "L4: 29.79", "L5: 20.56", "L6: 13.61", "L7: 11.10", "L8: 4.00", "L9: 3.29", "L10: 0.87", "L11: 1.35", "L12: 0.25", "L13: 0.40"]
[2m2025-10-21T17:44:59.250753Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 119.76", "L1: 119.76", "L2: 71.80", "L3: 50.58", "L4: 31.04", "L5: 21.59", "L6: 13.60", "L7: 11.29", "L8: 4.06", "L9: 3.48", "L10: 0.89", "L11: 1.36", "L12: 0.26", "L13: 0.41"]
[2m2025-10-21T17:44:59.292737Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 118.46", "L1: 118.46", "L2: 70.91", "L3: 50.08", "L4: 30.99", "L5: 21.64", "L6: 13.55", "L7: 11.20", "L8: 4.17", "L9: 3.55", "L10: 0.88", "L11: 1.36", "L12: 0.26", "L13: 0.41"]
[2m2025-10-21T17:44:59.325834Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 108.71", "L1: 108.71", "L2: 64.25", "L3: 45.19", "L4: 28.78", "L5: 19.79", "L6: 13.04", "L7: 10.75", "L8: 3.87", "L9: 3.21", "L10: 0.84", "L11: 1.31", "L12: 0.25", "L13: 0.40"]
[2m2025-10-21T17:44:59.340160Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 5/15) | MoH L1: 4.55h@0.56p | L5: 4.50h@0.54p | L9: 3.71h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 10.9513 | Complexity: 0.464 [0.032-0.893] | Temp: 5.30 [2.50-7.22] [3mepoch[0m[2m=[0m4 [3mloss[0m[2m=[0m9.77726936340332 [3mgrad_norm[0m[2m=[0m80.37608337402344 [3mlearning_rate[0m[2m=[0m0.00016666667943354696
[2m2025-10-21T17:44:59.373487Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 117.13", "L1: 117.13", "L2: 66.68", "L3: 46.90", "L4: 29.66", "L5: 20.38", "L6: 13.73", "L7: 11.29", "L8: 4.11", "L9: 3.52", "L10: 0.86", "L11: 1.34", "L12: 0.25", "L13: 0.39"]
[2m2025-10-21T17:44:59.416588Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 96.85", "L1: 96.85", "L2: 57.47", "L3: 40.45", "L4: 25.42", "L5: 17.46", "L6: 11.71", "L7: 9.60", "L8: 3.60", "L9: 3.05", "L10: 0.84", "L11: 1.31", "L12: 0.25", "L13: 0.39"]
[2m2025-10-21T17:44:59.458109Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 125.37", "L1: 125.37", "L2: 72.79", "L3: 51.17", "L4: 31.91", "L5: 21.95", "L6: 14.04", "L7: 11.52", "L8: 4.12", "L9: 3.33", "L10: 0.84", "L11: 1.30", "L12: 0.25", "L13: 0.40"]
[2m2025-10-21T17:44:59.502124Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 108.89", "L1: 108.89", "L2: 64.77", "L3: 45.41", "L4: 28.82", "L5: 19.70", "L6: 13.42", "L7: 10.85", "L8: 4.13", "L9: 3.34", "L10: 0.81", "L11: 1.28", "L12: 0.24", "L13: 0.38"]
[2m2025-10-21T17:44:59.545507Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 105.94", "L1: 105.94", "L2: 62.77", "L3: 44.11", "L4: 26.60", "L5: 18.33", "L6: 11.86", "L7: 9.82", "L8: 3.48", "L9: 3.18", "L10: 0.83", "L11: 1.29", "L12: 0.25", "L13: 0.39"]
[2m2025-10-21T17:44:59.585243Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 125.26", "L1: 125.26", "L2: 70.85", "L3: 49.96", "L4: 29.33", "L5: 20.42", "L6: 12.88", "L7: 10.81", "L8: 3.89", "L9: 3.44", "L10: 0.81", "L11: 1.26", "L12: 0.24", "L13: 0.40"]
[2m2025-10-21T17:44:59.614633Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 91.40", "L1: 91.40", "L2: 52.29", "L3: 36.75", "L4: 21.77", "L5: 15.11", "L6: 9.59", "L7: 7.98", "L8: 2.82", "L9: 2.53", "L10: 0.76", "L11: 1.20", "L12: 0.23", "L13: 0.39"]
[2m2025-10-21T17:44:59.627795Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 6/15) | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.75h@0.59p | ThreshRange: [0.54-0.59] | PredNorm: 10.9602 | Complexity: 0.467 [0.028-0.906] | Temp: 5.29 [2.47-7.25] [3mepoch[0m[2m=[0m5 [3mloss[0m[2m=[0m8.42197036743164 [3mgrad_norm[0m[2m=[0m70.82148742675781 [3mlearning_rate[0m[2m=[0m0.00020000000949949026
[2m2025-10-21T17:44:59.661302Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 94.35", "L1: 94.35", "L2: 55.73", "L3: 39.14", "L4: 25.58", "L5: 17.52", "L6: 12.33", "L7: 10.02", "L8: 3.71", "L9: 2.99", "L10: 0.79", "L11: 1.25", "L12: 0.24", "L13: 0.38"]
[2m2025-10-21T17:44:59.706407Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 72.92", "L1: 72.92", "L2: 43.76", "L3: 30.77", "L4: 19.53", "L5: 13.47", "L6: 9.00", "L7: 7.44", "L8: 2.80", "L9: 2.54", "L10: 0.78", "L11: 1.22", "L12: 0.24", "L13: 0.37"]
[2m2025-10-21T17:44:59.747415Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 134.90", "L1: 134.90", "L2: 76.69", "L3: 53.96", "L4: 32.55", "L5: 22.43", "L6: 14.30", "L7: 11.82", "L8: 4.07", "L9: 3.52", "L10: 0.82", "L11: 1.27", "L12: 0.24", "L13: 0.38"]
[2m2025-10-21T17:44:59.790089Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 116.05", "L1: 116.05", "L2: 65.95", "L3: 46.26", "L4: 27.57", "L5: 18.98", "L6: 12.35", "L7: 10.08", "L8: 3.63", "L9: 3.11", "L10: 0.77", "L11: 1.23", "L12: 0.23", "L13: 0.37"]
[2m2025-10-21T17:44:59.830951Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 107.45", "L1: 107.45", "L2: 61.20", "L3: 43.01", "L4: 25.26", "L5: 17.54", "L6: 10.90", "L7: 9.11", "L8: 3.15", "L9: 2.84", "L10: 0.76", "L11: 1.18", "L12: 0.23", "L13: 0.37"]
[2m2025-10-21T17:44:59.872404Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 99.00", "L1: 99.00", "L2: 60.48", "L3: 42.62", "L4: 26.91", "L5: 18.61", "L6: 11.90", "L7: 9.76", "L8: 3.40", "L9: 2.88", "L10: 0.74", "L11: 1.16", "L12: 0.23", "L13: 0.38"]
[2m2025-10-21T17:44:59.902791Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 91.93", "L1: 91.93", "L2: 48.96", "L3: 34.53", "L4: 20.24", "L5: 14.17", "L6: 8.98", "L7: 7.62", "L8: 2.55", "L9: 2.38", "L10: 0.71", "L11: 1.15", "L12: 0.22", "L13: 0.37"]
[2m2025-10-21T17:44:59.915645Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 7/15) | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.79h@0.59p | ThreshRange: [0.54-0.59] | PredNorm: 10.9707 | Complexity: 0.469 [0.025-0.914] | Temp: 5.28 [2.42-7.26] [3mepoch[0m[2m=[0m6 [3mloss[0m[2m=[0m7.119266510009766 [3mgrad_norm[0m[2m=[0m67.57420349121094 [3mlearning_rate[0m[2m=[0m0.00023333333956543356
[2m2025-10-21T17:44:59.945183Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 100.68", "L1: 100.68", "L2: 58.59", "L3: 41.13", "L4: 25.55", "L5: 17.48", "L6: 11.95", "L7: 9.68", "L8: 3.40", "L9: 2.96", "L10: 0.75", "L11: 1.19", "L12: 0.23", "L13: 0.37"]
[2m2025-10-21T17:44:59.987266Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 138.90", "L1: 138.90", "L2: 83.42", "L3: 58.48", "L4: 36.31", "L5: 24.58", "L6: 14.93", "L7: 12.03", "L8: 3.81", "L9: 3.08", "L10: 0.74", "L11: 1.17", "L12: 0.23", "L13: 0.36"]
[2m2025-10-21T17:45:00.032612Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 107.85", "L1: 107.85", "L2: 65.71", "L3: 46.11", "L4: 28.73", "L5: 19.97", "L6: 11.65", "L7: 9.61", "L8: 3.43", "L9: 3.08", "L10: 0.78", "L11: 1.21", "L12: 0.23", "L13: 0.37"]
[2m2025-10-21T17:45:00.077440Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 105.47", "L1: 105.47", "L2: 64.36", "L3: 45.18", "L4: 27.50", "L5: 19.00", "L6: 11.65", "L7: 9.40", "L8: 3.32", "L9: 2.81", "L10: 0.74", "L11: 1.18", "L12: 0.22", "L13: 0.36"]
[2m2025-10-21T17:45:00.119494Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 104.07", "L1: 104.07", "L2: 63.10", "L3: 44.20", "L4: 27.12", "L5: 18.80", "L6: 11.37", "L7: 9.49", "L8: 3.21", "L9: 2.82", "L10: 0.67", "L11: 1.07", "L12: 0.21", "L13: 0.36"]
[2m2025-10-21T17:45:00.166453Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 80.82", "L1: 80.82", "L2: 49.01", "L3: 34.51", "L4: 21.57", "L5: 14.95", "L6: 9.23", "L7: 7.58", "L8: 2.68", "L9: 2.32", "L10: 0.68", "L11: 1.09", "L12: 0.21", "L13: 0.37"]
[2m2025-10-21T17:45:00.200463Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 75.07", "L1: 75.07", "L2: 44.63", "L3: 31.37", "L4: 19.51", "L5: 13.55", "L6: 8.46", "L7: 7.02", "L8: 2.42", "L9: 2.18", "L10: 0.63", "L11: 1.03", "L12: 0.20", "L13: 0.35"]
[2m2025-10-21T17:45:00.216269Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 8/15) | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.86h@0.59p | ThreshRange: [0.54-0.59] | PredNorm: 10.9830 | Complexity: 0.473 [0.021-0.912] | Temp: 5.30 [2.54-7.28] [3mepoch[0m[2m=[0m7 [3mloss[0m[2m=[0m5.956182956695557 [3mgrad_norm[0m[2m=[0m67.96772766113281 [3mlearning_rate[0m[2m=[0m0.0002666666987352073
[2m2025-10-21T17:45:00.249145Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 86.71", "L1: 86.71", "L2: 50.14", "L3: 35.26", "L4: 21.56", "L5: 14.80", "L6: 9.74", "L7: 7.92", "L8: 2.83", "L9: 2.48", "L10: 0.69", "L11: 1.12", "L12: 0.21", "L13: 0.36"]
[2m2025-10-21T17:45:00.293188Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 87.12", "L1: 87.12", "L2: 51.34", "L3: 35.99", "L4: 22.62", "L5: 15.47", "L6: 10.35", "L7: 8.50", "L8: 3.10", "L9: 2.59", "L10: 0.67", "L11: 1.07", "L12: 0.21", "L13: 0.35"]
[2m2025-10-21T17:45:00.339204Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 98.85", "L1: 98.85", "L2: 59.82", "L3: 42.03", "L4: 26.15", "L5: 17.93", "L6: 11.52", "L7: 9.61", "L8: 3.32", "L9: 3.13", "L10: 0.70", "L11: 1.12", "L12: 0.21", "L13: 0.36"]
[2m2025-10-21T17:45:00.384011Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 103.68", "L1: 103.68", "L2: 59.49", "L3: 41.77", "L4: 25.03", "L5: 17.21", "L6: 11.06", "L7: 9.09", "L8: 3.20", "L9: 2.94", "L10: 0.68", "L11: 1.10", "L12: 0.20", "L13: 0.35"]
[2m2025-10-21T17:45:00.428458Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 105.85", "L1: 105.85", "L2: 63.80", "L3: 44.83", "L4: 28.14", "L5: 19.17", "L6: 11.95", "L7: 9.83", "L8: 3.28", "L9: 2.68", "L10: 0.61", "L11: 0.99", "L12: 0.20", "L13: 0.35"]
[2m2025-10-21T17:45:00.472552Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 85.33", "L1: 85.33", "L2: 53.80", "L3: 37.81", "L4: 23.71", "L5: 16.30", "L6: 9.53", "L7: 7.74", "L8: 2.70", "L9: 2.36", "L10: 0.63", "L11: 1.02", "L12: 0.20", "L13: 0.35"]
[2m2025-10-21T17:45:00.505857Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 119.02", "L1: 119.02", "L2: 71.43", "L3: 50.06", "L4: 29.44", "L5: 20.44", "L6: 11.21", "L7: 9.11", "L8: 3.01", "L9: 2.76", "L10: 0.57", "L11: 0.94", "L12: 0.18", "L13: 0.34"]
[2m2025-10-21T17:45:00.520352Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 9/15) | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.92h@0.59p | ThreshRange: [0.54-0.59] | PredNorm: 10.9972 | Complexity: 0.475 [0.018-0.900] | Temp: 5.31 [2.75-7.35] [3mepoch[0m[2m=[0m8 [3mloss[0m[2m=[0m4.929044246673584 [3mgrad_norm[0m[2m=[0m67.48069763183594 [3mlearning_rate[0m[2m=[0m0.0003000000142492354
[2m2025-10-21T17:45:00.550891Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 88.37", "L1: 88.37", "L2: 48.29", "L3: 33.85", "L4: 20.14", "L5: 13.94", "L6: 8.75", "L7: 7.19", "L8: 2.55", "L9: 2.30", "L10: 0.65", "L11: 1.05", "L12: 0.20", "L13: 0.34"]
[2m2025-10-21T17:45:00.596598Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 92.06", "L1: 92.06", "L2: 52.43", "L3: 36.82", "L4: 21.22", "L5: 14.85", "L6: 9.11", "L7: 7.49", "L8: 2.64", "L9: 2.47", "L10: 0.61", "L11: 0.99", "L12: 0.19", "L13: 0.33"]
[2m2025-10-21T17:45:00.643283Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 92.92", "L1: 92.92", "L2: 52.98", "L3: 37.08", "L4: 22.74", "L5: 15.61", "L6: 9.67", "L7: 7.92", "L8: 2.89", "L9: 2.55", "L10: 0.65", "L11: 1.03", "L12: 0.20", "L13: 0.34"]
[2m2025-10-21T17:45:00.687830Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 69.16", "L1: 69.16", "L2: 45.51", "L3: 31.81", "L4: 20.45", "L5: 14.01", "L6: 8.78", "L7: 7.23", "L8: 2.57", "L9: 2.24", "L10: 0.60", "L11: 0.99", "L12: 0.18", "L13: 0.33"]
[2m2025-10-21T17:45:00.734330Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 125.83", "L1: 125.83", "L2: 71.45", "L3: 49.76", "L4: 28.14", "L5: 19.22", "L6: 10.59", "L7: 8.76", "L8: 2.89", "L9: 2.72", "L10: 0.60", "L11: 0.98", "L12: 0.19", "L13: 0.33"]
[2m2025-10-21T17:45:00.779548Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 75.79", "L1: 75.79", "L2: 42.67", "L3: 29.91", "L4: 17.65", "L5: 12.13", "L6: 7.04", "L7: 5.75", "L8: 2.08", "L9: 1.92", "L10: 0.55", "L11: 0.91", "L12: 0.18", "L13: 0.34"]
[2m2025-10-21T17:45:00.812064Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 61.34", "L1: 61.34", "L2: 36.54", "L3: 25.63", "L4: 15.62", "L5: 10.77", "L6: 6.66", "L7: 5.51", "L8: 1.85", "L9: 1.74", "L10: 0.52", "L11: 0.87", "L12: 0.16", "L13: 0.31"]
[2m2025-10-21T17:45:00.826963Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 10/15) | MoH L1: 4.55h@0.56p | L5: 4.50h@0.54p | L9: 3.90h@0.59p | ThreshRange: [0.54-0.59] | PredNorm: 11.0138 | Complexity: 0.473 [0.017-0.905] | Temp: 5.31 [2.83-7.40] [3mepoch[0m[2m=[0m9 [3mloss[0m[2m=[0m4.041543006896973 [3mgrad_norm[0m[2m=[0m51.878570556640625 [3mlearning_rate[0m[2m=[0m0.0003333333588670939
[2m2025-10-21T17:45:00.861095Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 93.27", "L1: 93.27", "L2: 54.63", "L3: 38.10", "L4: 23.51", "L5: 15.99", "L6: 9.95", "L7: 8.07", "L8: 2.88", "L9: 2.54", "L10: 0.61", "L11: 0.99", "L12: 0.19", "L13: 0.34"]
[2m2025-10-21T17:45:00.908329Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 66.36", "L1: 66.36", "L2: 41.19", "L3: 28.84", "L4: 18.12", "L5: 12.48", "L6: 7.79", "L7: 6.33", "L8: 2.33", "L9: 2.07", "L10: 0.57", "L11: 0.93", "L12: 0.18", "L13: 0.31"]
[2m2025-10-21T17:45:00.953893Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 77.47", "L1: 77.47", "L2: 48.17", "L3: 33.67", "L4: 21.41", "L5: 14.89", "L6: 8.26", "L7: 6.74", "L8: 2.37", "L9: 2.36", "L10: 0.61", "L11: 0.98", "L12: 0.19", "L13: 0.33"]
[2m2025-10-21T17:45:00.999704Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 70.15", "L1: 70.15", "L2: 42.35", "L3: 29.55", "L4: 18.01", "L5: 12.32", "L6: 7.48", "L7: 6.07", "L8: 2.22", "L9: 1.99", "L10: 0.56", "L11: 0.91", "L12: 0.17", "L13: 0.32"]
[2m2025-10-21T17:45:01.045654Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 83.39", "L1: 83.39", "L2: 51.89", "L3: 36.26", "L4: 21.76", "L5: 14.88", "L6: 7.67", "L7: 6.35", "L8: 2.18", "L9: 2.09", "L10: 0.57", "L11: 0.93", "L12: 0.18", "L13: 0.32"]
[2m2025-10-21T17:45:01.090903Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 72.72", "L1: 72.72", "L2: 44.58", "L3: 31.22", "L4: 19.46", "L5: 13.37", "L6: 7.77", "L7: 6.27", "L8: 2.31", "L9: 2.14", "L10: 0.53", "L11: 0.86", "L12: 0.17", "L13: 0.33"]
[2m2025-10-21T17:45:01.139001Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 58.14", "L1: 58.14", "L2: 32.87", "L3: 22.96", "L4: 13.17", "L5: 9.23", "L6: 5.49", "L7: 4.54", "L8: 1.59", "L9: 1.54", "L10: 0.45", "L11: 0.74", "L12: 0.14", "L13: 0.28"]
[2m2025-10-21T17:45:01.153942Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 11/15) | MoH L1: 4.55h@0.56p | L5: 4.50h@0.54p | L9: 3.90h@0.59p | ThreshRange: [0.54-0.59] | PredNorm: 11.0321 | Complexity: 0.474 [0.015-0.912] | Temp: 5.30 [2.76-7.42] [3mepoch[0m[2m=[0m10 [3mloss[0m[2m=[0m3.2901651859283447 [3mgrad_norm[0m[2m=[0m52.01192092895508 [3mlearning_rate[0m[2m=[0m0.00036666670348495245
[2m2025-10-21T17:45:01.189582Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 59.49", "L1: 59.49", "L2: 35.71", "L3: 24.99", "L4: 15.76", "L5: 10.81", "L6: 6.96", "L7: 5.68", "L8: 2.04", "L9: 1.88", "L10: 0.54", "L11: 0.87", "L12: 0.17", "L13: 0.32"]
[2m2025-10-21T17:45:01.241509Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 68.16", "L1: 68.16", "L2: 42.30", "L3: 29.46", "L4: 18.21", "L5: 12.49", "L6: 7.40", "L7: 6.07", "L8: 2.08", "L9: 2.13", "L10: 0.54", "L11: 0.88", "L12: 0.17", "L13: 0.30"]
[2m2025-10-21T17:45:01.290790Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 74.13", "L1: 74.13", "L2: 43.49", "L3: 30.37", "L4: 18.74", "L5: 12.83", "L6: 7.94", "L7: 6.45", "L8: 2.27", "L9: 2.07", "L10: 0.54", "L11: 0.86", "L12: 0.16", "L13: 0.30"]
[2m2025-10-21T17:45:01.340941Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 54.83", "L1: 54.83", "L2: 33.41", "L3: 23.31", "L4: 14.83", "L5: 10.14", "L6: 6.58", "L7: 5.34", "L8: 1.87", "L9: 1.69", "L10: 0.53", "L11: 0.84", "L12: 0.16", "L13: 0.30"]
[2m2025-10-21T17:45:01.391433Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 79.18", "L1: 79.18", "L2: 48.01", "L3: 33.56", "L4: 20.07", "L5: 13.84", "L6: 7.80", "L7: 6.37", "L8: 2.10", "L9: 1.98", "L10: 0.53", "L11: 0.86", "L12: 0.17", "L13: 0.31"]
[2m2025-10-21T17:45:01.437322Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 70.12", "L1: 70.12", "L2: 41.42", "L3: 28.98", "L4: 17.48", "L5: 12.00", "L6: 6.87", "L7: 5.59", "L8: 2.00", "L9: 1.92", "L10: 0.49", "L11: 0.81", "L12: 0.15", "L13: 0.31"]
[2m2025-10-21T17:45:01.470297Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 61.16", "L1: 61.16", "L2: 39.76", "L3: 27.77", "L4: 17.51", "L5: 11.95", "L6: 6.82", "L7: 5.52", "L8: 1.92", "L9: 1.54", "L10: 0.35", "L11: 0.59", "L12: 0.11", "L13: 0.24"]
[2m2025-10-21T17:45:01.485246Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 12/15) | MoH L1: 4.55h@0.56p | L5: 4.49h@0.54p | L9: 3.88h@0.60p | ThreshRange: [0.54-0.60] | PredNorm: 11.0527 | Complexity: 0.473 [0.013-0.902] | Temp: 5.30 [2.83-7.49] [3mepoch[0m[2m=[0m11 [3mloss[0m[2m=[0m2.612053871154785 [3mgrad_norm[0m[2m=[0m44.58480453491211 [3mlearning_rate[0m[2m=[0m0.0004000000189989805
[2m2025-10-21T17:45:01.520216Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 62.89", "L1: 62.89", "L2: 36.98", "L3: 25.80", "L4: 15.63", "L5: 10.74", "L6: 6.45", "L7: 5.28", "L8: 1.76", "L9: 1.64", "L10: 0.47", "L11: 0.75", "L12: 0.14", "L13: 0.29"]
[2m2025-10-21T17:45:01.569361Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 74.11", "L1: 74.11", "L2: 43.89", "L3: 30.63", "L4: 18.54", "L5: 12.81", "L6: 7.50", "L7: 6.05", "L8: 2.13", "L9: 1.90", "L10: 0.47", "L11: 0.77", "L12: 0.15", "L13: 0.27"]
[2m2025-10-21T17:45:01.612526Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 56.24", "L1: 56.24", "L2: 35.39", "L3: 24.69", "L4: 15.73", "L5: 10.78", "L6: 6.39", "L7: 5.15", "L8: 1.83", "L9: 1.67", "L10: 0.47", "L11: 0.76", "L12: 0.14", "L13: 0.28"]
[2m2025-10-21T17:45:01.657833Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 74.29", "L1: 74.29", "L2: 40.86", "L3: 28.49", "L4: 17.19", "L5: 11.77", "L6: 7.74", "L7: 6.25", "L8: 2.18", "L9: 1.79", "L10: 0.47", "L11: 0.74", "L12: 0.14", "L13: 0.27"]
[2m2025-10-21T17:45:01.704188Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 66.47", "L1: 66.47", "L2: 38.14", "L3: 26.55", "L4: 15.64", "L5: 10.74", "L6: 6.20", "L7: 5.09", "L8: 1.74", "L9: 1.72", "L10: 0.48", "L11: 0.76", "L12: 0.15", "L13: 0.28"]
[2m2025-10-21T17:45:01.748279Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 53.37", "L1: 53.37", "L2: 33.23", "L3: 23.22", "L4: 14.52", "L5: 9.98", "L6: 5.95", "L7: 4.78", "L8: 1.68", "L9: 1.60", "L10: 0.42", "L11: 0.69", "L12: 0.13", "L13: 0.28"]
[2m2025-10-21T17:45:01.783150Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 49.25", "L1: 49.25", "L2: 27.82", "L3: 19.43", "L4: 11.32", "L5: 7.87", "L6: 4.71", "L7: 3.83", "L8: 1.33", "L9: 1.21", "L10: 0.31", "L11: 0.52", "L12: 0.09", "L13: 0.21"]
[2m2025-10-21T17:45:01.797756Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 13/15) | MoH L1: 4.55h@0.56p | L5: 4.50h@0.54p | L9: 3.84h@0.60p | ThreshRange: [0.54-0.60] | PredNorm: 11.0754 | Complexity: 0.471 [0.012-0.888] | Temp: 5.30 [2.75-7.52] [3mepoch[0m[2m=[0m12 [3mloss[0m[2m=[0m2.0102832317352295 [3mgrad_norm[0m[2m=[0m42.936702728271484 [3mlearning_rate[0m[2m=[0m0.00043333336361683905
[2m2025-10-21T17:45:01.830730Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 52.66", "L1: 52.66", "L2: 31.46", "L3: 21.93", "L4: 13.39", "L5: 9.19", "L6: 5.31", "L7: 4.29", "L8: 1.51", "L9: 1.45", "L10: 0.42", "L11: 0.68", "L12: 0.12", "L13: 0.26"]
[2m2025-10-21T17:45:01.880795Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 55.05", "L1: 55.05", "L2: 34.00", "L3: 23.66", "L4: 14.74", "L5: 10.13", "L6: 5.93", "L7: 4.82", "L8: 1.68", "L9: 1.55", "L10: 0.43", "L11: 0.70", "L12: 0.13", "L13: 0.25"]
[2m2025-10-21T17:45:01.925095Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 60.66", "L1: 60.66", "L2: 38.31", "L3: 26.70", "L4: 17.02", "L5: 11.78", "L6: 7.03", "L7: 5.68", "L8: 1.89", "L9: 1.60", "L10: 0.41", "L11: 0.67", "L12: 0.12", "L13: 0.25"]
[2m2025-10-21T17:45:01.969958Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 60.99", "L1: 60.99", "L2: 34.66", "L3: 24.13", "L4: 14.32", "L5: 9.84", "L6: 6.29", "L7: 5.03", "L8: 1.84", "L9: 1.59", "L10: 0.39", "L11: 0.62", "L12: 0.11", "L13: 0.24"]
[2m2025-10-21T17:45:02.015293Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 56.50", "L1: 56.50", "L2: 32.95", "L3: 22.87", "L4: 13.90", "L5: 9.52", "L6: 5.72", "L7: 4.66", "L8: 1.60", "L9: 1.48", "L10: 0.42", "L11: 0.67", "L12: 0.13", "L13: 0.25"]
[2m2025-10-21T17:45:02.057093Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 53.08", "L1: 53.08", "L2: 34.98", "L3: 24.28", "L4: 15.28", "L5: 10.56", "L6: 5.63", "L7: 4.58", "L8: 1.50", "L9: 1.42", "L10: 0.36", "L11: 0.58", "L12: 0.11", "L13: 0.26"]
[2m2025-10-21T17:45:02.093090Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 35.60", "L1: 35.60", "L2: 19.35", "L3: 13.46", "L4: 7.49", "L5: 5.21", "L6: 2.92", "L7: 2.44", "L8: 0.84", "L9: 0.83", "L10: 0.25", "L11: 0.43", "L12: 0.07", "L13: 0.18"]
[2m2025-10-21T17:45:02.107609Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 14/15) | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.77h@0.59p | ThreshRange: [0.54-0.59] | PredNorm: 11.1008 | Complexity: 0.468 [0.011-0.876] | Temp: 5.28 [2.85-7.56] [3mepoch[0m[2m=[0m13 [3mloss[0m[2m=[0m1.527604579925537 [3mgrad_norm[0m[2m=[0m34.8601188659668 [3mlearning_rate[0m[2m=[0m0.0004666666791308671
[2m2025-10-21T17:45:02.138926Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 53.40", "L1: 53.40", "L2: 32.64", "L3: 22.72", "L4: 13.90", "L5: 9.58", "L6: 5.40", "L7: 4.37", "L8: 1.47", "L9: 1.36", "L10: 0.37", "L11: 0.61", "L12: 0.11", "L13: 0.24"]
[2m2025-10-21T17:45:02.185310Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 46.28", "L1: 46.28", "L2: 29.52", "L3: 20.55", "L4: 13.08", "L5: 9.02", "L6: 5.51", "L7: 4.49", "L8: 1.48", "L9: 1.37", "L10: 0.35", "L11: 0.58", "L12: 0.11", "L13: 0.22"]
[2m2025-10-21T17:45:02.230101Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 43.59", "L1: 43.59", "L2: 26.80", "L3: 18.68", "L4: 11.35", "L5: 7.93", "L6: 4.24", "L7: 3.47", "L8: 1.21", "L9: 1.18", "L10: 0.35", "L11: 0.58", "L12: 0.10", "L13: 0.21"]
[2m2025-10-21T17:45:02.276376Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 45.18", "L1: 45.18", "L2: 27.19", "L3: 18.95", "L4: 11.93", "L5: 8.22", "L6: 5.19", "L7: 4.28", "L8: 1.48", "L9: 1.38", "L10: 0.40", "L11: 0.63", "L12: 0.11", "L13: 0.23"]
[2m2025-10-21T17:45:02.320390Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 56.51", "L1: 56.51", "L2: 37.79", "L3: 26.31", "L4: 16.79", "L5: 11.76", "L6: 6.37", "L7: 5.25", "L8: 1.61", "L9: 1.48", "L10: 0.35", "L11: 0.57", "L12: 0.11", "L13: 0.22"]
[2m2025-10-21T17:45:02.365165Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 68.78", "L1: 68.78", "L2: 46.58", "L3: 32.12", "L4: 20.84", "L5: 14.22", "L6: 6.62", "L7: 5.40", "L8: 1.73", "L9: 1.61", "L10: 0.33", "L11: 0.54", "L12: 0.10", "L13: 0.23"]
[2m2025-10-21T17:45:02.396792Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 48.66", "L1: 48.66", "L2: 25.20", "L3: 17.49", "L4: 9.54", "L5: 6.55", "L6: 3.90", "L7: 3.21", "L8: 0.94", "L9: 0.95", "L10: 0.24", "L11: 0.40", "L12: 0.07", "L13: 0.16"]
[2m2025-10-21T17:45:02.410760Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed (warmup 15/15) | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.81h@0.59p | ThreshRange: [0.54-0.59] | PredNorm: 11.1287 | Complexity: 0.470 [0.010-0.882] | Temp: 5.27 [2.83-7.59] [3mepoch[0m[2m=[0m14 [3mloss[0m[2m=[0m1.1682144403457642 [3mgrad_norm[0m[2m=[0m35.570865631103516 [3mlearning_rate[0m[2m=[0m0.0005000000237487257
[2m2025-10-21T17:45:02.441530Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 35.75", "L1: 35.75", "L2: 22.48", "L3: 15.63", "L4: 9.77", "L5: 6.73", "L6: 4.18", "L7: 3.43", "L8: 1.17", "L9: 1.16", "L10: 0.32", "L11: 0.54", "L12: 0.10", "L13: 0.21"]
[2m2025-10-21T17:45:02.488850Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 48.34", "L1: 48.34", "L2: 28.62", "L3: 19.95", "L4: 12.38", "L5: 8.64", "L6: 4.90", "L7: 4.07", "L8: 1.33", "L9: 1.41", "L10: 0.37", "L11: 0.60", "L12: 0.10", "L13: 0.20"]
[2m2025-10-21T17:45:02.531009Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 33.82", "L1: 33.82", "L2: 19.40", "L3: 13.48", "L4: 7.98", "L5: 5.54", "L6: 3.35", "L7: 2.74", "L8: 0.97", "L9: 0.94", "L10: 0.29", "L11: 0.48", "L12: 0.09", "L13: 0.19"]
[2m2025-10-21T17:45:02.574639Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 44.10", "L1: 44.10", "L2: 26.80", "L3: 18.70", "L4: 11.38", "L5: 7.75", "L6: 4.38", "L7: 3.58", "L8: 1.24", "L9: 1.27", "L10: 0.31", "L11: 0.50", "L12: 0.09", "L13: 0.19"]
[2m2025-10-21T17:45:02.619697Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 32.71", "L1: 32.71", "L2: 21.90", "L3: 15.22", "L4: 9.85", "L5: 6.76", "L6: 3.98", "L7: 3.29", "L8: 1.06", "L9: 1.04", "L10: 0.28", "L11: 0.46", "L12: 0.09", "L13: 0.19"]
[2m2025-10-21T17:45:02.663510Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 51.10", "L1: 51.10", "L2: 29.87", "L3: 20.79", "L4: 12.69", "L5: 8.74", "L6: 5.15", "L7: 4.23", "L8: 1.44", "L9: 1.39", "L10: 0.32", "L11: 0.51", "L12: 0.09", "L13: 0.21"]
[2m2025-10-21T17:45:02.697172Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 32.57", "L1: 32.57", "L2: 17.85", "L3: 12.34", "L4: 6.91", "L5: 4.87", "L6: 2.70", "L7: 2.24", "L8: 0.71", "L9: 0.75", "L10: 0.22", "L11: 0.36", "L12: 0.06", "L13: 0.15"]
[2m2025-10-21T17:45:02.712695Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.50h@0.54p | L9: 3.80h@0.59p | ThreshRange: [0.54-0.59] | PredNorm: 11.1581 | Complexity: 0.469 [0.008-0.898] | Temp: 5.27 [2.84-7.66] [3mepoch[0m[2m=[0m15 [3mloss[0m[2m=[0m0.8907878994941711 [3mgrad_norm[0m[2m=[0m27.305795669555664 [3mlearning_rate[0m[2m=[0m0.0005000000237487257
[2m2025-10-21T17:45:02.744509Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 54.49", "L1: 54.49", "L2: 34.92", "L3: 24.26", "L4: 15.36", "L5: 10.62", "L6: 6.19", "L7: 5.00", "L8: 1.70", "L9: 1.56", "L10: 0.37", "L11: 0.58", "L12: 0.10", "L13: 0.19"]
[2m2025-10-21T17:45:02.802559Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 32.53", "L1: 32.53", "L2: 18.81", "L3: 13.07", "L4: 7.72", "L5: 5.40", "L6: 3.09", "L7: 2.52", "L8: 0.87", "L9: 0.95", "L10: 0.26", "L11: 0.42", "L12: 0.08", "L13: 0.17"]
[2m2025-10-21T17:45:02.857702Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 27.26", "L1: 27.26", "L2: 15.76", "L3: 10.95", "L4: 6.63", "L5: 4.56", "L6: 2.82", "L7: 2.30", "L8: 0.80", "L9: 0.81", "L10: 0.26", "L11: 0.41", "L12: 0.07", "L13: 0.16"]
[2m2025-10-21T17:45:02.902530Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 32.48", "L1: 32.48", "L2: 20.22", "L3: 14.07", "L4: 9.07", "L5: 6.20", "L6: 4.13", "L7: 3.27", "L8: 1.22", "L9: 1.08", "L10: 0.27", "L11: 0.43", "L12: 0.07", "L13: 0.17"]
[2m2025-10-21T17:45:02.951745Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 45.92", "L1: 45.92", "L2: 25.05", "L3: 17.41", "L4: 10.07", "L5: 7.01", "L6: 3.92", "L7: 3.26", "L8: 1.03", "L9: 1.02", "L10: 0.27", "L11: 0.43", "L12: 0.08", "L13: 0.18"]
[2m2025-10-21T17:45:02.996722Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 31.66", "L1: 31.66", "L2: 18.64", "L3: 12.97", "L4: 7.74", "L5: 5.34", "L6: 3.33", "L7: 2.75", "L8: 0.98", "L9: 1.06", "L10: 0.27", "L11: 0.43", "L12: 0.08", "L13: 0.19"]
[2m2025-10-21T17:45:03.028906Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 31.15", "L1: 31.15", "L2: 17.51", "L3: 12.16", "L4: 7.30", "L5: 4.99", "L6: 3.17", "L7: 2.53", "L8: 0.87", "L9: 0.78", "L10: 0.18", "L11: 0.31", "L12: 0.05", "L13: 0.13"]
[2m2025-10-21T17:45:03.043433Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.50h@0.54p | L9: 3.79h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 11.1881 | Complexity: 0.469 [0.008-0.902] | Temp: 5.27 [2.83-7.69] [3mepoch[0m[2m=[0m16 [3mloss[0m[2m=[0m0.7046493887901306 [3mgrad_norm[0m[2m=[0m26.405990600585938 [3mlearning_rate[0m[2m=[0m0.0004998463555239141
[2m2025-10-21T17:45:03.074122Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 38.76", "L1: 38.76", "L2: 23.59", "L3: 16.37", "L4: 10.27", "L5: 7.08", "L6: 4.33", "L7: 3.47", "L8: 1.10", "L9: 1.04", "L10: 0.30", "L11: 0.49", "L12: 0.08", "L13: 0.17"]
[2m2025-10-21T17:45:03.117166Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 29.20", "L1: 29.20", "L2: 17.90", "L3: 12.46", "L4: 7.83", "L5: 5.39", "L6: 3.18", "L7: 2.59", "L8: 0.89", "L9: 0.90", "L10: 0.24", "L11: 0.39", "L12: 0.07", "L13: 0.16"]
[2m2025-10-21T17:45:03.162807Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 27.36", "L1: 27.36", "L2: 16.63", "L3: 11.55", "L4: 7.28", "L5: 5.00", "L6: 3.15", "L7: 2.52", "L8: 0.90", "L9: 0.87", "L10: 0.23", "L11: 0.38", "L12: 0.07", "L13: 0.15"]
[2m2025-10-21T17:45:03.207937Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 33.89", "L1: 33.89", "L2: 20.60", "L3: 14.31", "L4: 9.02", "L5: 6.17", "L6: 3.61", "L7: 2.92", "L8: 0.93", "L9: 0.98", "L10: 0.22", "L11: 0.36", "L12: 0.06", "L13: 0.15"]
[2m2025-10-21T17:45:03.253335Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.92", "L1: 24.92", "L2: 15.20", "L3: 10.54", "L4: 6.51", "L5: 4.46", "L6: 2.51", "L7: 2.07", "L8: 0.69", "L9: 0.72", "L10: 0.21", "L11: 0.36", "L12: 0.07", "L13: 0.15"]
[2m2025-10-21T17:45:03.297216Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.89", "L1: 25.89", "L2: 16.09", "L3: 11.15", "L4: 7.00", "L5: 4.80", "L6: 2.83", "L7: 2.27", "L8: 0.81", "L9: 0.81", "L10: 0.25", "L11: 0.40", "L12: 0.07", "L13: 0.17"]
[2m2025-10-21T17:45:03.331625Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.43", "L1: 15.43", "L2: 9.25", "L3: 6.44", "L4: 3.95", "L5: 2.80", "L6: 1.59", "L7: 1.32", "L8: 0.42", "L9: 0.45", "L10: 0.12", "L11: 0.20", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:03.346866Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.82h@0.59p | ThreshRange: [0.54-0.59] | PredNorm: 11.2205 | Complexity: 0.471 [0.008-0.921] | Temp: 5.26 [2.83-7.75] [3mepoch[0m[2m=[0m17 [3mloss[0m[2m=[0m0.5683272480964661 [3mgrad_norm[0m[2m=[0m19.777530670166016 [3mlearning_rate[0m[2m=[0m0.0004993855836801231
[2m2025-10-21T17:45:03.378958Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 28.75", "L1: 28.75", "L2: 18.21", "L3: 12.54", "L4: 7.75", "L5: 5.30", "L6: 2.72", "L7: 2.18", "L8: 0.72", "L9: 0.70", "L10: 0.20", "L11: 0.33", "L12: 0.06", "L13: 0.14"]
[2m2025-10-21T17:45:03.424375Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.94", "L1: 22.94", "L2: 13.71", "L3: 9.51", "L4: 5.83", "L5: 4.04", "L6: 2.45", "L7: 1.99", "L8: 0.68", "L9: 0.70", "L10: 0.19", "L11: 0.32", "L12: 0.06", "L13: 0.14"]
[2m2025-10-21T17:45:03.473583Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.37", "L1: 21.37", "L2: 12.04", "L3: 8.35", "L4: 4.83", "L5: 3.35", "L6: 2.00", "L7: 1.65", "L8: 0.57", "L9: 0.53", "L10: 0.16", "L11: 0.26", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:45:03.522659Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 42.80", "L1: 42.80", "L2: 24.36", "L3: 16.84", "L4: 9.77", "L5: 6.78", "L6: 4.28", "L7: 3.56", "L8: 1.17", "L9: 1.15", "L10: 0.23", "L11: 0.36", "L12: 0.06", "L13: 0.14"]
[2m2025-10-21T17:45:03.574138Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.95", "L1: 21.95", "L2: 13.54", "L3: 9.42", "L4: 5.65", "L5: 3.91", "L6: 2.06", "L7: 1.70", "L8: 0.55", "L9: 0.53", "L10: 0.15", "L11: 0.25", "L12: 0.05", "L13: 0.14"]
[2m2025-10-21T17:45:03.620311Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 27.89", "L1: 27.89", "L2: 17.38", "L3: 12.05", "L4: 7.33", "L5: 5.02", "L6: 2.68", "L7: 2.19", "L8: 0.72", "L9: 0.77", "L10: 0.21", "L11: 0.34", "L12: 0.06", "L13: 0.15"]
[2m2025-10-21T17:45:03.656770Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 29.24", "L1: 29.24", "L2: 16.04", "L3: 11.06", "L4: 5.55", "L5: 3.89", "L6: 1.98", "L7: 1.62", "L8: 0.52", "L9: 0.62", "L10: 0.16", "L11: 0.27", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:45:03.672472Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.50h@0.54p | L9: 3.82h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 11.2534 | Complexity: 0.470 [0.008-0.928] | Temp: 5.25 [2.77-7.79] [3mepoch[0m[2m=[0m18 [3mloss[0m[2m=[0m0.4746207594871521 [3mgrad_norm[0m[2m=[0m20.851520538330078 [3mlearning_rate[0m[2m=[0m0.0004986183485016227
[2m2025-10-21T17:45:03.708791Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 26.17", "L1: 26.17", "L2: 18.00", "L3: 12.46", "L4: 8.14", "L5: 5.51", "L6: 3.44", "L7: 2.76", "L8: 0.88", "L9: 0.70", "L10: 0.17", "L11: 0.29", "L12: 0.05", "L13: 0.13"]
[2m2025-10-21T17:45:03.758331Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.83", "L1: 19.83", "L2: 12.30", "L3: 8.54", "L4: 5.37", "L5: 3.72", "L6: 2.45", "L7: 1.94", "L8: 0.65", "L9: 0.64", "L10: 0.17", "L11: 0.28", "L12: 0.05", "L13: 0.13"]
[2m2025-10-21T17:45:03.806483Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.81", "L1: 18.81", "L2: 10.96", "L3: 7.60", "L4: 4.61", "L5: 3.18", "L6: 1.94", "L7: 1.60", "L8: 0.55", "L9: 0.57", "L10: 0.16", "L11: 0.26", "L12: 0.05", "L13: 0.12"]
[2m2025-10-21T17:45:03.855344Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.53", "L1: 25.53", "L2: 13.83", "L3: 9.60", "L4: 5.53", "L5: 3.80", "L6: 2.29", "L7: 1.85", "L8: 0.66", "L9: 0.68", "L10: 0.20", "L11: 0.31", "L12: 0.05", "L13: 0.13"]
[2m2025-10-21T17:45:03.904664Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.93", "L1: 13.93", "L2: 8.50", "L3: 5.90", "L4: 3.54", "L5: 2.47", "L6: 1.48", "L7: 1.26", "L8: 0.41", "L9: 0.44", "L10: 0.14", "L11: 0.24", "L12: 0.04", "L13: 0.13"]
[2m2025-10-21T17:45:03.953264Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.71", "L1: 17.71", "L2: 11.59", "L3: 8.04", "L4: 5.02", "L5: 3.45", "L6: 2.02", "L7: 1.64", "L8: 0.57", "L9: 0.54", "L10: 0.17", "L11: 0.28", "L12: 0.05", "L13: 0.14"]
[2m2025-10-21T17:45:03.986649Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 31.28", "L1: 31.28", "L2: 20.39", "L3: 14.10", "L4: 8.36", "L5: 5.57", "L6: 2.80", "L7: 2.24", "L8: 0.68", "L9: 0.85", "L10: 0.14", "L11: 0.23", "L12: 0.04", "L13: 0.11"]
[2m2025-10-21T17:45:04.001879Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.81h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 11.2881 | Complexity: 0.470 [0.007-0.934] | Temp: 5.24 [2.79-7.70] [3mepoch[0m[2m=[0m19 [3mloss[0m[2m=[0m0.4108026623725891 [3mgrad_norm[0m[2m=[0m16.05742073059082 [3mlearning_rate[0m[2m=[0m0.0004975456395186484
[2m2025-10-21T17:45:04.035926Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 29.73", "L1: 29.73", "L2: 17.63", "L3: 12.19", "L4: 7.45", "L5: 5.08", "L6: 3.21", "L7: 2.58", "L8: 0.89", "L9: 0.83", "L10: 0.18", "L11: 0.28", "L12: 0.05", "L13: 0.13"]
[2m2025-10-21T17:45:04.085057Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.62", "L1: 22.62", "L2: 13.98", "L3: 9.70", "L4: 5.74", "L5: 3.92", "L6: 2.36", "L7: 1.87", "L8: 0.60", "L9: 0.56", "L10: 0.14", "L11: 0.22", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:45:04.131954Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 25.48", "L1: 25.48", "L2: 14.55", "L3: 10.10", "L4: 6.14", "L5: 4.24", "L6: 2.24", "L7: 1.81", "L8: 0.57", "L9: 0.66", "L10: 0.17", "L11: 0.26", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:45:04.179080Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.37", "L1: 13.37", "L2: 7.70", "L3: 5.33", "L4: 3.22", "L5: 2.22", "L6: 1.38", "L7: 1.11", "L8: 0.40", "L9: 0.40", "L10: 0.13", "L11: 0.20", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:45:04.230866Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.71", "L1: 20.71", "L2: 13.14", "L3: 9.15", "L4: 5.62", "L5: 3.92", "L6: 2.39", "L7: 1.96", "L8: 0.65", "L9: 0.66", "L10: 0.16", "L11: 0.26", "L12: 0.05", "L13: 0.13"]
[2m2025-10-21T17:45:04.276028Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.93", "L1: 14.93", "L2: 9.34", "L3: 6.48", "L4: 4.14", "L5: 2.84", "L6: 1.71", "L7: 1.38", "L8: 0.48", "L9: 0.48", "L10: 0.14", "L11: 0.24", "L12: 0.04", "L13: 0.13"]
[2m2025-10-21T17:45:04.309077Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.48", "L1: 24.48", "L2: 13.95", "L3: 9.63", "L4: 5.68", "L5: 3.92", "L6: 1.93", "L7: 1.59", "L8: 0.51", "L9: 0.47", "L10: 0.10", "L11: 0.16", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:04.324354Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.76h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 11.3229 | Complexity: 0.467 [0.007-0.936] | Temp: 5.25 [2.78-7.66] [3mepoch[0m[2m=[0m20 [3mloss[0m[2m=[0m0.37895092368125916 [3mgrad_norm[0m[2m=[0m17.050283432006836 [3mlearning_rate[0m[2m=[0m0.000496168970130384
[2m2025-10-21T17:45:04.358424Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.08", "L1: 17.08", "L2: 10.27", "L3: 7.13", "L4: 4.35", "L5: 3.01", "L6: 1.70", "L7: 1.38", "L8: 0.46", "L9: 0.46", "L10: 0.14", "L11: 0.22", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:45:04.403344Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.31", "L1: 23.31", "L2: 12.97", "L3: 8.98", "L4: 5.43", "L5: 3.70", "L6: 2.17", "L7: 1.75", "L8: 0.57", "L9: 0.58", "L10: 0.14", "L11: 0.22", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:45:04.446787Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 18.43", "L1: 18.43", "L2: 10.56", "L3: 7.34", "L4: 4.28", "L5: 2.99", "L6: 1.73", "L7: 1.44", "L8: 0.48", "L9: 0.46", "L10: 0.13", "L11: 0.21", "L12: 0.04", "L13: 0.11"]
[2m2025-10-21T17:45:04.491851Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.90", "L1: 15.90", "L2: 9.16", "L3: 6.32", "L4: 3.91", "L5: 2.69", "L6: 1.65", "L7: 1.35", "L8: 0.47", "L9: 0.45", "L10: 0.15", "L11: 0.23", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:45:04.536911Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.35", "L1: 21.35", "L2: 12.72", "L3: 8.77", "L4: 5.29", "L5: 3.64", "L6: 2.08", "L7: 1.72", "L8: 0.55", "L9: 0.54", "L10: 0.16", "L11: 0.27", "L12: 0.05", "L13: 0.13"]
[2m2025-10-21T17:45:04.580144Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.12", "L1: 14.12", "L2: 8.93", "L3: 6.17", "L4: 3.77", "L5: 2.61", "L6: 1.54", "L7: 1.26", "L8: 0.42", "L9: 0.46", "L10: 0.15", "L11: 0.26", "L12: 0.04", "L13: 0.13"]
[2m2025-10-21T17:45:04.612853Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.67", "L1: 10.67", "L2: 5.88", "L3: 4.08", "L4: 2.40", "L5: 1.68", "L6: 0.99", "L7: 0.84", "L8: 0.27", "L9: 0.28", "L10: 0.09", "L11: 0.15", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:04.627491Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.78h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 11.3597 | Complexity: 0.469 [0.007-0.933] | Temp: 5.25 [2.80-7.61] [3mepoch[0m[2m=[0m21 [3mloss[0m[2m=[0m0.3487398326396942 [3mgrad_norm[0m[2m=[0m12.208704948425293 [3mlearning_rate[0m[2m=[0m0.0004944902611896396
[2m2025-10-21T17:45:04.662450Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.34", "L1: 17.34", "L2: 9.92", "L3: 6.86", "L4: 3.90", "L5: 2.71", "L6: 1.57", "L7: 1.28", "L8: 0.39", "L9: 0.42", "L10: 0.11", "L11: 0.18", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:04.712278Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.63", "L1: 15.63", "L2: 9.43", "L3: 6.53", "L4: 3.87", "L5: 2.67", "L6: 1.62", "L7: 1.30", "L8: 0.43", "L9: 0.50", "L10: 0.13", "L11: 0.22", "L12: 0.04", "L13: 0.11"]
[2m2025-10-21T17:45:04.755960Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.49", "L1: 14.49", "L2: 8.57", "L3: 5.91", "L4: 3.75", "L5: 2.58", "L6: 1.54", "L7: 1.25", "L8: 0.43", "L9: 0.46", "L10: 0.12", "L11: 0.20", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:04.802085Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.56", "L1: 17.56", "L2: 10.14", "L3: 7.04", "L4: 4.22", "L5: 2.94", "L6: 1.63", "L7: 1.36", "L8: 0.44", "L9: 0.53", "L10: 0.11", "L11: 0.18", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:04.847454Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.26", "L1: 12.26", "L2: 7.21", "L3: 4.98", "L4: 2.94", "L5: 2.04", "L6: 1.21", "L7: 1.01", "L8: 0.34", "L9: 0.38", "L10: 0.11", "L11: 0.19", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:45:04.890761Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.93", "L1: 14.93", "L2: 8.81", "L3: 6.07", "L4: 3.51", "L5: 2.45", "L6: 1.39", "L7: 1.16", "L8: 0.38", "L9: 0.43", "L10: 0.12", "L11: 0.20", "L12: 0.03", "L13: 0.12"]
[2m2025-10-21T17:45:04.939428Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.78h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 11.3988 | Complexity: 0.469 [0.007-0.922] | Temp: 5.24 [2.69-7.58] [3mepoch[0m[2m=[0m22 [3mloss[0m[2m=[0m0.3191395103931427 [3mgrad_norm[0m[2m=[0m9.469300270080566 [3mlearning_rate[0m[2m=[0m0.000492511666379869
[2m2025-10-21T17:45:04.971666Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.19", "L1: 16.19", "L2: 9.80", "L3: 6.79", "L4: 4.11", "L5: 2.81", "L6: 1.65", "L7: 1.33", "L8: 0.41", "L9: 0.40", "L10: 0.11", "L11: 0.18", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:05.025194Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.67", "L1: 22.67", "L2: 12.90", "L3: 8.93", "L4: 5.15", "L5: 3.60", "L6: 2.19", "L7: 1.84", "L8: 0.62", "L9: 0.56", "L10: 0.13", "L11: 0.21", "L12: 0.04", "L13: 0.11"]
[2m2025-10-21T17:45:05.069889Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.27", "L1: 14.27", "L2: 8.53", "L3: 5.89", "L4: 3.44", "L5: 2.38", "L6: 1.26", "L7: 1.01", "L8: 0.33", "L9: 0.40", "L10: 0.10", "L11: 0.17", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:05.114850Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.95", "L1: 17.95", "L2: 10.36", "L3: 7.19", "L4: 4.13", "L5: 2.84", "L6: 1.58", "L7: 1.29", "L8: 0.45", "L9: 0.45", "L10: 0.13", "L11: 0.21", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:45:05.162826Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.57", "L1: 22.57", "L2: 12.25", "L3: 8.46", "L4: 4.63", "L5: 3.27", "L6: 1.76", "L7: 1.48", "L8: 0.49", "L9: 0.50", "L10: 0.12", "L11: 0.19", "L12: 0.04", "L13: 0.12"]
[2m2025-10-21T17:45:05.207017Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 16.63", "L1: 16.63", "L2: 9.12", "L3: 6.31", "L4: 3.57", "L5: 2.52", "L6: 1.30", "L7: 1.08", "L8: 0.37", "L9: 0.37", "L10: 0.12", "L11: 0.20", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:05.256884Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.75h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 11.4381 | Complexity: 0.468 [0.006-0.920] | Temp: 5.23 [2.55-7.53] [3mepoch[0m[2m=[0m23 [3mloss[0m[2m=[0m0.30414316058158875 [3mgrad_norm[0m[2m=[0m11.632604598999023 [3mlearning_rate[0m[2m=[0m0.000490236037876457
[2m2025-10-21T17:45:05.289412Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 22.71", "L1: 22.71", "L2: 13.84", "L3: 9.56", "L4: 5.52", "L5: 3.76", "L6: 1.84", "L7: 1.49", "L8: 0.51", "L9: 0.46", "L10: 0.11", "L11: 0.18", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:05.337039Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.23", "L1: 17.23", "L2: 10.31", "L3: 7.11", "L4: 4.25", "L5: 2.95", "L6: 1.62", "L7: 1.33", "L8: 0.45", "L9: 0.50", "L10: 0.12", "L11: 0.19", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:05.383590Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 19.76", "L1: 19.76", "L2: 11.85", "L3: 8.18", "L4: 4.85", "L5: 3.31", "L6: 1.74", "L7: 1.40", "L8: 0.44", "L9: 0.50", "L10: 0.13", "L11: 0.21", "L12: 0.04", "L13: 0.11"]
[2m2025-10-21T17:45:05.429760Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.39", "L1: 15.39", "L2: 9.09", "L3: 6.29", "L4: 3.82", "L5: 2.62", "L6: 1.48", "L7: 1.21", "L8: 0.39", "L9: 0.39", "L10: 0.10", "L11: 0.15", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:05.476342Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.94", "L1: 10.94", "L2: 6.66", "L3: 4.60", "L4: 2.82", "L5: 1.94", "L6: 1.11", "L7: 0.94", "L8: 0.30", "L9: 0.32", "L10: 0.10", "L11: 0.16", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:05.523715Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.72", "L1: 15.72", "L2: 9.34", "L3: 6.44", "L4: 3.81", "L5: 2.65", "L6: 1.33", "L7: 1.11", "L8: 0.35", "L9: 0.39", "L10: 0.11", "L11: 0.19", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:05.555763Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.77", "L1: 15.77", "L2: 10.47", "L3: 7.21", "L4: 4.52", "L5: 3.07", "L6: 2.11", "L7: 1.77", "L8: 0.49", "L9: 0.55", "L10: 0.07", "L11: 0.12", "L12: 0.02", "L13: 0.09"]
[2m2025-10-21T17:45:05.571243Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.72h@0.58p | ThreshRange: [0.54-0.58] | PredNorm: 11.4769 | Complexity: 0.466 [0.005-0.921] | Temp: 5.23 [2.59-7.48] [3mepoch[0m[2m=[0m24 [3mloss[0m[2m=[0m0.2927928566932678 [3mgrad_norm[0m[2m=[0m12.142155647277832 [3mlearning_rate[0m[2m=[0m0.0004876663733739406
[2m2025-10-21T17:45:05.609438Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.91", "L1: 10.91", "L2: 6.62", "L3: 4.57", "L4: 2.80", "L5: 1.94", "L6: 1.09", "L7: 0.89", "L8: 0.29", "L9: 0.30", "L10: 0.08", "L11: 0.14", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:05.666681Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.42", "L1: 10.42", "L2: 5.86", "L3: 4.04", "L4: 2.33", "L5: 1.62", "L6: 0.90", "L7: 0.74", "L8: 0.25", "L9: 0.29", "L10: 0.09", "L11: 0.14", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:05.715873Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.82", "L1: 12.82", "L2: 7.72", "L3: 5.34", "L4: 3.27", "L5: 2.23", "L6: 1.24", "L7: 0.99", "L8: 0.32", "L9: 0.33", "L10: 0.09", "L11: 0.14", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:05.761853Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.81", "L1: 12.81", "L2: 7.57", "L3: 5.23", "L4: 3.25", "L5: 2.23", "L6: 1.26", "L7: 1.02", "L8: 0.35", "L9: 0.34", "L10: 0.09", "L11: 0.14", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:05.808272Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.28", "L1: 10.28", "L2: 6.72", "L3: 4.64", "L4: 2.94", "L5: 2.04", "L6: 1.13", "L7: 0.94", "L8: 0.32", "L9: 0.33", "L10: 0.11", "L11: 0.17", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:05.852844Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.55", "L1: 14.55", "L2: 8.44", "L3: 5.83", "L4: 3.29", "L5: 2.35", "L6: 1.24", "L7: 1.03", "L8: 0.31", "L9: 0.36", "L10: 0.09", "L11: 0.16", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:05.901028Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.66h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.5194 | Complexity: 0.463 [0.005-0.928] | Temp: 5.23 [2.59-7.47] [3mepoch[0m[2m=[0m25 [3mloss[0m[2m=[0m0.27806925773620605 [3mgrad_norm[0m[2m=[0m7.414852619171143 [3mlearning_rate[0m[2m=[0m0.00048480628174729645
[2m2025-10-21T17:45:05.932986Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.50", "L1: 10.50", "L2: 6.58", "L3: 4.52", "L4: 2.78", "L5: 1.90", "L6: 1.10", "L7: 0.88", "L8: 0.30", "L9: 0.30", "L10: 0.10", "L11: 0.16", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:05.980602Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.56", "L1: 12.56", "L2: 6.75", "L3: 4.67", "L4: 2.60", "L5: 1.82", "L6: 1.01", "L7: 0.82", "L8: 0.27", "L9: 0.32", "L10: 0.10", "L11: 0.17", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:06.026990Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.63", "L1: 11.63", "L2: 6.33", "L3: 4.38", "L4: 2.40", "L5: 1.67", "L6: 0.97", "L7: 0.79", "L8: 0.26", "L9: 0.27", "L10: 0.09", "L11: 0.14", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:06.077238Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.35", "L1: 14.35", "L2: 8.01", "L3: 5.54", "L4: 3.20", "L5: 2.21", "L6: 1.31", "L7: 1.05", "L8: 0.34", "L9: 0.36", "L10: 0.10", "L11: 0.16", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:06.127797Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.54", "L1: 17.54", "L2: 11.10", "L3: 7.66", "L4: 4.64", "L5: 3.15", "L6: 1.74", "L7: 1.41", "L8: 0.45", "L9: 0.39", "L10: 0.10", "L11: 0.16", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:06.172825Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.80", "L1: 13.80", "L2: 7.85", "L3: 5.41", "L4: 3.12", "L5: 2.17", "L6: 1.15", "L7: 0.95", "L8: 0.31", "L9: 0.34", "L10: 0.10", "L11: 0.16", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:06.223935Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.5620 | Complexity: 0.464 [0.004-0.933] | Temp: 5.22 [2.60-7.47] [3mepoch[0m[2m=[0m26 [3mloss[0m[2m=[0m0.2729698121547699 [3mgrad_norm[0m[2m=[0m8.457573890686035 [3mlearning_rate[0m[2m=[0m0.0004816595755983144
[2m2025-10-21T17:45:06.257245Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.47", "L1: 14.47", "L2: 8.09", "L3: 5.59", "L4: 3.26", "L5: 2.27", "L6: 1.21", "L7: 0.97", "L8: 0.32", "L9: 0.35", "L10: 0.07", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:06.305511Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.91", "L1: 12.91", "L2: 8.02", "L3: 5.55", "L4: 3.69", "L5: 2.49", "L6: 1.59", "L7: 1.24", "L8: 0.43", "L9: 0.38", "L10: 0.10", "L11: 0.17", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:06.354976Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.83", "L1: 12.83", "L2: 7.87", "L3: 5.43", "L4: 3.45", "L5: 2.38", "L6: 1.46", "L7: 1.19", "L8: 0.39", "L9: 0.40", "L10: 0.11", "L11: 0.17", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:06.403678Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.80", "L1: 11.80", "L2: 6.78", "L3: 4.68", "L4: 2.77", "L5: 1.91", "L6: 0.94", "L7: 0.76", "L8: 0.27", "L9: 0.26", "L10: 0.08", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:06.452989Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.22", "L1: 14.22", "L2: 8.99", "L3: 6.20", "L4: 3.75", "L5: 2.59", "L6: 1.34", "L7: 1.08", "L8: 0.39", "L9: 0.34", "L10: 0.09", "L11: 0.13", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:06.496650Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 24.60", "L1: 24.60", "L2: 13.72", "L3: 9.46", "L4: 5.38", "L5: 3.80", "L6: 1.91", "L7: 1.55", "L8: 0.49", "L9: 0.48", "L10: 0.12", "L11: 0.20", "L12: 0.03", "L13: 0.12"]
[2m2025-10-21T17:45:06.542611Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.71h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.6021 | Complexity: 0.465 [0.004-0.929] | Temp: 5.22 [2.48-7.48] [3mepoch[0m[2m=[0m27 [3mloss[0m[2m=[0m0.27317121624946594 [3mgrad_norm[0m[2m=[0m11.372888565063477 [3mlearning_rate[0m[2m=[0m0.00047823062050156295
[2m2025-10-21T17:45:06.572943Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.42", "L1: 10.42", "L2: 5.72", "L3: 3.94", "L4: 2.26", "L5: 1.56", "L6: 0.89", "L7: 0.71", "L8: 0.23", "L9: 0.28", "L10: 0.06", "L11: 0.11", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:06.663268Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.71", "L1: 10.71", "L2: 7.20", "L3: 4.98", "L4: 3.09", "L5: 2.14", "L6: 1.40", "L7: 1.15", "L8: 0.37", "L9: 0.37", "L10: 0.10", "L11: 0.16", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:06.754906Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.97", "L1: 11.97", "L2: 6.46", "L3: 4.46", "L4: 2.49", "L5: 1.73", "L6: 0.98", "L7: 0.81", "L8: 0.27", "L9: 0.28", "L10: 0.07", "L11: 0.12", "L12: 0.02", "L13: 0.11"]
[2m2025-10-21T17:45:06.844988Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.74h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.6472 | Complexity: 0.467 [0.005-0.920] | Temp: 5.21 [2.50-7.49] [3mepoch[0m[2m=[0m28 [3mloss[0m[2m=[0m0.2597060799598694 [3mgrad_norm[0m[2m=[0m6.0757293701171875 [3mlearning_rate[0m[2m=[0m0.00047452410217374563
[2m2025-10-21T17:45:06.878097Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.22", "L1: 10.22", "L2: 6.24", "L3: 4.27", "L4: 2.60", "L5: 1.78", "L6: 0.91", "L7: 0.75", "L8: 0.22", "L9: 0.25", "L10: 0.08", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:07.015914Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.78", "L1: 14.78", "L2: 9.36", "L3: 6.47", "L4: 4.03", "L5: 2.75", "L6: 1.75", "L7: 1.43", "L8: 0.47", "L9: 0.44", "L10: 0.09", "L11: 0.15", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:07.059783Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.52", "L1: 11.52", "L2: 6.17", "L3: 4.25", "L4: 2.41", "L5: 1.68", "L6: 0.96", "L7: 0.78", "L8: 0.26", "L9: 0.29", "L10: 0.09", "L11: 0.14", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:07.149190Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.71h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.6907 | Complexity: 0.466 [0.004-0.916] | Temp: 5.22 [2.54-7.48] [3mepoch[0m[2m=[0m29 [3mloss[0m[2m=[0m0.2624959349632263 [3mgrad_norm[0m[2m=[0m7.332118988037109 [3mlearning_rate[0m[2m=[0m0.000470545026473701
[2m2025-10-21T17:45:07.183282Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.52", "L1: 10.52", "L2: 6.05", "L3: 4.16", "L4: 2.56", "L5: 1.76", "L6: 1.00", "L7: 0.82", "L8: 0.28", "L9: 0.31", "L10: 0.09", "L11: 0.14", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:07.230388Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.89", "L1: 11.89", "L2: 7.22", "L3: 4.97", "L4: 2.96", "L5: 2.05", "L6: 1.12", "L7: 0.90", "L8: 0.32", "L9: 0.32", "L10: 0.10", "L11: 0.16", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:07.277306Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.56", "L1: 13.56", "L2: 7.73", "L3: 5.34", "L4: 3.04", "L5: 2.09", "L6: 1.15", "L7: 0.95", "L8: 0.33", "L9: 0.35", "L10: 0.09", "L11: 0.15", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:07.322803Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.35", "L1: 13.35", "L2: 7.95", "L3: 5.49", "L4: 3.27", "L5: 2.22", "L6: 1.18", "L7: 0.96", "L8: 0.31", "L9: 0.33", "L10: 0.08", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:07.407597Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.25", "L1: 10.25", "L2: 6.63", "L3: 4.58", "L4: 2.81", "L5: 1.95", "L6: 0.93", "L7: 0.76", "L8: 0.25", "L9: 0.26", "L10: 0.08", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:07.453529Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.68h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.7342 | Complexity: 0.464 [0.004-0.912] | Temp: 5.22 [2.56-7.50] [3mepoch[0m[2m=[0m30 [3mloss[0m[2m=[0m0.25361940264701843 [3mgrad_norm[0m[2m=[0m7.493801593780518 [3mlearning_rate[0m[2m=[0m0.0004662988649215549
[2m2025-10-21T17:45:07.530731Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 21.41", "L1: 21.41", "L2: 11.93", "L3: 8.21", "L4: 4.78", "L5: 3.31", "L6: 1.78", "L7: 1.43", "L8: 0.46", "L9: 0.47", "L10: 0.14", "L11: 0.21", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:07.575399Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.01", "L1: 10.01", "L2: 5.80", "L3: 4.01", "L4: 2.32", "L5: 1.59", "L6: 0.85", "L7: 0.70", "L8: 0.23", "L9: 0.26", "L10: 0.07", "L11: 0.12", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:07.666670Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.69", "L1: 17.69", "L2: 11.75", "L3: 8.05", "L4: 5.27", "L5: 3.50", "L6: 2.16", "L7: 1.78", "L8: 0.55", "L9: 0.46", "L10: 0.09", "L11: 0.14", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:07.754856Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.61h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.7774 | Complexity: 0.461 [0.004-0.913] | Temp: 5.23 [2.63-7.50] [3mepoch[0m[2m=[0m31 [3mloss[0m[2m=[0m0.2525940239429474 [3mgrad_norm[0m[2m=[0m9.297469139099121 [3mlearning_rate[0m[2m=[0m0.0004617914091795683
[2m2025-10-21T17:45:07.840645Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.05", "L1: 11.05", "L2: 5.88", "L3: 4.05", "L4: 2.18", "L5: 1.53", "L6: 0.82", "L7: 0.67", "L8: 0.22", "L9: 0.25", "L10: 0.08", "L11: 0.14", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:07.885323Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.24", "L1: 10.24", "L2: 5.94", "L3: 4.10", "L4: 2.36", "L5: 1.63", "L6: 0.94", "L7: 0.76", "L8: 0.24", "L9: 0.27", "L10: 0.08", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:07.974622Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.38", "L1: 10.38", "L2: 5.81", "L3: 4.00", "L4: 2.27", "L5: 1.58", "L6: 0.93", "L7: 0.80", "L8: 0.25", "L9: 0.31", "L10: 0.09", "L11: 0.15", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:08.067988Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.64h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.8232 | Complexity: 0.461 [0.004-0.912] | Temp: 5.24 [2.67-7.48] [3mepoch[0m[2m=[0m32 [3mloss[0m[2m=[0m0.24730134010314941 [3mgrad_norm[0m[2m=[0m5.141598224639893 [3mlearning_rate[0m[2m=[0m0.00045702882925979793
[2m2025-10-21T17:45:08.100784Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.25", "L1: 14.25", "L2: 8.59", "L3: 5.93", "L4: 3.59", "L5: 2.45", "L6: 1.17", "L7: 0.96", "L8: 0.27", "L9: 0.28", "L10: 0.07", "L11: 0.11", "L12: 0.02", "L13: 0.09"]
[2m2025-10-21T17:45:08.341651Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.10", "L1: 10.10", "L2: 6.70", "L3: 4.60", "L4: 2.98", "L5: 2.01", "L6: 1.04", "L7: 0.80", "L8: 0.28", "L9: 0.30", "L10: 0.08", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:08.388517Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.62h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.8669 | Complexity: 0.461 [0.004-0.909] | Temp: 5.25 [2.68-7.51] [3mepoch[0m[2m=[0m33 [3mloss[0m[2m=[0m0.24247394502162933 [3mgrad_norm[0m[2m=[0m7.512210845947266 [3mlearning_rate[0m[2m=[0m0.00045201764442026615
[2m2025-10-21T17:45:08.466443Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.61", "L1: 10.61", "L2: 5.71", "L3: 3.94", "L4: 2.14", "L5: 1.51", "L6: 0.85", "L7: 0.70", "L8: 0.23", "L9: 0.26", "L10: 0.08", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:08.687959Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.65h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.9125 | Complexity: 0.462 [0.004-0.909] | Temp: 5.24 [2.74-7.56] [3mepoch[0m[2m=[0m34 [3mloss[0m[2m=[0m0.23712585866451263 [3mgrad_norm[0m[2m=[0m5.07230806350708 [3mlearning_rate[0m[2m=[0m0.00044676463585346937
[2m2025-10-21T17:45:08.719198Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.63", "L1: 15.63", "L2: 10.74", "L3: 7.43", "L4: 5.00", "L5: 3.32", "L6: 1.91", "L7: 1.48", "L8: 0.45", "L9: 0.39", "L10: 0.09", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:08.982606Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 11.9572 | Complexity: 0.464 [0.004-0.909] | Temp: 5.25 [2.74-7.61] [3mepoch[0m[2m=[0m35 [3mloss[0m[2m=[0m0.23814097046852112 [3mgrad_norm[0m[2m=[0m6.303807258605957 [3mlearning_rate[0m[2m=[0m0.00044127702130936086
[2m2025-10-21T17:45:09.166529Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 20.04", "L1: 20.04", "L2: 12.07", "L3: 8.32", "L4: 5.22", "L5: 3.53", "L6: 1.96", "L7: 1.58", "L8: 0.45", "L9: 0.41", "L10: 0.07", "L11: 0.11", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:09.311491Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.66h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.0012 | Complexity: 0.463 [0.004-0.909] | Temp: 5.25 [2.74-7.63] [3mepoch[0m[2m=[0m36 [3mloss[0m[2m=[0m0.23517432808876038 [3mgrad_norm[0m[2m=[0m7.832180023193359 [3mlearning_rate[0m[2m=[0m0.00043556230957619846
[2m2025-10-21T17:45:09.480674Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.47", "L1: 13.47", "L2: 7.43", "L3: 5.12", "L4: 2.99", "L5: 2.08", "L6: 1.17", "L7: 0.96", "L8: 0.32", "L9: 0.34", "L10: 0.10", "L11: 0.17", "L12: 0.03", "L13: 0.10"]
[2m2025-10-21T17:45:09.522864Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 12.69", "L1: 12.69", "L2: 7.62", "L3: 5.23", "L4: 2.91", "L5: 2.02", "L6: 0.86", "L7: 0.70", "L8: 0.21", "L9: 0.25", "L10: 0.06", "L11: 0.10", "L12: 0.02", "L13: 0.11"]
[2m2025-10-21T17:45:09.614089Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.0454 | Complexity: 0.461 [0.004-0.911] | Temp: 5.24 [2.73-7.64] [3mepoch[0m[2m=[0m37 [3mloss[0m[2m=[0m0.2382548302412033 [3mgrad_norm[0m[2m=[0m6.282956600189209 [3mlearning_rate[0m[2m=[0m0.0004296282713767141
[2m2025-10-21T17:45:09.922880Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.61h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.0906 | Complexity: 0.460 [0.004-0.910] | Temp: 5.25 [2.73-7.64] [3mepoch[0m[2m=[0m38 [3mloss[0m[2m=[0m0.2322411686182022 [3mgrad_norm[0m[2m=[0m4.937021732330322 [3mlearning_rate[0m[2m=[0m0.00042348302667960525
[2m2025-10-21T17:45:10.140984Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 29.39", "L1: 29.39", "L2: 18.79", "L3: 12.89", "L4: 8.12", "L5: 5.48", "L6: 2.68", "L7: 2.23", "L8: 0.72", "L9: 0.73", "L10: 0.07", "L11: 0.12", "L12: 0.02", "L13: 0.11"]
[2m2025-10-21T17:45:10.228218Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.51h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.1333 | Complexity: 0.462 [0.004-0.908] | Temp: 5.25 [2.77-7.66] [3mepoch[0m[2m=[0m39 [3mloss[0m[2m=[0m0.22989937663078308 [3mgrad_norm[0m[2m=[0m9.801774024963379 [3mlearning_rate[0m[2m=[0m0.00041713498649187386
[2m2025-10-21T17:45:10.434486Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.04", "L1: 10.04", "L2: 6.33", "L3: 4.33", "L4: 2.54", "L5: 1.73", "L6: 0.83", "L7: 0.69", "L8: 0.22", "L9: 0.26", "L10: 0.07", "L11: 0.11", "L12: 0.02", "L13: 0.11"]
[2m2025-10-21T17:45:10.523034Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.67h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.1770 | Complexity: 0.464 [0.005-0.907] | Temp: 5.24 [2.78-7.66] [3mepoch[0m[2m=[0m40 [3mloss[0m[2m=[0m0.23245231807231903 [3mgrad_norm[0m[2m=[0m4.73348331451416 [3mlearning_rate[0m[2m=[0m0.00041059282375499606
[2m2025-10-21T17:45:10.814720Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.70h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.2213 | Complexity: 0.466 [0.005-0.910] | Temp: 5.24 [2.79-7.63] [3mepoch[0m[2m=[0m41 [3mloss[0m[2m=[0m0.2304583340883255 [3mgrad_norm[0m[2m=[0m4.450244903564453 [3mlearning_rate[0m[2m=[0m0.00040386541513726115
[2m2025-10-21T17:45:11.111048Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.75h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.2661 | Complexity: 0.468 [0.005-0.903] | Temp: 5.23 [2.75-7.65] [3mepoch[0m[2m=[0m42 [3mloss[0m[2m=[0m0.22923798859119415 [3mgrad_norm[0m[2m=[0m3.648695468902588 [3mlearning_rate[0m[2m=[0m0.0003969620156567544
[2m2025-10-21T17:45:11.416155Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.77h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.3107 | Complexity: 0.468 [0.005-0.904] | Temp: 5.23 [2.76-7.67] [3mepoch[0m[2m=[0m43 [3mloss[0m[2m=[0m0.22517704963684082 [3mgrad_norm[0m[2m=[0m3.5781590938568115 [3mlearning_rate[0m[2m=[0m0.00038989202585071325
[2m2025-10-21T17:45:11.712059Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.54", "L1: 11.54", "L2: 8.19", "L3: 5.56", "L4: 3.33", "L5: 2.28", "L6: 1.17", "L7: 0.91", "L8: 0.31", "L9: 0.23", "L10: 0.06", "L11: 0.09", "L12: 0.02", "L13: 0.09"]
[2m2025-10-21T17:45:11.726824Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.78h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.3526 | Complexity: 0.469 [0.005-0.908] | Temp: 5.23 [2.74-7.65] [3mepoch[0m[2m=[0m44 [3mloss[0m[2m=[0m0.22571226954460144 [3mgrad_norm[0m[2m=[0m6.003726482391357 [3mlearning_rate[0m[2m=[0m0.0003826651081908494
[2m2025-10-21T17:45:11.977041Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.50", "L1: 13.50", "L2: 7.90", "L3: 5.44", "L4: 3.04", "L5: 2.17", "L6: 1.12", "L7: 1.01", "L8: 0.27", "L9: 0.30", "L10: 0.06", "L11: 0.10", "L12: 0.02", "L13: 0.11"]
[2m2025-10-21T17:45:12.079142Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.76h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.3938 | Complexity: 0.468 [0.004-0.902] | Temp: 5.24 [2.77-7.61] [3mepoch[0m[2m=[0m45 [3mloss[0m[2m=[0m0.22769863903522491 [3mgrad_norm[0m[2m=[0m5.836104393005371 [3mlearning_rate[0m[2m=[0m0.0003752911288756877
[2m2025-10-21T17:45:12.398100Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.51", "L1: 11.51", "L2: 7.05", "L3: 4.83", "L4: 3.06", "L5: 2.12", "L6: 1.22", "L7: 0.98", "L8: 0.29", "L9: 0.31", "L10: 0.05", "L11: 0.09", "L12: 0.02", "L13: 0.09"]
[2m2025-10-21T17:45:12.412614Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.75h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.4351 | Complexity: 0.468 [0.004-0.904] | Temp: 5.24 [2.75-7.60] [3mepoch[0m[2m=[0m46 [3mloss[0m[2m=[0m0.22425411641597748 [3mgrad_norm[0m[2m=[0m5.373810768127441 [3mlearning_rate[0m[2m=[0m0.0003677802160382271
[2m2025-10-21T17:45:12.583698Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 11.97", "L1: 11.97", "L2: 6.04", "L3: 4.17", "L4: 2.24", "L5: 1.56", "L6: 0.97", "L7: 0.81", "L8: 0.22", "L9: 0.22", "L10: 0.05", "L11: 0.08", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:12.730047Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.75h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.4761 | Complexity: 0.468 [0.004-0.909] | Temp: 5.23 [2.76-7.61] [3mepoch[0m[2m=[0m47 [3mloss[0m[2m=[0m0.22283518314361572 [3mgrad_norm[0m[2m=[0m4.660459995269775 [3mlearning_rate[0m[2m=[0m0.000360142468707636
[2m2025-10-21T17:45:12.895595Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 17.70", "L1: 17.70", "L2: 9.61", "L3: 6.60", "L4: 3.50", "L5: 2.44", "L6: 1.27", "L7: 1.07", "L8: 0.30", "L9: 0.26", "L10: 0.09", "L11: 0.15", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:13.035329Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.77h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.5163 | Complexity: 0.469 [0.004-0.910] | Temp: 5.23 [2.72-7.61] [3mepoch[0m[2m=[0m48 [3mloss[0m[2m=[0m0.22495406866073608 [3mgrad_norm[0m[2m=[0m6.383477210998535 [3mlearning_rate[0m[2m=[0m0.0003523885097820312
[2m2025-10-21T17:45:13.205332Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.88", "L1: 13.88", "L2: 8.05", "L3: 5.59", "L4: 3.23", "L5: 2.25", "L6: 0.90", "L7: 0.72", "L8: 0.24", "L9: 0.23", "L10: 0.09", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:13.342724Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.77h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.5563 | Complexity: 0.469 [0.004-0.909] | Temp: 5.22 [2.57-7.60] [3mepoch[0m[2m=[0m49 [3mloss[0m[2m=[0m0.22606545686721802 [3mgrad_norm[0m[2m=[0m4.806630611419678 [3mlearning_rate[0m[2m=[0m0.00034452881664037704
[2m2025-10-21T17:45:13.521879Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.75", "L1: 14.75", "L2: 8.04", "L3: 5.54", "L4: 3.04", "L5: 2.15", "L6: 1.00", "L7: 0.82", "L8: 0.27", "L9: 0.28", "L10: 0.08", "L11: 0.13", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:13.668880Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.74h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.5950 | Complexity: 0.468 [0.004-0.908] | Temp: 5.22 [2.51-7.59] [3mepoch[0m[2m=[0m50 [3mloss[0m[2m=[0m0.22411583364009857 [3mgrad_norm[0m[2m=[0m5.582622051239014 [3mlearning_rate[0m[2m=[0m0.0003365741576999426
[2m2025-10-21T17:45:13.976261Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.71h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.6330 | Complexity: 0.466 [0.004-0.906] | Temp: 5.22 [2.48-7.60] [3mepoch[0m[2m=[0m51 [3mloss[0m[2m=[0m0.22343774139881134 [3mgrad_norm[0m[2m=[0m5.5601630210876465 [3mlearning_rate[0m[2m=[0m0.00032853541779331863
[2m2025-10-21T17:45:14.279460Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.70h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.6723 | Complexity: 0.465 [0.004-0.903] | Temp: 5.23 [2.56-7.62] [3mepoch[0m[2m=[0m52 [3mloss[0m[2m=[0m0.2201901525259018 [3mgrad_norm[0m[2m=[0m3.102135181427002 [3mlearning_rate[0m[2m=[0m0.0003204235399607569
[2m2025-10-21T17:45:14.581753Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.71h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.7109 | Complexity: 0.466 [0.004-0.903] | Temp: 5.23 [2.61-7.62] [3mepoch[0m[2m=[0m53 [3mloss[0m[2m=[0m0.21735581755638123 [3mgrad_norm[0m[2m=[0m2.7873451709747314 [3mlearning_rate[0m[2m=[0m0.0003122496127616614
[2m2025-10-21T17:45:14.880848Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.71h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.7483 | Complexity: 0.466 [0.004-0.902] | Temp: 5.23 [2.62-7.62] [3mepoch[0m[2m=[0m54 [3mloss[0m[2m=[0m0.21679195761680603 [3mgrad_norm[0m[2m=[0m2.7003519535064697 [3mlearning_rate[0m[2m=[0m0.000304024841170758
[2m2025-10-21T17:45:15.182041Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.71h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.7854 | Complexity: 0.466 [0.004-0.901] | Temp: 5.22 [2.60-7.61] [3mepoch[0m[2m=[0m55 [3mloss[0m[2m=[0m0.2163841724395752 [3mgrad_norm[0m[2m=[0m2.201198101043701 [3mlearning_rate[0m[2m=[0m0.0002957603719551116
[2m2025-10-21T17:45:15.480485Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.71h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.8206 | Complexity: 0.466 [0.004-0.901] | Temp: 5.22 [2.59-7.60] [3mepoch[0m[2m=[0m56 [3mloss[0m[2m=[0m0.2170770764350891 [3mgrad_norm[0m[2m=[0m3.3329243659973145 [3mlearning_rate[0m[2m=[0m0.00028746758471243083
[2m2025-10-21T17:45:15.771649Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.71h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.8556 | Complexity: 0.466 [0.004-0.898] | Temp: 5.22 [2.59-7.59] [3mepoch[0m[2m=[0m57 [3mloss[0m[2m=[0m0.2155795395374298 [3mgrad_norm[0m[2m=[0m2.4075586795806885 [3mlearning_rate[0m[2m=[0m0.00027915777172893286
[2m2025-10-21T17:45:16.072880Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.8893 | Complexity: 0.465 [0.004-0.898] | Temp: 5.22 [2.56-7.59] [3mepoch[0m[2m=[0m58 [3mloss[0m[2m=[0m0.21486027538776398 [3mgrad_norm[0m[2m=[0m2.735027313232422 [3mlearning_rate[0m[2m=[0m0.00027084225439466536
[2m2025-10-21T17:45:16.375562Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.9228 | Complexity: 0.465 [0.004-0.902] | Temp: 5.22 [2.55-7.59] [3mepoch[0m[2m=[0m59 [3mloss[0m[2m=[0m0.212869331240654 [3mgrad_norm[0m[2m=[0m1.9632997512817383 [3mlearning_rate[0m[2m=[0m0.0002625324123073369
[2m2025-10-21T17:45:16.684002Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.9555 | Complexity: 0.465 [0.004-0.903] | Temp: 5.22 [2.55-7.59] [3mepoch[0m[2m=[0m60 [3mloss[0m[2m=[0m0.21358253061771393 [3mgrad_norm[0m[2m=[0m1.9837654829025269 [3mlearning_rate[0m[2m=[0m0.0002542395959608257
[2m2025-10-21T17:45:16.999155Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.70h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 12.9873 | Complexity: 0.466 [0.004-0.903] | Temp: 5.22 [2.57-7.58] [3mepoch[0m[2m=[0m61 [3mloss[0m[2m=[0m0.21237322688102722 [3mgrad_norm[0m[2m=[0m1.835302710533142 [3mlearning_rate[0m[2m=[0m0.00024597521405667067
[2m2025-10-21T17:45:17.325124Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.70h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.0181 | Complexity: 0.465 [0.004-0.902] | Temp: 5.21 [2.57-7.57] [3mepoch[0m[2m=[0m62 [3mloss[0m[2m=[0m0.21179404854774475 [3mgrad_norm[0m[2m=[0m1.7993539571762085 [3mlearning_rate[0m[2m=[0m0.00023775038425810635
[2m2025-10-21T17:45:17.638168Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.70h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.0478 | Complexity: 0.465 [0.004-0.902] | Temp: 5.21 [2.57-7.56] [3mepoch[0m[2m=[0m63 [3mloss[0m[2m=[0m0.21210424602031708 [3mgrad_norm[0m[2m=[0m1.8586757183074951 [3mlearning_rate[0m[2m=[0m0.0002295764716109261
[2m2025-10-21T17:45:17.957903Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.0767 | Complexity: 0.465 [0.004-0.902] | Temp: 5.22 [2.59-7.64] [3mepoch[0m[2m=[0m64 [3mloss[0m[2m=[0m0.21179640293121338 [3mgrad_norm[0m[2m=[0m1.7300007343292236 [3mlearning_rate[0m[2m=[0m0.0002214645646745339
[2m2025-10-21T17:45:18.282010Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.1046 | Complexity: 0.465 [0.004-0.902] | Temp: 5.22 [2.62-7.65] [3mepoch[0m[2m=[0m65 [3mloss[0m[2m=[0m0.21103398501873016 [3mgrad_norm[0m[2m=[0m1.5246576070785522 [3mlearning_rate[0m[2m=[0m0.00021342582476790994
[2m2025-10-21T17:45:18.606049Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.1315 | Complexity: 0.465 [0.004-0.902] | Temp: 5.22 [2.65-7.65] [3mepoch[0m[2m=[0m66 [3mloss[0m[2m=[0m0.21099631488323212 [3mgrad_norm[0m[2m=[0m1.50723135471344 [3mlearning_rate[0m[2m=[0m0.00020547113672364503
[2m2025-10-21T17:45:18.936342Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.1575 | Complexity: 0.465 [0.004-0.901] | Temp: 5.22 [2.66-7.65] [3mepoch[0m[2m=[0m67 [3mloss[0m[2m=[0m0.21007762849330902 [3mgrad_norm[0m[2m=[0m1.6667206287384033 [3mlearning_rate[0m[2m=[0m0.00019761148723773658
[2m2025-10-21T17:45:19.283770Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.1825 | Complexity: 0.465 [0.004-0.900] | Temp: 5.22 [2.67-7.65] [3mepoch[0m[2m=[0m68 [3mloss[0m[2m=[0m0.20972606539726257 [3mgrad_norm[0m[2m=[0m1.4226802587509155 [3mlearning_rate[0m[2m=[0m0.00018985752831213176
[2m2025-10-21T17:45:19.586953Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.2067 | Complexity: 0.465 [0.004-0.899] | Temp: 5.22 [2.68-7.64] [3mepoch[0m[2m=[0m69 [3mloss[0m[2m=[0m0.20948843657970428 [3mgrad_norm[0m[2m=[0m1.3262722492218018 [3mlearning_rate[0m[2m=[0m0.00018221982463728637
[2m2025-10-21T17:45:19.879261Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.2296 | Complexity: 0.465 [0.004-0.899] | Temp: 5.23 [2.69-7.64] [3mepoch[0m[2m=[0m70 [3mloss[0m[2m=[0m0.21107590198516846 [3mgrad_norm[0m[2m=[0m2.0442328453063965 [3mlearning_rate[0m[2m=[0m0.00017470888269599527
[2m2025-10-21T17:45:20.183171Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.2517 | Complexity: 0.465 [0.004-0.899] | Temp: 5.22 [2.68-7.63] [3mepoch[0m[2m=[0m71 [3mloss[0m[2m=[0m0.21132096648216248 [3mgrad_norm[0m[2m=[0m1.9776102304458618 [3mlearning_rate[0m[2m=[0m0.0001673348742770031
[2m2025-10-21T17:45:20.480882Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.2732 | Complexity: 0.465 [0.004-0.900] | Temp: 5.22 [2.67-7.63] [3mepoch[0m[2m=[0m72 [3mloss[0m[2m=[0m0.20998530089855194 [3mgrad_norm[0m[2m=[0m1.6710864305496216 [3mlearning_rate[0m[2m=[0m0.00016010795661713928
[2m2025-10-21T17:45:20.690013Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 87.02", "L1: 87.02", "L2: 45.65", "L3: 31.14", "L4: 18.72", "L5: 12.73", "L6: 6.17", "L7: 5.08", "L8: 1.95", "L9: 1.37", "L10: 0.06", "L11: 0.11", "L12: 0.02", "L13: 0.11"]
[2m2025-10-21T17:45:20.777532Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.52h@0.54p | L9: 3.70h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.2924 | Complexity: 0.465 [0.004-0.900] | Temp: 5.22 [2.67-7.63] [3mepoch[0m[2m=[0m73 [3mloss[0m[2m=[0m0.212320476770401 [3mgrad_norm[0m[2m=[0m15.524675369262695 [3mlearning_rate[0m[2m=[0m0.0001530380395706743
[2m2025-10-21T17:45:20.935101Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 14.19", "L1: 14.19", "L2: 7.56", "L3: 5.16", "L4: 2.84", "L5: 1.97", "L6: 1.12", "L7: 0.89", "L8: 0.30", "L9: 0.34", "L10: 0.09", "L11: 0.14", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:20.977437Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 15.58", "L1: 15.58", "L2: 9.12", "L3: 6.24", "L4: 3.65", "L5: 2.59", "L6: 1.24", "L7: 0.99", "L8: 0.33", "L9: 0.35", "L10: 0.10", "L11: 0.16", "L12: 0.03", "L13: 0.11"]
[2m2025-10-21T17:45:21.049629Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 10.64", "L1: 10.64", "L2: 7.72", "L3: 5.26", "L4: 3.35", "L5: 2.23", "L6: 0.83", "L7: 0.71", "L8: 0.18", "L9: 0.16", "L10: 0.03", "L11: 0.05", "L12: 0.01", "L13: 0.09"]
[2m2025-10-21T17:45:21.062805Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.71h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.3104 | Complexity: 0.466 [0.004-0.901] | Temp: 5.24 [2.67-7.63] [3mepoch[0m[2m=[0m74 [3mloss[0m[2m=[0m0.2172563374042511 [3mgrad_norm[0m[2m=[0m6.837289333343506 [3mlearning_rate[0m[2m=[0m0.0001461345818825066
[2m2025-10-21T17:45:21.275325Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 23.49", "L1: 23.49", "L2: 17.77", "L3: 12.08", "L4: 7.77", "L5: 5.27", "L6: 1.72", "L7: 1.40", "L8: 0.39", "L9: 0.38", "L10: 0.06", "L11: 0.10", "L12: 0.02", "L13: 0.11"]
[2m2025-10-21T17:45:21.370692Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.69h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.3279 | Complexity: 0.465 [0.004-0.899] | Temp: 5.24 [2.67-7.61] [3mepoch[0m[2m=[0m75 [3mloss[0m[2m=[0m0.2173076868057251 [3mgrad_norm[0m[2m=[0m8.27014446258545 [3mlearning_rate[0m[2m=[0m0.00013940720236860216
[2m2025-10-21T17:45:21.574412Z[0m [33m WARN[0m [2mllm::llm[0m[2m:[0m Layer-wise gradient norms: ["L0: 13.63", "L1: 13.63", "L2: 7.72", "L3: 5.29", "L4: 2.70", "L5: 1.93", "L6: 1.12", "L7: 0.90", "L8: 0.26", "L9: 0.36", "L10: 0.07", "L11: 0.12", "L12: 0.02", "L13: 0.10"]
[2m2025-10-21T17:45:21.708111Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.67h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.3454 | Complexity: 0.464 [0.003-0.899] | Temp: 5.24 [2.67-7.60] [3mepoch[0m[2m=[0m76 [3mloss[0m[2m=[0m0.2154049128293991 [3mgrad_norm[0m[2m=[0m4.645033359527588 [3mlearning_rate[0m[2m=[0m0.00013286503963172436
[2m2025-10-21T17:45:21.994034Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.65h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.3622 | Complexity: 0.463 [0.003-0.901] | Temp: 5.23 [2.66-7.60] [3mepoch[0m[2m=[0m77 [3mloss[0m[2m=[0m0.21443188190460205 [3mgrad_norm[0m[2m=[0m3.29081130027771 [3mlearning_rate[0m[2m=[0m0.00012651699944399297
[2m2025-10-21T17:45:22.277173Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.64h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.3787 | Complexity: 0.463 [0.003-0.902] | Temp: 5.23 [2.66-7.61] [3mepoch[0m[2m=[0m78 [3mloss[0m[2m=[0m0.21281388401985168 [3mgrad_norm[0m[2m=[0m2.189758777618408 [3mlearning_rate[0m[2m=[0m0.00012037171836709604
[2m2025-10-21T17:45:22.580846Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.3943 | Complexity: 0.463 [0.003-0.904] | Temp: 5.23 [2.66-7.61] [3mepoch[0m[2m=[0m79 [3mloss[0m[2m=[0m0.21138858795166016 [3mgrad_norm[0m[2m=[0m2.1231420040130615 [3mlearning_rate[0m[2m=[0m0.0001144376874435693
[2m2025-10-21T17:45:22.887195Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.4092 | Complexity: 0.462 [0.003-0.906] | Temp: 5.23 [2.65-7.62] [3mepoch[0m[2m=[0m80 [3mloss[0m[2m=[0m0.21053580939769745 [3mgrad_norm[0m[2m=[0m1.9289277791976929 [3mlearning_rate[0m[2m=[0m0.00010872301936615258
[2m2025-10-21T17:45:23.183310Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.4233 | Complexity: 0.462 [0.004-0.907] | Temp: 5.23 [2.65-7.62] [3mepoch[0m[2m=[0m81 [3mloss[0m[2m=[0m0.21140486001968384 [3mgrad_norm[0m[2m=[0m2.24495530128479 [3mlearning_rate[0m[2m=[0m0.00010323537571821362
[2m2025-10-21T17:45:23.476382Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.53h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.4368 | Complexity: 0.462 [0.004-0.907] | Temp: 5.23 [2.66-7.62] [3mepoch[0m[2m=[0m82 [3mloss[0m[2m=[0m0.20961125195026398 [3mgrad_norm[0m[2m=[0m1.7682372331619263 [3mlearning_rate[0m[2m=[0m9.798238170333207e-5
[2m2025-10-21T17:45:23.780733Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.4496 | Complexity: 0.462 [0.004-0.907] | Temp: 5.23 [2.66-7.62] [3mepoch[0m[2m=[0m83 [3mloss[0m[2m=[0m0.20920461416244507 [3mgrad_norm[0m[2m=[0m1.7002859115600586 [3mlearning_rate[0m[2m=[0m9.297116775996983e-5
[2m2025-10-21T17:45:24.084526Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.62h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.4617 | Complexity: 0.462 [0.004-0.907] | Temp: 5.23 [2.66-7.62] [3mepoch[0m[2m=[0m84 [3mloss[0m[2m=[0m0.20898140966892242 [3mgrad_norm[0m[2m=[0m1.5773853063583374 [3mlearning_rate[0m[2m=[0m8.820859511615708e-5
[2m2025-10-21T17:45:24.377145Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.62h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.4733 | Complexity: 0.462 [0.004-0.906] | Temp: 5.23 [2.66-7.62] [3mepoch[0m[2m=[0m85 [3mloss[0m[2m=[0m0.2093593329191208 [3mgrad_norm[0m[2m=[0m1.929587721824646 [3mlearning_rate[0m[2m=[0m8.370111754629761e-5
[2m2025-10-21T17:45:24.664164Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.62h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.4843 | Complexity: 0.462 [0.004-0.906] | Temp: 5.23 [2.66-7.62] [3mepoch[0m[2m=[0m86 [3mloss[0m[2m=[0m0.208805114030838 [3mgrad_norm[0m[2m=[0m1.5594643354415894 [3mlearning_rate[0m[2m=[0m7.94549923739396e-5
[2m2025-10-21T17:45:24.954129Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.62h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.4947 | Complexity: 0.462 [0.004-0.906] | Temp: 5.23 [2.66-7.62] [3mepoch[0m[2m=[0m87 [3mloss[0m[2m=[0m0.20841728150844574 [3mgrad_norm[0m[2m=[0m1.4824495315551758 [3mlearning_rate[0m[2m=[0m7.54759312258102e-5
[2m2025-10-21T17:45:25.243863Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.62h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5047 | Complexity: 0.462 [0.004-0.906] | Temp: 5.23 [2.66-7.62] [3mepoch[0m[2m=[0m88 [3mloss[0m[2m=[0m0.20827460289001465 [3mgrad_norm[0m[2m=[0m1.5069429874420166 [3mlearning_rate[0m[2m=[0m7.176936196628958e-5
[2m2025-10-21T17:45:25.528430Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.62h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5142 | Complexity: 0.462 [0.004-0.906] | Temp: 5.23 [2.66-7.62] [3mepoch[0m[2m=[0m89 [3mloss[0m[2m=[0m0.20814445614814758 [3mgrad_norm[0m[2m=[0m1.5035066604614258 [3mlearning_rate[0m[2m=[0m6.834041414549574e-5
[2m2025-10-21T17:45:25.814282Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5233 | Complexity: 0.462 [0.004-0.906] | Temp: 5.23 [2.66-7.62] [3mepoch[0m[2m=[0m90 [3mloss[0m[2m=[0m0.20837421715259552 [3mgrad_norm[0m[2m=[0m1.6653820276260376 [3mlearning_rate[0m[2m=[0m6.519373710034415e-5
[2m2025-10-21T17:45:26.107440Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5320 | Complexity: 0.462 [0.004-0.906] | Temp: 5.23 [2.66-7.63] [3mepoch[0m[2m=[0m91 [3mloss[0m[2m=[0m0.20809200406074524 [3mgrad_norm[0m[2m=[0m1.6309934854507446 [3mlearning_rate[0m[2m=[0m6.233363092178479e-5
[2m2025-10-21T17:45:26.390895Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5403 | Complexity: 0.463 [0.004-0.906] | Temp: 5.23 [2.66-7.63] [3mepoch[0m[2m=[0m92 [3mloss[0m[2m=[0m0.20956115424633026 [3mgrad_norm[0m[2m=[0m1.7840057611465454 [3mlearning_rate[0m[2m=[0m5.9764010075014085e-5
[2m2025-10-21T17:45:26.682440Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5483 | Complexity: 0.463 [0.004-0.906] | Temp: 5.23 [2.66-7.62] [3mepoch[0m[2m=[0m93 [3mloss[0m[2m=[0m0.20835553109645844 [3mgrad_norm[0m[2m=[0m2.0027642250061035 [3mlearning_rate[0m[2m=[0m5.748835610575043e-5
[2m2025-10-21T17:45:26.997518Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5560 | Complexity: 0.463 [0.004-0.906] | Temp: 5.23 [2.65-7.62] [3mepoch[0m[2m=[0m94 [3mloss[0m[2m=[0m0.20864450931549072 [3mgrad_norm[0m[2m=[0m1.770279884338379 [3mlearning_rate[0m[2m=[0m5.5509808589704335e-5
[2m2025-10-21T17:45:27.292202Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.63h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5636 | Complexity: 0.463 [0.004-0.906] | Temp: 5.23 [2.65-7.62] [3mepoch[0m[2m=[0m95 [3mloss[0m[2m=[0m0.20803675055503845 [3mgrad_norm[0m[2m=[0m1.5585373640060425 [3mlearning_rate[0m[2m=[0m5.383104144129902e-5
[2m2025-10-21T17:45:27.587627Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.64h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5710 | Complexity: 0.463 [0.004-0.906] | Temp: 5.23 [2.65-7.62] [3mepoch[0m[2m=[0m96 [3mloss[0m[2m=[0m0.20813465118408203 [3mgrad_norm[0m[2m=[0m1.5813496112823486 [3mlearning_rate[0m[2m=[0m5.245439751888625e-5
[2m2025-10-21T17:45:27.909213Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.64h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5782 | Complexity: 0.463 [0.004-0.906] | Temp: 5.23 [2.65-7.62] [3mepoch[0m[2m=[0m97 [3mloss[0m[2m=[0m0.20790596306324005 [3mgrad_norm[0m[2m=[0m1.5999635457992554 [3mlearning_rate[0m[2m=[0m5.138170308782719e-5
[2m2025-10-21T17:45:28.209303Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.64h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5853 | Complexity: 0.463 [0.003-0.907] | Temp: 5.23 [2.65-7.62] [3mepoch[0m[2m=[0m98 [3mloss[0m[2m=[0m0.20791280269622803 [3mgrad_norm[0m[2m=[0m2.0070202350616455 [3mlearning_rate[0m[2m=[0m5.06144278915599e-5
[2m2025-10-21T17:45:28.508370Z[0m [32m INFO[0m [2mllm::llm[0m[2m:[0m Training epoch completed | MoH L1: 4.55h@0.56p | L5: 4.54h@0.54p | L9: 3.64h@0.57p | ThreshRange: [0.54-0.57] | PredNorm: 13.5924 | Complexity: 0.463 [0.003-0.907] | Temp: 5.23 [2.65-7.62] [3mepoch[0m[2m=[0m99 [3mloss[0m[2m=[0m0.20769715309143066 [3mgrad_norm[0m[2m=[0m1.5879663228988647 [3mlearning_rate[0m[2m=[0m5.015366696170531e-5

=== AFTER TRAINING ===
Input: User: How do mountains form?
Output: Assistant : Mountains are formed through tectonic forces or volcanism over long geological time periods </s>
======================

