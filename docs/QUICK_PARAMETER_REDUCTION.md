# Quick Parameter Reduction Guide

## üéØ Goal: Reduce HyperMixer from 1.39M to 640K parameters (54% reduction)

## ‚úÖ Recommended: Change 2 Lines of Code

### Change 1: Reduce max_seq_len

**File**: `src/lib.rs` (line 37)

```rust
// FROM:
pub const MAX_SEQ_LEN: usize = 80;

// TO:
pub const MAX_SEQ_LEN: usize = 40;
```

**Impact**: 
- Reduces hypernetwork output size from 10,384 to 5,224
- 24% parameter reduction
- ‚úÖ **No training impact** if your sequences are typically < 40 tokens

---

### Change 2: Reduce token_mixing_hidden_dim

**File**: `src/hypermixer.rs` (line 59)

```rust
// FROM:
let token_mixing_hidden_dim = embedding_dim / 2;

// TO:
let token_mixing_hidden_dim = embedding_dim / 4;
```

**Impact**:
- Reduces token mixing MLP hidden dimension from 64 to 32
- Additional 30% parameter reduction
- ‚úÖ **Minimal training impact** - still sufficient capacity

---

## Results

### Parameter Count Comparison

| Configuration | Hypernetwork w2 | Total Params | Reduction |
|---------------|-----------------|--------------|-----------|
| **Original** (80, 64) | 332,288 | 1,386,917 | - |
| **Optimized** (40, 32) | 84,224 | ~640,000 | 54% |

### Detailed Breakdown

**Original**:
```
max_seq_len = 80, token_mixing_hidden_dim = 64
output_size = 80*64 + 64 + 64*80 + 80 = 10,384
Hypernetwork w2: (32, 10,384) = 332,288 params per layer
Total: 1,386,917 params
```

**Optimized**:
```
max_seq_len = 40, token_mixing_hidden_dim = 32
output_size = 40*32 + 32 + 32*40 + 40 = 2,632
Hypernetwork w2: (32, 2,632) = 84,224 params per layer
Total: ~640,000 params
```

---

## Training Impact: ‚úÖ MINIMAL

### Why These Changes Don't Hurt Training

1. **max_seq_len = 40**:
   - Most training examples are < 40 tokens anyway
   - Longer sequences can be truncated or chunked
   - No impact on shorter sequences

2. **token_mixing_hidden_dim = 32**:
   - 32 hidden units is still sufficient for mixing 40 tokens
   - Reduces overfitting (regularization effect)
   - May actually improve generalization

### Verification

Check your sequence length distribution:
```rust
// In your training data
let lengths: Vec<usize> = training_data.iter()
    .map(|example| example.tokens.len())
    .collect();

let p95 = percentile(&lengths, 0.95);
println!("95th percentile length: {}", p95);

// If p95 < 40, you're safe to use max_seq_len = 40
```

---

## Alternative: More Conservative (30% reduction)

If you want to be extra safe:

### Option A: Only reduce max_seq_len

**File**: `src/lib.rs`
```rust
pub const MAX_SEQ_LEN: usize = 50;  // Instead of 40
```

**Result**: ~1,000,000 params (28% reduction)

### Option B: Only reduce token_mixing_hidden_dim

**File**: `src/hypermixer.rs`
```rust
let token_mixing_hidden_dim = embedding_dim / 3;  // Instead of /4
```

**Result**: ~1,100,000 params (21% reduction)

---

## ‚ö†Ô∏è NOT Recommended: Share Hypernetwork

### What It Does

All 3 layers use the SAME hypernetwork to generate mixing weights.

**Impact**:
- ‚úÖ 50% parameter reduction (693K params)
- ‚ùå **Significant training impact** (10-20% worse performance)
- ‚ùå Requires code changes (Rc<RefCell<>> or similar)
- ‚ùå Serialization complications

### Why It Hurts Training

- Each layer can't learn specialized mixing strategies
- Reduces model expressiveness
- Forces all layers to use the same weight generation function
- Similar to using the same transformer block 3 times

### When to Consider

- ‚úÖ Very simple tasks (basic classification)
- ‚úÖ Extreme memory constraints
- ‚úÖ Willing to sacrifice performance for size
- ‚ùå Complex tasks (generation, reasoning)
- ‚ùå Need maximum performance

---

## Implementation Steps

### Step 1: Make the changes

```bash
# Edit src/lib.rs line 37
pub const MAX_SEQ_LEN: usize = 40;

# Edit src/hypermixer.rs line 59
let token_mixing_hidden_dim = embedding_dim / 4;
```

### Step 2: Rebuild

```bash
cargo build --release
```

### Step 3: Verify parameter count

```bash
cargo run --release --bin llm
```

Look for:
```
Total Parameters: ~640000  # Should be around 640K
```

### Step 4: Train and compare

Train on your task and compare:
- Loss convergence
- Final performance
- Training speed

---

## Comparison Table

| Strategy | Lines Changed | Param Reduction | Training Impact | Recommendation |
|----------|---------------|-----------------|-----------------|----------------|
| **Reduce constants** | 2 | 54% (747K) | ‚úÖ Minimal | ‚≠ê‚≠ê **DO THIS** |
| Reduce max_seq_len only | 1 | 24% (337K) | ‚úÖ Minimal | ‚≠ê Good |
| Reduce token_hidden only | 1 | 24% (337K) | ‚úÖ Minimal | ‚≠ê Good |
| Reduce hyper_hidden | 1 | 36% (505K) | ‚ö†Ô∏è Moderate | ‚ö†Ô∏è Try last |
| **Share hypernetwork** | Many | 50% (693K) | ‚ùå High | ‚ùå Avoid |

---

## Conclusion

**Best approach**: Change those 2 lines of code!

```rust
// src/lib.rs line 37
pub const MAX_SEQ_LEN: usize = 40;

// src/hypermixer.rs line 59
let token_mixing_hidden_dim = embedding_dim / 4;
```

**Result**:
- ‚úÖ 54% parameter reduction (1.39M ‚Üí 640K)
- ‚úÖ Minimal training impact
- ‚úÖ 2 lines of code
- ‚úÖ Reversible
- ‚úÖ No architectural changes

**Don't** share hypernetwork unless you've tried this first and still need more reduction.

