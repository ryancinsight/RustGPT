# Adaptive Window Attention Implementation (Phase 4)

## Overview

Adaptive Window Attention is a Phase 4 enhancement that dynamically adjusts the sliding window size based on context and computational constraints. This builds upon Phase 3's Sliding Window Attention by making the window size adaptive rather than fixed.

## What is Adaptive Window Sizing?

In Phase 3, we implemented Sliding Window Attention with a fixed window size (e.g., 4096 tokens). While this provides significant efficiency gains, a fixed window may not be optimal for all sequences:

- **Short sequences** don't need large windows
- **Long sequences** might benefit from larger windows
- **Different attention patterns** (focused vs. diffuse) may require different window sizes

Adaptive Window Sizing solves this by dynamically computing the window size for each forward pass based on the chosen strategy.

## Key Benefits

1. **Better Resource Utilization**: Smaller windows for short sequences save computation
2. **Improved Quality**: Larger windows when needed for complex patterns
3. **Automatic Tuning**: No need to manually tune window size for different tasks
4. **Smooth Transitions**: Window size adapts gradually, maintaining training stability
5. **Multiple Strategies**: Choose the adaptation strategy that fits your use case

## Adaptation Strategies

### 1. SequenceLengthBased (Recommended)

**Strategy**: `window_size = min(max_window, max(min_window, seq_len / 2))`

**When to use**:
- General-purpose applications
- When you want simple, predictable behavior
- As a baseline for comparison

**Characteristics**:
- Most stable and predictable
- Scales linearly with sequence length
- No additional computation overhead
- Good default choice

**Example**:
```rust
let attention = SelfAttention::new_with_adaptive_window(
    embedding_dim,
    num_heads,
    num_kv_heads,
    use_rope,
    max_seq_len,
    None,
)
.min_window_size(512)
.max_window_size(4096)
.strategy(WindowAdaptationStrategy::SequenceLengthBased)
.build();
```

### 2. AttentionEntropy

**Strategy**: Adjust window based on attention distribution entropy

**When to use**:
- When attention patterns vary significantly
- For tasks requiring dynamic context windows
- When you want the model to "decide" window size

**Characteristics**:
- Higher entropy (diffuse attention) â†’ larger window
- Lower entropy (focused attention) â†’ smaller window
- Responds to actual attention patterns
- Slightly more computational overhead

**How it works**:
1. Compute attention entropy: `H = -Î£ p(i,j) * log(p(i,j))`
2. Normalize entropy to [0, 1] range
3. Map to window size: `window = min_window + (max_window - min_window) * normalized_entropy`

**Example**:
```rust
let attention = SelfAttention::new_with_adaptive_window(
    embedding_dim,
    num_heads,
    num_kv_heads,
    use_rope,
    max_seq_len,
    None,
)
.min_window_size(512)
.max_window_size(4096)
.strategy(WindowAdaptationStrategy::AttentionEntropy)
.build();
```

### 3. PerplexityBased (Future Enhancement)

**Strategy**: Adjust window based on prediction perplexity

**When to use**:
- When model confidence varies significantly
- For adaptive generation tasks
- When you want larger windows for uncertain predictions

**Characteristics**:
- Higher perplexity (uncertain) â†’ larger window
- Lower perplexity (confident) â†’ smaller window
- Requires perplexity computation from model output
- Currently falls back to SequenceLengthBased

**Note**: Full implementation requires integration with model output layer.

### 4. Fixed

**Strategy**: Use configured `window_size` (no adaptation)

**When to use**:
- For backward compatibility
- When you want Phase 3 behavior
- For benchmarking against adaptive strategies

**Characteristics**:
- Behaves exactly like Phase 3 Sliding Window
- No adaptation overhead
- Predictable performance

## Configuration

### ModelConfig Fields

```rust
pub struct ModelConfig {
    // ... other fields ...
    
    /// Enable adaptive window sizing
    pub use_adaptive_window: bool,
    
    /// Minimum window size for adaptive sizing
    pub min_window_size: usize,
    
    /// Maximum window size for adaptive sizing
    pub max_window_size: usize,
    
    /// Strategy for adapting window size
    pub window_adaptation_strategy: WindowAdaptationStrategy,
}
```

### Default Values

- `use_adaptive_window`: `false` (backward compatibility)
- `min_window_size`: `512` (reasonable minimum)
- `max_window_size`: `4096` (Mistral 7B style)
- `window_adaptation_strategy`: `SequenceLengthBased` (most stable)

## Implementation Details

### Builder Pattern

To avoid too many constructor parameters, we use a builder pattern:

```rust
let attention = SelfAttention::new_with_adaptive_window(
    embedding_dim,
    num_heads,
    num_kv_heads,
    use_rope,
    max_seq_len,
    window_size,
)
.min_window_size(512)
.max_window_size(4096)
.strategy(WindowAdaptationStrategy::SequenceLengthBased)
.build();
```

### Adaptive Window Computation

The window size is computed at the beginning of each forward pass:

```rust
fn compute_adaptive_window_size(&self, seq_len: usize) -> usize {
    if !self.use_adaptive_window {
        return self.window_size.unwrap_or(seq_len);
    }

    match self.window_adaptation_strategy {
        WindowAdaptationStrategy::SequenceLengthBased => {
            let proposed_window = seq_len / 2;
            proposed_window.clamp(self.min_window_size, self.max_window_size)
        }
        // ... other strategies ...
    }
}
```

### Forward Pass Integration

```rust
fn forward(&mut self, input: &Array2<f32>) -> Array2<f32> {
    let seq_len = input.shape()[0];
    
    // Compute adaptive window size if enabled
    let effective_window_size = if self.use_adaptive_window {
        let adaptive_size = self.compute_adaptive_window_size(seq_len);
        self.current_window_size = Some(adaptive_size);
        Some(adaptive_size)
    } else {
        self.window_size
    };
    
    // Use effective_window_size for attention computation
    // ...
}
```

## Performance Characteristics

### Computational Overhead

| Strategy | Overhead | Notes |
|----------|----------|-------|
| SequenceLengthBased | ~0% | Simple arithmetic |
| AttentionEntropy | ~1-2% | Entropy computation |
| PerplexityBased | ~2-3% | Perplexity computation (future) |
| Fixed | 0% | No adaptation |

### Memory Usage

Adaptive window sizing has **no additional memory overhead** compared to fixed sliding window. The window size is computed on-the-fly and not stored.

### Training Stability

All strategies maintain training stability through:
- **Clamping**: Window size always within [min_window, max_window]
- **Smooth transitions**: Window size changes gradually with sequence length
- **Fallback mechanisms**: Strategies fall back to SequenceLengthBased if needed

## Usage Examples

### Basic Usage (SequenceLengthBased)

```rust
use llm::{ModelConfig, WindowAdaptationStrategy};

let mut config = ModelConfig::transformer(512, 2048, 6, 8192, None, Some(8));

// Enable adaptive window
config.use_adaptive_window = true;
config.min_window_size = 512;
config.max_window_size = 4096;
config.window_adaptation_strategy = WindowAdaptationStrategy::SequenceLengthBased;

let layers = build_network(&config);
```

### Advanced Usage (AttentionEntropy)

```rust
let mut config = ModelConfig::transformer(512, 2048, 6, 8192, None, Some(8));

// Enable adaptive window with entropy-based strategy
config.use_adaptive_window = true;
config.min_window_size = 256;  // Smaller minimum for focused attention
config.max_window_size = 8192; // Larger maximum for diffuse attention
config.window_adaptation_strategy = WindowAdaptationStrategy::AttentionEntropy;

let layers = build_network(&config);
```

### Combined with GQA and RoPE

```rust
let mut config = ModelConfig::transformer(512, 2048, 6, 8192, None, Some(8));

// Full modern stack: RMSNorm + SwiGLU + RoPE + GQA + Adaptive Window
config.use_rms_norm = true;
config.use_swiglu = true;
config.use_rope = true;
config.num_kv_heads = Some(4);
config.use_adaptive_window = true;
config.min_window_size = 512;
config.max_window_size = 4096;
config.window_adaptation_strategy = WindowAdaptationStrategy::SequenceLengthBased;

let layers = build_network(&config);
```

## Testing

Comprehensive tests in `tests/adaptive_window_test.rs`:

- âœ… SequenceLengthBased strategy
- âœ… Min/max bounds enforcement
- âœ… Integration with GQA
- âœ… Integration with RoPE
- âœ… Combined GQA + RoPE + Adaptive Window
- âœ… Backward pass correctness
- âœ… Training stability over multiple steps
- âœ… Different sequence lengths
- âœ… AttentionEntropy strategy
- âœ… PerplexityBased strategy (fallback)
- âœ… Fixed strategy
- âœ… Comparison with fixed window

Run tests:
```bash
cargo test --test adaptive_window_test
```

## Architecture Summary

The enhanced architecture summary now shows adaptive window configuration:

```
ðŸš€ Modern LLM Enhancements:
  âœ“ Adaptive Sliding Window Attention (Phase 4)
    - Strategy: SequenceLengthBased
    - Window Range: 512 - 4096
    - Base Window: None
    - Adapts dynamically based on context
```

## Future Enhancements

1. **PerplexityBased Strategy**: Full implementation with model output integration
2. **Learned Adaptation**: Train a small network to predict optimal window size
3. **Task-Specific Strategies**: Different strategies for different tasks (summarization, QA, etc.)
4. **Hierarchical Windows**: Multiple window sizes at different layers
5. **Dynamic Min/Max**: Adjust min/max bounds based on available memory

## References

1. **Sliding Window Attention**: Mistral 7B (Jiang et al., 2023)
2. **Adaptive Attention**: "Adaptive Attention Span in Transformers" (Sukhbaatar et al., 2019)
3. **Entropy-Based Adaptation**: "Learning to Control Fast-Weight Memories" (Schlag et al., 2021)

---

**Phase 4 Complete**: RustGPT now has adaptive window sizing for dynamic, context-aware attention! ðŸŽ‰

